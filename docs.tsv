[Author: Franco Moretti; From essay:"Conjectures on World Literature "] - NOWADAYS, NATIONAL LITERATURE doesnt mean much: the age of world literature is beginning, and everybody should contribute to hasten its advent.’ This was Goethe, of course, talking to Eckermann in 1827; and these are Marx and Engels, twenty years later, in 1848: ‘National one-sidedness and narrow-mindedness become more and more impossible, and from the many national and local literatures, a world literature arises.’ Weltliteratur: this is what Goethe and Marx have in mind. Not ‘comparative’, but world literature: the Chinese novel that Goethe was reading at the time of that exchange, or the bourgeoisie of the Manifesto, which has ‘given a cosmopolitan character to production and consumption in every country’. Well, let me put it very simply: comparative literature has not lived up to these beginnings. Its been a much more modest intellectual enterprise, fundamentally limited to Western Europe, and mostly revolving around the river Rhine (German philologists working on French literature). Not much more.
[Author: Franco Moretti; From essay:"Conjectures on World Literature "] - This is my own intellectual formation, and scientific work always has limits. But limits change, and I think its time we returned to that old ambition of Weltliteratur: after all, the literature around us is now unmistakably a planetary system. The question is not really what we should do—the question is how. What does it mean, studying world literature? How do we do it? I work on West European narrative between 1790 and 1930, and already feel like a charlatan outside of Britain or France. World literature?
[Author: Franco Moretti; From essay:"Conjectures on World Literature "] - Many people have read more and better than I have, of course, but still, we are talking of hundreds of languages and literatures here. Reading ‘more’ seems hardly to be the solution. Especially because weve just started rediscovering what Margaret Cohen calls the ‘great unread’. ‘I work on West European narrative, etc. . . .’ Not really, I work on its canonical fraction, which is not even one per cent of published literature. And again, some people have read more, but the point is that there are thirty thousand nineteenth-century British novels out there, forty, fifty, sixty thousand—no one really knows, no one has read them, no one ever will. And then there are French novels, Chinese, Argentinian, American . . . Reading ‘more’ is always a good thing, but not the solution.
[Author: Franco Moretti; From essay:"Conjectures on World Literature "] - Perhaps its too much, tackling the world and the unread at the same time. But I actually think that its our greatest chance, because the sheer enormity of the task makes it clear that world literature cannot be literature, bigger; what we are already doing, just more of it. It has to be different. The categories have to be different. ‘It is not the “actual” interconnection of “things”’, Max Weber wrote, ‘but the conceptual interconnection of problems which define the scope of the various sciences. A new “science” emerges where a new problem is pursued by a new method.’ Thats the point: world literature is not an object, its a problem, and a problem that asks for a new critical method: and no one has ever found a method by just reading more texts. Thats not how theories come into being; they need a leap, a wager—a hypothesis, to get started.
[Author: Franco Moretti; From essay:"Conjectures on World Literature "] - World literature: one and unequalI will borrow this initial hypothesis from the world-system school of economic history, for which international capitalism is a system that is simultaneously one, and unequal: with a core, and a periphery (and a semiperiphery) that are bound together in a relationship of growing inequality. One, and unequal: one literature (Weltliteratur, singular, as in Goethe and Marx), or perhaps, better, one world literary system (of inter-related literatures); but a system which is different from what Goethe and Marx had hoped for, because its profoundly unequal. Foreign debt is as inevitable in Brazilian letters as in any other field, writes Roberto Schwarz in a splendid essay on ‘The Importing of the Novel to Brazil: ‘its not simply an easily dispensable part of the work in which it appears, but a complex feature of it’; and Itamar Even-Zohar, reflecting on Hebrew literature: ‘Interference [is] a relationship between literatures, whereby a . . . source literature may become a source of direct or indirect loans [Importing of the novel, direct and indirect loans, foreign debt: see how economic metaphors have been subterraneously at work in literary history]—a source of loans for . . . a target literature . . . There is no symmetry in literary interference. A target literature is, more often than not, interfered with by a source literature which completely ignores it.’
[Author: Franco Moretti; From essay:"Conjectures on World Literature "] - This is what one and unequal means: the destiny of a culture (usually a culture of the periphery, as Montserrat Iglesias Santos has specified) is intersected and altered by another culture (from the core) that ‘completely ignores it. A familiar scenario, this asymmetry in international power—and later I will say more about Schwarzs foreign debt as a complex literary feature. Right now, let me spell out the consequences of taking an explanatory matrix from social history and applying it to literary history.
[Author: Franco Moretti; From essay:"Conjectures on World Literature "] - Distant readingWriting about comparative social history, Marc Bloch once coined a lovely slogan, as he himself called it: ‘years of analysis for a day of synthesis’; and if you read Braudel or Wallerstein you immediately see what Bloch had in mind. The text which is strictly Wallersteins, his day of synthesis, occupies one third of a page, one fourth, maybe half; the rest are quotations (fourteen hundred, in the first volume of The Modern World-System). Years of analysis; other peoples analysis, which Wallersteins page synthesizes into a system.
[Author: Franco Moretti; From essay:"Conjectures on World Literature "] - Now, if we take this model seriously, the study of world literature will somehow have to reproduce this page which is to say: this relationship between analysis and synthesis—for the literary field. But in that case, literary history will quickly become very different from what it is now: it will become ‘second hand: a patchwork of other peoples research, without a single direct textual reading. Still ambitious, and actually even more so than before (world literature!); but the ambition is now directly proportional to the distance from the text: the more ambitious the project, the greater must the distance be.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - Large-scale literary history is far from a new idea. Vernacular literary study entered nineteenth-century universities as an already-ambitious project that sought to trace the parallel development of literature, language, and society across a thousand years. It was only in the twentieth century that literary scholarship began to restrict itself paradigmatically to the close reading of single texts. If we take a long view of disciplinary history, recent research on large digital libraries is just one expression of a much broader trend, beginning around the middle of the twentieth century, that has tended to reinstate the original historical ambitions of literary scholarship.
[Author: Franco Moretti; From essay:"Conjectures on World Literature "] - The United States is the country of close reading, so I dont expect this idea to be particularly popular. But the trouble with close reading (in all of its incarnations, from the new criticism to deconstruction) is that it necessarily depends on an extremely small canon. This may have become an unconscious and invisible premiss by now, but it is an iron one nonetheless: you invest so much in individual texts only if you think that very few of them really matter. Otherwise, it doesnt make sense. And if you want to look beyond the canon (and of course, world literature will do so: it would be absurd if it didnt!) close reading will not do it. Its not designed to do it, its designed to do the opposite. At bottom, its a theological exercise—very solemn treatment of very few texts taken very seriously—whereas what we really need is a little pact with the devil: we know how to read texts, now lets learn how not to read them. Distant reading: where distance, let me repeat it, is a condition of knowledge: it allows you to focus on units that are much smaller or much larger than the text: devices, themes, tropes—or genres and systems. And if, between the very small and the very large, the text itself disappears, well, it is one of those cases when one can justifiably say, Less is more. If we want to understand the system in its entirety, we must accept losing something. We always pay a price for theoretical knowledge: reality is infinitely rich; concepts are abstract, are poor. But its precisely this poverty that makes it possible to handle them, and therefore to know. This is why less is actually more.
[Author: Franco Moretti; From essay:"Conjectures on World Literature "] - The Western European novel: rule or exception?Let me give you an example of the conjunction of distant reading and world literature. An example, not a model; and of course my example, based on the field I know (elsewhere, things may be very different). A few years ago, introducing Kojin Karatanis Origins of Modern Japanese Literature, Fredric Jameson noticed that in the take-off of the modern Japanese novel, ‘the raw material of Japanese social experience and the abstract formal patterns of Western novel construction cannot always be welded together seamlessly; and he referred in this respect to Masao Miyoshis Accomplices of Silence, and Meenakshi Mukherjees Realism and Reality (a study of the early Indian novel). And its true, these books return quite often to the complicated ‘problems (Mukherjees term) arising from the encounter of western form and Japanese or Indian reality.
[Author: Franco Moretti; From essay:"Conjectures on World Literature "] - Now, that the same configuration should occur in such different cultures as India and Japan—this was curious; and it became even more curious when I realized that Roberto Schwarz had independently discovered very much the same pattern in Brazil. So, eventually, I started using these pieces of evidence to reflect on the relationship between markets and forms; and then, without really knowing what I was doing, began to treat Jamesons insight as if it were—one should always be cautious with these claims, but there is really no other way to say it—as if it were a law of literary evolution: in cultures that belong to the periphery of the literary system (which means: almost all cultures, inside and outside Europe), the modern novel first arises not as an autonomous development but as a compromise between a western formal influence (usually French or English) and local materials.
[Author: Franco Moretti; From essay:"Conjectures on World Literature "] - This first idea expanded into a little cluster of laws, and it was all very interesting, but . . . it was still just an idea; a conjecture that had to be tested, possibly on a large scale, and so I decided to follow the wave of diffusion of the modern novel (roughly: from 1750 to 1950) in the pages of literary history. Gasperetti and Goscilo on late eighteenth-century Eastern Europe; Toschi and Martí-López on early nineteenth-century Southern Europe; Franco and Sommer on mid-century Latin America; Frieden on the Yiddish novels of the 1860s; Moosa, Said and Allen on the Arabic novels of the 1870s; Evin and Parla on the Turkish novels of the same years; Anderson on the Filipino Noli Me Tangere, of 1887; Zhao and Wang on turn-of-the-century Qing fiction; Obiechina, Irele and Quayson on West African novels between the 1920s and the 1950s (plus of course Karatani, Miyoshi, Mukherjee, Even-Zohar and Schwarz). Four continents, two hundred years, over twenty independent critical studies, and they all agreed: when a culture starts moving towards the modern novel, it’s always as a compromise between foreign form and local materials. Jamesons law had passed the test—the first test, anyway. And actually more than that: it had completely reversed the received historical explanation of these matters: because if the compromise between the foreign and the local is so ubiquitous, then those independent paths that are usually taken to be the rule of the rise of the novel (the Spanish, the French, and especially the British case)—well, theyre not the rule at all, theyre the exception. They come first, yes, but theyre not at all typical. The typical rise of the novel is Krasicki, Kemal, Rizal, Maran—not Defoe.
[Author: Franco Moretti; From essay:"Conjectures on World Literature "] - Experiments with historySee the beauty of distant reading plus world literature: they go against the grain of national historiography. And they do so in the form of an experiment. You define a unit of analysis (like here, the formal compromise), and then follow its metamorphoses in a variety of environments—until, ideally, all of literary history becomes a long chain of related experiments: a dialogue between fact and fancy, as Peter Medawar calls it: between what could be true, and what is in fact the case. Apt words for this research, in the course of which, as I was reading my fellow historians, it became clear that the encounter of western forms and local reality did indeed produce everywhere a structural compromise—as the law predicted—but also, that the compromise itself was taking rather different forms. At times, especially in the second half of the nineteenth century and in Asia, it tended to be very unstable: an impossible programme, as Miyoshi says of Japan. At other times it was not so: at the beginning and at the end of the wave, for instance (Poland, Italy and Spain at one extreme; and West Africa on the other), historians describe novels that had, certainly, their own problems—but not problems arising from the clash of irreconcilable elements.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - There is nothing wrong with writing a history of food in America, and also nothing wrong with Earharts decision to focus on a particular critical tradition initiated by the advent of the web. As long as readers remember that many ingredients of this history have longer backstories elsewhere, no one will be misled. But of course, backstories do get forgotten with the passage of time, and new generations learn to associate pizza mainly with Chicago or New York. Today it is common to see distant reading and the sociology of literature folded into discussions of Big Data research in digital humanities”. To my eyes, this puts the emphasis in a strange place, and suggests that we have begun to forget (or at least underplay) important aspects of our own past. Big data is a twenty-first century technical buzzword. It is odd to see it used as a filing category for the much older aspiration to survey patterns that organize human culture.
[Author: Franco Moretti; From essay:"Conjectures on World Literature "] - I hadnt expected such a spectrum of outcomes, so at first I was taken aback, and only later realized that this was probably the most valuable finding of them all, because it showed that world literature was indeed a system—but a system of variations. The system was one, not uniform. The pressure from the Anglo-French core tried to make it uniform, but it could never fully erase the reality of difference. (See here, by the way, how the study of world literature is—inevitably—a study of the struggle for symbolic hegemony across the world.) The system was one, not uniform. And, retrospectively, of course it had to be like this: if after 1750 the novel arises just about everywhere as a compromise between West European patterns and local reality—well, local reality was different in the various places, just as western influence was also very uneven: much stronger in Southern Europe around 1800, to return to my example, than in West Africa around 1940. The forces in play kept changing, and so did the compromise that resulted from their interaction. And this, incidentally, opens a fantastic field of inquiry for comparative morphology (the systematic study of how forms vary in space and time, which is also the only reason to keep the adjective ‘comparative in comparative literature): but comparative morphology is a complex issue, that deserves its own paper.
[Author: Franco Moretti; From essay:"Conjectures on World Literature "] - Forms as abstracts of social relationshipsLet me now add a few words on that term compromise—by which I mean something a little different from what Jameson had in mind in his introduction to Karatani. For him, the relationship is fundamentally a binary one: ‘the abstract formal patterns of Western novel construction’ and the raw material of Japanese social experience: form and content, basically. For me, its more of a triangle: foreign form, local material— and local form. Simplifying somewhat: foreign plot; local characters; and then, local narrative voice: and its precisely in this third dimension that these novels seem to be most unstable—most uneasy, as Zhao says of the late Qing narrator. Which makes sense: the narrator is the pole of comment, of explanation, of evaluation, and when foreign formal patterns (or actual foreign presence, for that matter) make characters behave in strange ways (like Bunzo, or Ibarra, or Bràs Cubas), then of course comment becomes uneasy—garrulous, erratic, rudderless.
[Author: Franco Moretti; From essay:"Conjectures on World Literature "] - ‘Interferences, Even-Zohar calls them: powerful literatures making life hard for the others—making structure hard. And Schwarz: a part of the original historical conditions reappears as a sociological form . . . In this sense, forms are the abstract of specific social relationships.’ Yes, and in our case the historical conditions reappear as a sort of ‘crack in the form; as a faultline running between story and discourse, world and worldview: the world goes in the strange direction dictated by an outside power; the worldview tries to make sense of it, and is thrown off balance all the time. Like Rizals voice (oscillating between Catholic melodrama and Enlightenment sarcasm), or Futabatei’s (caught between Bunzos Russian behaviour, and the Japanese audience inscribed in the text), or Zhaos hypertrophic narrator, who has completely lost control of the plot, but still tries to dominate it at all costs. This is what Schwarz meant with that foreign debt that becomes a complex feature of the text: the foreign presence ‘interferes with the very utterance of the novel. The one-and-unequal literary system is not just an external network here, it doesnt remain outside the text: its embedded well into its form.
[Author: Franco Moretti; From essay:"Conjectures on World Literature "] - Trees, waves and cultural historyForms are the abstract of social relationships: so, formal analysis is in its own modest way an analysis of power. (Thats why comparative morphology is such a fascinating field: studying how forms vary, you discover how symbolic power varies from place to place.) And indeed, sociological formalism has always been my interpretive method, and I think that its particularly appropriate for world literature . . . But, unfortunately, at this point I must stop, because my competence stops. Once it became clear that the key variable of the experiment was the narrators voice, well, a genuine formal analysis was off limits for me, because it required a linguistic competence that I couldnt even dream of (French, English, Spanish, Russian, Japanese, Chinese and Portuguese, just for the core of the argument). And probably, no matter what the object of analysis is, there will always be a point where the study of world literature must yield to the specialist of the national literature, in a sort of cosmic and inevitable division of labour. Inevitable not just for practical reasons, but for theoretical ones. This is a large issue, but let me at least sketch its outline.
[Author: Franco Moretti; From essay:"Conjectures on World Literature "] - When historians have analysed culture on a world scale (or on a large scale anyway), they have tended to use two basic cognitive metaphors: the tree and the wave. The tree, the phylogenetic tree derived from Darwin, was the tool of comparative philology: language families branching off from each other—Slavo-Germanic from Aryan-Greco-Italo-Celtic, then Balto-Slavic from Germanic, then Lithuanian from Slavic. And this kind of tree allowed comparative philology to solve that great puzzle which was also perhaps the first world system of culture: Indo-European: a family of languages spreading from India to Ireland (and perhaps not just languages, a common cultural repertoire, too: but here the evidence is notoriously shakier). The other metaphor, the wave, was also used in historical linguistics (as in Schmidts wave hypothesis, that explained certain overlaps among languages), but it played a role in many other fields as well: the study of technological diffusion, for instance, or the fantastic interdisciplinary theory of the wave of advance by Cavalli-Sforza and Ammerman (a geneticist and an archaeologist), which explains how agriculture spread from the fertile crescent in the Middle East towards the North-West and then throughout Europe.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - This essay will turn the calendar back to the middle of the twentieth century, in order to tease apart intellectual traditions that have begun to be conflated. In particular, I want to emphasize that distant reading is not a new trend, defined by digital technology or by contemporary obsession with the word data. The questions posed by distant readers were originally framed by scholars (like Raymond Williams and Janice Radway) who worked on the boundary between literary history and social science. Of course, computer science has also been a crucial influence. But the central practice that distinguishes distant reading from other forms of literary criticism is not at bottom a technology. It is, I will argue, the practice of framing historical inquiry as an experiment, using hypotheses and samples (of texts or other social evidence) that are defined before the writer settles on a conclusion.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - Literary scholars have been much slower to imitate her methods, which depended on questionnaires, interviews, and numbers.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - As Hitchcock noted, the forces that we are seeing in the use of digitized newspapers are present in other places, such as Google Books views.
[Author: Franco Moretti; From essay:"Conjectures on World Literature "] - Now, trees and waves are both metaphors—but except for this, they have absolutely nothing in common. The tree describes the passage from unity to diversity: one tree, with many branches: from Indo-European, to dozens of different languages. The wave is the opposite: it observes uniformity engulfing an initial diversity: Hollywood films conquering one market after another (or English swallowing language after language). Trees need geographical discontinuity (in order to branch off from each other, languages must first be separated in space, just like animal species); waves dislike barriers, and thrive on geographical continuity (from the viewpoint of a wave, the ideal world is a pond). Trees and branches are what nation-states cling to; waves are what markets do. And so on. Nothing in common, between the two metaphors. But—they both work. Cultural history is made of trees and waves—the wave of agricultural advance supporting the tree of Indo-European languages, which is then swept by new waves of linguistic and cultural contact . . . And as world culture oscillates between the two mechanisms, its products are inevitably composite ones. Compromises, as in Jamesons law. Thats why the law works: because it intuitively captures the intersection of the two mechanisms. Think of the modern novel: certainly a wave (and Ive actually called it a wave a few times)—but a wave that runs into the branches of local traditions, and is always significantly transformed by them.
[Author: Franco Moretti; From essay:"Conjectures on World Literature "] - This, then, is the basis for the division of labour between national and world literature: national literature, for people who see trees; world literature, for people who see waves. Division of labour . . . and challenge; because both metaphors work, yes, but that doesnt mean that they work equally well. The products of cultural history are always composite ones: but which is the dominant mechanism in their composition? The internal, or the external one? The nation or the world? The tree or the wave? There is no way to settle this controversy once and for all—fortunately: because comparatists need controversy. They have always been too shy in the presence of national literatures, too diplomatic: as if one had English, American, German literature—and then, next door, a sort of little parallel universe where comparatists studied a second set of literatures, trying not to disturb the first set. No; the universe is the same, the literatures are the same, we just look at them from a different viewpoint; and you become a comparatist for a very simple reason: because you are convinced that that viewpoint is better. It has greater explanatory power; its conceptually more elegant; it avoids that ugly ‘one-sidedness and narrow-mindedness; whatever. The point is that there is no other justification for the study of world literature (and for the existence of departments of comparative literature) but this: to be a thorn in the side, a permanent intellectual challenge to national literatures—especially the local literature. If comparative literature is not this, its nothing. Nothing. Dont delude yourself, writes Stendhal of his favourite character: for you, there is no middle road. The same is true for us.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - It has recently become common to describe all empirical approaches to literature as subfields of digital humanities. This essay argues that distant reading has a largely distinct genealogy stretching back many decades before the advent of the internet – a genealogy that is not for the most part centrally concerned with computers. It would be better to understand this field as a conversation between literary studies and social science, inititated by scholars like Raymond Williams and Janice Radway, and moving slowly toward an explicitly experimental method. Candor about the social-scientific dimension of distant reading is needed now, in order to refocus a research agenda that can drift into diffuse exploration of digital tools. Clarity on this topic might also reduce miscommunication between distant readers and digital humanists.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - Over the last decade or so, it has become common to describe all empirical approaches to literary history as subfields of digital humanities. At first, I didnt take this conflation seriously; I thought it was journalistic shorthand for a history that scholars understood to be more complex. Writing in The New York Times, for instance, Kathryn Schulz described distant reading in 2011 as one of many approaches currently proliferating under the broad rubric of digital humanities. A reader who thought it worthwhile to quibble could have replied that neither thing is a subset of the other. Although the projects were certainly in conversation by 2011, the phrases distant reading and digital humanities had been coined ten years earlier, in different academic communities, to describe different kinds of research. Digital technology hadnt even played a central role in early examples of distant reading. But why quibble? No one expects a short newspaper article to give a full history of academic trends.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - More recently, however, I have noticed that scholars themselves are beginning to narrate intellectual history in the same way: treating all quantitative or empirical approaches to literary history as aspects of a digital turn in the discipline. In Amy Earharts genealogy of digital literary studies, for instance, distant reading is presented as a recent change of course in an intellectual tradition originally centered on editorial theory and the internet.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - [T]he digital work I have traced in the first part of this book has been largely representational, with technology primarily used to create idealized or better versions than would be possible in print. Current trends in digital literary studies, and the larger digital humanities, appear to be moving away from representational concerns and toward interpretive functions as contemporary digital scholars, such as Stephen Ramsay, Franco Moretti, Matthew Jockers, Geoffrey Rockwell, and others, are using technology to devolve, manipulate, and reform the literary text.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - It may be correct to say that interpretive questions are a relatively late development in digital literary studies (a tradition that Earhart traces to the impact of the World Wide Web in the 1990s). But the interpretive questions posed by the scholars she mentions in this passage are much older than the web. Quantitative interpretation of literature is a story that stretches back through book history, sociology, and linguistics to a range of nineteenth-century experiments. This tradition is a branch of the digital humanities only in a parochial sense – as we might call pizza a branch of American cuisine. Both things were imported from a different social context, and inherit a longer history of their own.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - Integrating experimental inquiry in the humanities poses rhetorical and social challenges that are quite distinct from the challenges of integrating digital media. It seems desirable — even likely — that distant readers and digital humanists will coexist productively. But that compatibility cannot be taken for granted, as if the two projects were self-evidently versions of the same thing. They are not, and the institutional forms of their coexistence still need to be negotiated.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - But that would be a very long view: it doesnt do much to help us understand current scholarly debate. For that, we need a tighter frame — a frame that can characterize the goals that have energized empirical approaches to literary history over the last half-century or so, without reducing them to an expression of twenty-first-century technology. This essay will provide an account on that intermediate scale. The frame I have chosen to use is the phrase distant reading. But I want to make clear from the outset that this phrase is not inevitable; there are other valid options. Andrew Goldstone observes that distant reading tends to foreground textual interpretation (reading) at the expense of questions about social structure. James F. English has shown that a similar account can be organized instead around the phrase sociology of literature. “Cultural analytics could be an equally valid choice, if we wanted to include disciplines other than literary studies. In short, like most historical phenomena, the trend I am describing is composed of multiple overlapping impulses. There is more than one right way to describe it.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - I have chosen distant reading because the phrase underlines the macroscopic scale of recent literary-historical experiments, without narrowly specifying theoretical presuppositions, methods, or objects of analysis. Although I understand Goldstones concerns about the word, I think we are free to interpret reading as an inquiry about social structures as well as literary forms. Distant reading also has the crucial advantage of being vivid, memorable, and less bristly than any of the alternatives that end in “mining” or “analysis.” On the other hand, it does have one significant disadvantage: it is often understood to imply a recent origin story that would prevent us from crediting any work done in the previous century. I will need to complicate that story in the pages that follow. It is true that Franco Moretti coined distant reading around the year 2000. But although Moretti is an important scholar in the tradition I will be tracing, there is no reason to treat his invention of the phrase as an originating moment for the whole tradition. Distant reading was not coined to describe a radically new method. The first occurrence of the phrase, in Conjectures on World Literature, seems in fact to describe the familiar scholarly activity of aggregating and summarizing previous research. Distant reading has evolved into a name for a more specific approach to literary history, but the approach described significantly predates this particular name for it.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - Morettis turn-of-the-century works were important, not because they invented the idea of macroscopic literary inquiry, but because they galvanized an existing project by infusing it with a new sense of possibility and a new polemical rationale. I will have more to say about his contribution, but this essay will mostly take aim at a larger target – a critical tradition, emerging in the later twentieth century, that would include things originally called book history or sociology of literature, as well as more recent, emphatically quantitative experiments. The common denominator that links all these projects is simply that they pose broad historical questions about literature, and answer them by studying samples of social or textual evidence. Those samples may range from a few dozen instances to a million or more. Instead of prescribing a particular mode or scale of representation, I want to highlight the underlying project of experimenting on samples, and the premise that samples of the literary past have to be constructed rather than passively received.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - This premise is general enough to have cropped up many times, so the tradition I am describing will lack crisp boundaries. Many traditional works of literary history pause at the outset to construct an informal sample of, say, Gothic novels. To the extent that those studies separate the construction of the sample from the process of historical inference, I would say they are approximating distant reading. Since versions of this approach to literature can be traced back to the nineteenth century, it would be pointless to go looking for a moment of origin. The emergence of distant reading was not contained in any eureka moment when a literary scholar decided to try social-scientific methods. It emerged rather through a long sequence of attempts, which gradually transformed casual historiographic practices into an explicitly experimental method.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - A longer study might follow this story down many different paths. Marxist literary theory has been one crucial influence; Raymond Williams might deserve a chapter of his own. The books he wrote around 1960 laid a theoretical foundation that still underpins much contemporary research — for instance, by insisting that literary culture is never a unified object, but rather a palimpsest of emergent and residual formations, transformed retrospectively by processes of selection. After reading Williams, it becomes hard to imagine that there could ever be a single definition of literary exemplarity, or a single correct sample of the literary past. In The Long Revolution, Williams also intriguingly foreshadows contemporary distant reading by grappling with a longue durée, and by emphasizing our ignorance about the past: nobody really knows the nineteenth-century novel; nobody has read, or could have read, all its examples, over the whole range from printed volumes to penny serials.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - A full account of the emergence of distant reading might also spend a chapter on book history. Book historians have been compelled to explicitly define samples, since libraries dont cover the full range of practices they study. Book historians have also pushed literary history to define its object of study more concretely — separating processes of production, for instance, from circulation and reading practices. But these parts of the story are already well known. In this limited space, I need to skip forward to a later stage of development, when the theoretical premises developed in book history and Marxist literary theory started to combine with an experimental method drawn from the social sciences. One excellent example of this fusion can be found in Janice Radways Reading the Romance.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - This book became a monument of feminist scholarship by challenging the widespread premise that popular literature simply transmitted ideology. In Radways view, critics had too quickly extrapolated their own interpretive practices to other readers. A critic may pick up a popular romance, for instance, identify the gender norms that seem implicit in the plot, and conclude that the effect of the book is to reinforce those norms. But how much does this tell us about the actual experience of romance readers? What aspects of the stories do they value? What role do the books play in their lives? Studying a community of women linked by a particular bookstore, Radway concluded that readers have more control over the meaning of stories than critics assume. Romances seemed to function in practice as a declaration of independence from the pressure of these readers responsibilities as wives and mothers, even when the gender roles represented in the narrative were traditional. Many subsequent arguments about the active agency of reception in fan culture are indebted to Radways conclusions.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - Radways quantitative methods may at first seem remote from familiar examples of distant reading. She doesnt discuss algorithms. Instead she uses numbers simply to count and compare — in order to ask, for instance, which elements of a romance novel are most valued by readers. Recent examples of distant reading can grow more complex than this. But they can also remain just as simple. Franco Moretti has relied on bibliographies to measure the lifespans of genres; I have quizzed readers about their impressions of elapsed time in ninety novels.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - Admittedly, contemporary distant reading is usually based on textual evidence, or on social evidence about dead people, rather than questionnaires. Distant readers are certainly concerned with reception. But it is hard to find living witnesses to interview when youre studying the longue durée, so few distant readers have characterized reception quite as richly as Reading the Romance. These are significant differences. But the central research practice I want to highlight is broad enough to encompass all these different kinds of evidence. It is simply that Radway separates the question she is posing from the evidence she gathers to address it, and from the conclusion she finally draws. Moreover, she organizes these aspects of the research process sequentially. In short, Radways book is designed as an experiment. It is admittedly an observational experiment: Radway isnt measuring the consequences of an intervention, and she doesnt express her reasoning in strict hypothetico-deductive form. Instead, she proceeds ethnographically, allowing herself to pause and comment when she sees an interesting detail. She is, after all, exploring a new research area, and encountering problems that are not yet formally defined. But Reading the Romance is still at bottom empirical research which aims to test the validity of ... a hypothesis. Because Radways voice is candid and engaging, the book may not always sound like social science. But the whole rhetorical performance has been organized around a scrupulous attempt to avoid confirmation bias. That is the point of using a clearly-defined sample of readers and novels, rather than casually adducing quotations and anecdotes that happen to fit a thesis defined in advance.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - Radways doctorate was in American Studies; she currently teaches in a department of Communication Studies. But other social-scientific traditions are also hovering in the background of Reading the Romance. Its questionnaires and interviews echo the methods of sociology. And when Radway looks at the romance novels themselves, her methods echo both sociology and structural anthropology. Reading systematically through a sample of twenty romance novels, she finds, for instance, a set of binary oppositions organizing the heroine, the female foil, the hero, and the male foil into a symmetrical structure. The plusses and minuses she uses to represent polarity in this structure recall Claude Lévi-Strausss diagrams in The Savage Mind. But her technique of systematically sampling and coding features of each novel also echoes the technique of content analysis that sociologists have applied to mass media.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - Linguistics was not particularly central to Radways project, and it may be worth pausing for a moment to underline this point. Contemporary distant reading has also been shaped by a different intellectual tradition devoted to quantitative analysis of linguistic detail. That tradition has made vital contributions, which I want to acknowledge. But I think linguistics may be looming a little too large in the foreground of contemporary narratives about distant reading, so much that it blocks our view of other things. Linguistic categories are just as important as the social categories Radway explored; its not that I want to champion one subject against the other. Rather, I think we need to see both influences at once in order to grasp the generality of the method that organizes this research agenda. Our knowledge about large-scale literary history isnt expanding because there was a special magic in linguistic analysis (or a special moral authority in feminist sociology). The project is succeeding, rather, because scholars have learned how to test broad literary-historical hypotheses in a way that resists confirmation bias. Otherwise it would be very difficult to make progress at this scale. If youre working in a domain where you could potentially cite 100,000 different novels as evidence, confirmation bias will make all generalizations equally true until you invent some procedure to limit your own freedom of selection. As psychologists have expressed this: fields with abundant evidence need some way to limit researcher degrees of freedom.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - Although Radways book was widely celebrated and widely cited in English departments throughout the 1990s, it was not widely imitated there. As James F. English has pointed out, literary scholars are traditionally quick to borrow social scientists conclusions, but slow to borrow their methods. We might justify our hesitation in various ways, but it is rooted, practically, in institutional inertia: literary curricula simply do not teach graduate students how to do content analysis or manipulate numbers. There are, however, a few cases where literary scholarship has developed along the lines suggested in Reading the Romance — including, notably, a scholar strongly associated with distant reading. In The Slaughterhouse of Literature, Franco Moretti developed a coding scheme to describe the role of clues in detective fiction. He then read a sample of about twenty stories, taking notes on the presence or absence of each aspect of clues in order to sort the stories into a tree.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - This method is very close to Radways approach to romance novels: from the sample of twenty texts, to the plan of reading systematically for particular features, to the little plusses and minuses that represent polarity in the diagram. I dont mean to suggest that Moretti was specifically influenced by Reading the Romance. It is more likely that both scholars drew their methods directly from structural anthropology and sociology. But whether the lines of influence run through literary criticism, or through social science, there is a coherent tradition to be traced here. Moretti adds an evolutionary hypothesis that is missing in Radway, and this may have been the aspect of his argument that most strongly shocked and provoked readers in the year 2000. But from the perspective of the present day, I think we can see that Morettis evolutionary hypothesis was no more decisive than Radways reliance on questionnaires. The crucial underlying similarity between these works, which has made both of them durably productive models for other scholars, is simply the decision to organize critical inquiry as an experiment.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - AI can seem like a spectral force — as disembodied computation—but these systems are anything but abstract. They are physical infrastructures that are reshaping the Earth, while simultaneously shifting how the world is seen and understood.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - To experiment on the past admittedly stretches the definition of experiment beyond its ordinary association with beakers and prisms. We cannot intervene in the past and then ask whether it changed as our hypothesis predicted. But this is a problem shared by geologists, astronomers, and computer scientists who run experiments on fixed datasets. Distant reading is a historical science, and it will need to draw on something like Carol Clelands definition of scientific method, which embraces not only future-oriented interventions, but any systematic test that seeks to protect the hypothesis from misleading confirmations. Literary historians can minimize misleading confirmations, for instance, by framing testable hypotheses about a sample of texts that are selected before the researcher settles on a conclusion. In calling this approach minimally scientific, I dont mean to imply that we must suddenly adopt all the mores of chemists, or even psychologists. Imaginative literature matters because readers enjoy it; criticism would gain nothing if we let meticulous hypothesis-testing drain all the warmth and flexibility from our writing. Literary historians who use numbers will have to somehow combine rigor with simplicity, and prune back a thicket of fiddly details that would be fatal to our reason for caring about the subject. But within those rhetorical limits, distant reading can, let us say, aspire to the methods of social science: it is defined not only by a commitment to historical breadth, but by a version of the scientific method appropriate for a historical discipline.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - Of course, not everyone will agree with this definition. For many scholars, the term distant reading is still shaped by the polemical context that surrounded it circa 2000, when it seemed to be the culmination of a long argument over the canon. The process of canon-revision that began by addressing imbalances of race and gender had evolved, by the late 1990s, into a more systematic expansion that sought to recover a larger great unread. Although the political implications of this project had become increasingly diffuse, it still retained some of the moral fervor of the canon wars. Readers of Morettis early experiments on large collections were accordingly tempted to interpret them as a normative argument that the only valid sample of literature is the largest possible one. This is not a position that his articles affirm systematically, but they sometimes leave themselves open to that interpretation. The decision to characterize the archive of forgotten books as a slaughterhouse of literature, for instance, echoes the moral pathos associated with a mission of recovery. I dont think the normative force of that pathos has turned out to be the most durable and influential part of the project. But its the part that readers were primed to pay attention to, and thus the part they often remember.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - Morettis insistence on reconstructing a maximally complete archive is also the part of distant reading that scholars have spent most time debating. Many critics have pointed out that it is impossible to recover everything. From that indisputable premise, they sometimes infer (more disputably) that comprehensiveness is not even an appropriate goal. I wont rehearse the debate here; it seems to me a waste of energy, since there are many valid ways to represent the past. Scholars interested in literary production may want to approximate completeness, while scholars interested in reception prefer to focus on a subset of influential works. Some social questions hinge on the demographic identity of writers, others on readers — while for other modes of moral engagement with human history, the whole issue of synchronic social breadth is less urgent than diachronic scope. All of these sampling strategies have their uses, and there is no reason to make a final choice between them. The legacy of the canon wars has perhaps made literary scholars a little too eager to force such a choice. Having seen many premature arguments on the topic, I try not to join any debate about the representativeness of different samples until I have seen some evidence that the debate makes a difference to the historical question under discussion. Considering more than one sample can be worthwhile, but samples are provisional, purpose-built things. They are not canons. It makes no sense to argue about their representativeness in the abstract, before a question is defined. Moreover, it often turns out that the same patterns are visible, whether you look at ten thousand obscure texts, or at two hundred lovingly curated editions. So it would be a mistake to stall out at the initial stage of research, in an argument about what constitutes an historically relevant and justifiable sample for analysis. That question has no right answer. We will get much further if we defer the argument, and start instead by comparing different samples.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - I have been at pains to downplay several aspects of Morettis contribution to distant reading that are often seen as definitional: his coinage of the phrase itself, and his emphasis on comprehensive samples that include many non-canonical works. However, I do think Moretti is rightly credited with sparking the twenty-first-century expansion of this research project. To illustrate why, I cant do better than quote the last paragraph of Slaughterhouse:
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - Fantastic opportunity, this uncharted expanse of literature, with room for the most varied approaches, and for a truly collective effort, like literary history has never seen. Great chance, great challenge — which calls for a maximum of methodological boldness: since no one knows what knowledge will mean in literary studies ten years from now, our best chance lies in the radical diversity of intellectual positions, and in their completely candid, outspoken competition. Anarchy. Not diplomacy, not compromises, not winks at every powerful academic lobby, not taboos. Anarchy.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - Two contributions are vital here. First, the recognition that literary history is not an exhausted, well-mapped field, but an uncharted expanse,” because we actually know little about its macroscopic shape. When I say that Moretti galvanized distant reading by infusing it with a new sense of possibility, this is the primary thing I mean. But I would also emphasize, secondly, his inference that the diplomatic reconciliation of conflicting normative claims is less urgent than many literary scholars assume.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - Thats why I have written this article — to tease out the elided social-scientific genealogy behind distant reading. There are other threads one could trace. For instance, as I have acknowledged, machine learning is exerting a powerful influence on the contemporary scene. I dont want to disparage any subfield, but I do want to insist that the genealogy of distant reading should be traced by disentangling its central intellectual impulses, not just by following the zone of overlap between computers and textual study as far back as possible. Roberto Busas concordance of Aquinas was a valuable thing, but a concordance of a single author does not constitute an important origin moment for distant reading. If we wanted to trace this tradition back to the middle of the twentieth century, we would need to follow different threads in several different directions. We might end up asking what Raymond Williams was doing with literature in the late 1950s, what Claude Lévi-Strauss was doing at the same time with social anthropology, and what Frank Rosenblatt was doing with the perceptron.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - Here we reach a zone of persistent miscommunication between distant readers and their colleagues. The discipline of literary studies has long organized itself around prescriptive debates that seek to define the proper concern of a literary critic. We inherit this polemical emphasis from nineteenth-century criticism, and it survives today in vigorous arguments that pit history against form, surface against depth, and critique against appreciation. Scholars rooted in this tradition understandably want to interpret distant reading as a normative stance of the same kind. Perhaps distant readers are expressing a principled opposition to, say, close reading? In that case, the natural next move would be to dialectically sublate the tension between close and distant. Observers are often quite willing to offer this sort of compromise solution. For literary critics, it is an obvious response. But from within the project of distant reading, it feels like a non sequitur. At bottom, distant readers are not arguing against close reading. Theyre just pointing to a blank space on our map of the past — where questions about large samples or long timelines might be located — in order to say none of us really know whats in there yet. A confession of ignorance isnt something one can meaningfully strike compromises about; it calls for a different genre of response. Instead of interpreting distant reading as a normative argument about the discipline, it would be better to judge it simply by asking whether the blind spot it identified is turning out to contain anything interesting.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - I am of course a biased observer. But personally, I became confident that new scales of inquiry were paying off in 2012, when Ryan Heuser and Long Le-Khac published evidence of a massive, steady shift from abstraction to concrete description in nineteenth-century novels. In subsequent years, distant readers have grappled with social questions about money, gender, race, geography, and literary circulation, as well as formal questions about genre, plot, emotion, and time. Some of these publications are still working their way through the press; in many cases, scholars are still struggling to reach consensus about the meaning of the evidence they uncovered. For instance, the shift toward concreteness discovered in fiction by Heuser and Le-Khac has alternatively been described as a broader parting of the ways between literary and nonliterary language, affecting poetry and nonfiction as well as the novel. If all of these discoveries are things we already knew in a tacit or unconscious way — as skeptics sometimes suggest — then our unconscious must have known so many conflicting things that the verb know seems oddly generous. Consensus about new evidence emerges very slowly: inventing an air-pump doesnt immediately convince readers that vacuums exist. So wariness about particular conclusions is definitely still warranted. But at this point, there is no doubt in my mind that literary scholarship turned out to have a blind spot. Many important patterns in literary history are still poorly understood, because they werent easily grasped at the scale of individual reading.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - Up to this point, I have said relatively little about numbers, and nothing at all about computers. I have characterized distant reading as a tradition continuous with earlier forms of macroscopic literary history, distinguished only by an increasingly experimental method, organized by samples and hypotheses that get defined before conclusions are drawn. The interdisciplinary connections that mattered most for this tradition were, until recently, located in the social rather than computational sciences.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - However, it is true that this sociological approach to literature has, over the last twenty-five years, fused with a computational tradition. The history of that fusion is complex, and I wont try to detail it fully here; one could point to Mark Olsen and the ARTFL project at Chicago, or to Matthew Jockers and the Stanford Literary Lab, or to John Unsworth and an archipelago of people involved with the MONK Project. In any case, it is clear that large-scale literary history is now suffused with ideas drawn from corpus linguistics, information retrieval, and machine learning. I dont intend to downplay the significance of this fusion; it has been the most exciting part of my career, and Im indebted to everyone I just mentioned.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - Nor do I want to suggest that computation was merely a means to achieve an end that Radway and Moretti had already fully defined. Critics of digital humanities often assume that computer science ought to remain merely instrumental for humanists; it should never challenge our fundamental standards or procedures. This misunderstands the place of computational disciplines in intellectual history. The value of computation is not merely to accelerate literary research or expand its scale; on the contrary, ideas drawn from computer science have given literary scholars new questions, and have encouraged us to frame existing questions in a more explicitly theorized way. Machine learning, for instance, represents a new way of thinking about literary concepts, like genre, that may be organized around loose family resemblances rather than crisp definitions.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - In short, I am not at all motivated to shore up disciplinary boundaries or insist on a strictly internalist history of literary studies. And yet I have to admit that, for me, distant reading remains the name of an approach to literary history rather than a computational method. To be sure, it has multiple genealogies, and roots in many disciplines. But in tracing connections to the past I would still, on the whole, emphasize the thread that runs back through Moretti, Radway, and Williams. My rationale is simple. An approach to literature informed by social science can produce significant historical results by itself — with or without computers. But the converse has not generally turned out to be true. Computational methods, by themselves and without a social scale of inquiry, have not been enough to transform literary history.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - In the twenty-first century, admittedly, these disciplinary stories are tending to converge and fuse. That creates an exciting challenge, but also a problem for graduate training. Scholars preparing to work as distant readers probably need some exposure to programming, social theory, and statistics, as well as fairly deep knowledge of a literary-historical tradition. Right now, the flexible interdisciplinary community called digital humanities” may be the best home for students trying to combine these modes of preparation.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - Table 4 examines a different set of data during the same slices of time. It focuses on the choronyms” attributed in an epitaph to the deceased and to affines of the deceased (usually the spouse or sons-in-law). The choronym” was a pre-Tang prefecture name that a great family claimed as its ancestral place of origin. By early Tang times, prefectures had all been renamed, such that choronyms became anachronistic place names usually easily distinguishable from other toponyms. Customarily, members of a great family preceded their surname with a choronym in order to establish the distinction of their ancestry.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - We know this, to be quite blunt, because computational methods were applied to literature for thirty years without making a great impact on the discipline. The journal Computers and the Humanities was founded in 1966. It became the center of an ambitious intellectual community, making important contributions to phonology and concordance-building, database design and the teaching of language. But the whole project made very little difference for literary history. Stanley Fish observed as much in the 1970s, and Mark Olsen couldnt really disagree, writing in the pages of the journal itself in 1993: Computer-aided literature studies have failed to have a significant impact on the field as a whole. According to Olsen, the mistake lay in trying to explain how a text achieves its literary effect by examining subtle semantic or grammatical structures in single texts or the works of individual authors. Computers had turned out to be very poorly suited to those New Critical questions, and concentration on them had tended to discourage researchers from using the tool to ask questions to which it is better adapted, the examination of large amounts of simple linguistic features. The irony, Olsen goes on to say, is that this broader and simpler kind of text processing is exactly what recent developments in literary theory and semiotics would seem to demand. (He cites Roland Barthes, Michel Foucault, and M. A. K. Halliday.) If only these two branches of research could be connected, computational analysis might finally assume a central role in literary studies.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - This was the article that originally pulled me toward distant reading in the mid-1990s. I still find it a prescient argument. One of Olsens strengths is that he ignores the false opposition between allowing our research to be shaped by properly literary questions, and allowing it to be guided by the capacities of digital tools. Instead, he considers both aspects of the landscape at once, and highlights a zone of intersection, where new literary questions happen to overlap with new technical opportunities. That zone of intersection turned out to be extremely productive, and Olsens prophecies have almost all come true. With the important (but isolated) exception of authorship attribution, computers still contribute relatively little to our understanding of individual texts and authors. But computational methods now matter deeply for literary history, because they can be applied to large digital libraries, guided by a theoretical framework that tells us how to pose meaningful questions on a social scale. Olsens article may overlook some scholars who were already moving in the direction he recommended. And the framework we use today may be more sociological, and less semiotic, than Olsen predicted. But as crystal balls go, his 1993 article isnt bad. It explains at once how the tradition embodied in Computers and the Humanities could eventually become significant for literary history, and why that significance wasnt for the most part achieved until the twenty-first century.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - Moreover, Olsens remarks are still a useful warning for scholars working in the area of overlap between digital humanities and distant reading. Algorithms are genuinely important; they arent merely instrumental. But they also arent sufficient for this project. So far, computation has only made a difference for literary history in combination with reasonably broad samples aimed at historical questions. A broad sample does not have to be an exhaustive collection; it might only amount to a few dozen books. But framing questions about dozens of books still tends to require a complete rethinking of received research questions. So I understand why scholars are often tempted to start with the algorithms instead, hoping that they will produce something interesting when applied to familiar author-sized questions. Unfortunately, in my experience, this is false economy. Olsens warning has not been superseded by any technical advance: computers still cant teach us much about New Criticism. (Maybe someday, but not quite yet.) Within the sprawling ecumenical community called digital humanities, it can be impolitic to insist on this barrier to easy assimilation of digital methods. But I make a point of distinguishing distant reading from digital humanities partly in order to signpost the problem: using computation, and reframing the scale of literary inquiry, are two distinct things. The first will not give you the results of the second.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - Writing in Computers and the Humanities, Olsen was naturally inclined to tell a story whose central characters were humanists and computers. He acknowledged the relevance of social science, but didnt foreground its methods. The same thing can be said about much contemporary work in distant reading. The best distant readers do in practice approach their projects as experiments. (We dont wander around aimlessly counting things.) But the experimental structure of our research is not always foregrounded when we write it up for publication. An article organized in social-scientific fashion (methods, then results, then conclusions) might not be warmly received by an audience of literary critics who are used to rhetorical panache. It can be more effective to pretend that your work grew in a casually discursive, thesis-driven way, and then happened to be illustrated with some scatterplots you had lying around.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - Im as guilty as anyone of striking this casual pose. It is often unavoidable. I have suggested that distant readers aspire to a version of the scientific method appropriate for a historical discipline. But we are also literary critics, and critics have an obligation to be interesting. This means that we sometimes have to tuck methods in an appendix, or make the analytical task look a bit easier than it truly was. On the whole, I accept this rhetorical double-bind as a consequence of our location on a tricky disciplinary boundary. But it does have the side-effect of obscuring an engine that powers the project. Readers can see why broad historical questions matter, and they can see the role of computers. But the value of an explicitly experimental approach can be hard to discern: distant readers have an incentive to play down that part of our work. And yet, the experimental framing of research questions is really the key to this field. Great work can still be done with Janice Radways quantitative methods, which require little more than paper and pencil. On the other hand, its very hard to do social research at scale without imitating Radways explicitness about hypotheses, samples, and results.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - Unfortunately, social-scientific methodology has not been a central subject of conversation in digital humanities, or in the forms of distant reading that cluster under the DH rubric. Andrew Goldstone is right that the word reading itself contributes to the elision of social science. But the recent tendency to treat distant reading as a subfield of digital humanities may also play a role. The term digital humanities stages intellectual life as a dialogue between humanists and machines. Instead of explicitly foregrounding experimental methods, it underlines a boundary between the humanities and social science.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - But if these two projects are to coexist under one roof, the differences between them need candid discussion. Digital humanists dont necessarily share distant readers admiration for social science. On the contrary, they are often concerned to defend a boundary between quantitative social science and humane reflection. If DH is unified at all, it is unified by reflection on digital technology, in a mood that ranges from playful exploration to monitory critique. Distant reading, on the other hand, is not primarily concerned with technology at all: it centers on a social-scientific approach to the literary past. This tension sets up a predictable conflict, which has already begun to unfold. Introductory courses and workshops in DH rarely teach students what they need to know in order to practice distant reading. So distant readers will have to agitate for a different kind of curriculum, with more emphasis on quantitative methods. The agitation is already underway, but in a conversation framed by the adjective digital, it can easily be misinterpreted as an attempt to push digital humanists in a more technical direction. For instance, distant readers interest in broad historical questions — which runs back at least to Raymond Williams — is widely conflated with the recent technical buzzword, big data. Conflations of that kind could begin to create an unproductive debate, where parties to the debate fail to grasp the reason for disagreement, because they misunderstand each others real positions and commitments.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - This article has tried to clarify the commitments that define distant reading. I have not aimed to produce consensus: I know that many scholars cited here will disagree with my definition of the field. In particular, I know that many scholars maintain strong ties to both digital humanities and distant reading, and I expect people who do both things will resist the conclusion that these are intellectually distinct projects. Certainly, the projects are at present fused, in ways that matter deeply to academics as human beings. For instance, job advertisements usually call for a digital humanist — almost never for a distant reader.” So it is pragmatically unwise for junior scholars to separate the two terms, and a purely descriptive account of the contemporary social scene might well fold them together. This essay has separated digital tools from experimental methods for reasons that are not purely pragmatic or descriptive. I have tried to ground the separation in a genealogical narrative, but I would also admit that it has a forward-looking prescriptive purpose.
[Author: Ted Underwood; From essay:"A Genealogy of Distant Reading "] - Over the last fifteen years, as distant readers have seized technological opportunities, the goals of the project have become diffuse. Often our immediate goal has really been exploratory: lets see what can be done with these tools. The exploration has been fruitful, but I think the field is ready to move past exploration. Large-scale literary history could now reorganize itself around clear research questions and rigorously advance our knowledge of the past. But in order to do that, I believe we need to set fascination with technology to one side and rediscover the guiding principle of experiment. I have defended that opinion by pointing to the history of the field, and especially to the importance of social science in Williams, Radway, and Moretti. But it is also, in the end, an opinion. This essay promises only a genealogy of distant reading. There will be other versions of the story, and I look forward to reading them.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - The effectiveness of both aerial photography and geographic information systems (GIS) in archaeological research has been well demonstrated in recent decades. Used in combination these tools represent a powerful methodological advance that adds depth and function to research on archaeological sites and their context in the larger landscape. The use of historical imagery from aerial or satellite platforms further provides a window through which we may view archaeological landscapes that have been lost to urban development. The present study introduces the previously-unexplored use of historical imagery and GIS tools in an effort to reconstruct the archaeological landscape of the Jian region of northeastern China, an area that has seen rapid urban development in recent decades at the expense of the rich archaeological record that had survived until the twentieth century. Jian served as the capital of the ancient kingdom of Koguryo (first century BCE to 668 CE), and the abundant archaeological remains of that kingdom reflect the historical phase in which the polity developed into a powerful expansive state. Although most of the surviving archaeological features of Ji an are now registered as protected cultural resources, urban development from the mid-twentieth century resulted in the destruction of many above-ground Koguryõ remains, especially mounded tombs. This study illustrates how utilization of historical aerial imagery from 1950 to 1965 allows us to recreate the pre-development landscape of Jian and to recover much of the archaeological data that had been presumed lost. In the following I focus primarily on lost mortuary features that are here revealed for the first time, offering new archaeological information and suggesting ways in which these newly rediscovered features can be understood in context with more recent archaeological work conducted in Jian.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - The small Chinese town of Jian in Jilin Province sits on the north bank of the Yalu River overlooking North Korea. Until recent years the town had been relatively difficult to access, yet it has long drawn large numbers of tourists from other parts of China and abroad. This is largely due to the fact that Jian is home to a very large number of remains belonging to the ancient kingdom called Gaogouli in modern Chinese but better known in western scholarship through the modern Korean pronunciation of Koguryo (the name Korea can be traced back to Koguryo). At its height this kingdom, which existed from ca. first century BCE until its collapse in 668 CE, governed vast territories spanning what are today southern Manchuria, all of North Korea, and the northern part of South Korea. During its period of expansive development Koguryõs rulers were based at Jian, which served as the kingdoms capital from circa 200 CE until it was removed to Pyongyang in 427.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - Historical imagery of Jian is available in a variety of media and contexts and includes terrestrial, aerial, and satellite imagery. Some of the most useful examples of terrestrial imagery were produced by Japanese scholars roughly between 1910 and 1940. Such photographs reveal the features and landscape of Jian prior to the most destructive phase of urban development, but because their coverage is selective they are less useful for comprehensive analysis of the region.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - While mining to finance war is one of the most extreme cases of harmful extraction, most minerals are not sourced from direct war zones. This doesn’t mean, however, that they are free from human suffering and environmental destruction. The focus on conflict minerals, though important, has also been used to avert focus from the harms of mining writ large.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - Jians location in a broad river plain surrounded by trackless mountain terrain made it an ideal place for the Koguryo capital at a time when the kingdom was under almost constant threat by the Chinese or Xianbei rulers of the Liaodong region to the west. The need for defense evidently outweighed the disadvantages such a remote and isolated location would have presented for transportation and distribution of goods. Nevertheless, the kingdom thrived in this location for over two centuries despite two devastating attacks, in 244 and 342, by neighboring polities based in Liaodong, and about a dozen kings and their nobles were buried in monumental tombs that still impress visitors in Jian today. By the early fifth century Koguryos rulers, no longer requiring the defensive protection afforded by the Jian valley, shifted their focus southward and moved the capital city in 427 to modern Pyongyang, where it would remain until the kingdom fell in 668. During this late phase of Koguryõ history, Jian served as a subsidiary capital, and many of the large tomb mounds there date to this period.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - After the fall of Koguryo the Jian site continued to serve as an administrative base for a succession of states, though its importance never approached that which it enjoyed during the Koguryõ period. Jian disappears from recorded history after the Mongols occupied the region in the thirteenth century, but it is likely that it lay in ruins for centuries, serving as a place of settlement only for very small populations of farmers and hunters. Once the territories of the Korean state of Choson (1392-1910) had reached northward to the Yalu in the late fourteenth century we find sporadic references to the ancient remains there, though by this time any connection with Koguryo had been lost. Although the north bank of the Yalu lay just beyond Chosons territories, the ancient remains were easily visible from the opposite shore and several literati observing those remains from afar were even moved to record their impressions in verse. After the Manchus overran Ming China and established the Qing empire in the mid-seventeenth century, most of Manchuria, including Jian, was considered a sacred ancestral land and was placed off limits to settlement, which is probably the reason the Koguryo remains there survived relatively unmolested for so many centuries.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - Eventually, however, the Jian region was opened for development in the late nineteenth century as the easternmost part of the new Huairen administration, established in 1875 with its base in the modern town of Huanren. The new settlement at Jian was established within the walls of the ancient Koguryõ city, and with the discovery in Jian of the King Kwanggaeto stele around 1876, the sites association with Koguryo was rediscovered. Early scholarly attention to Jian, then called Tonggou (though officially named Jian in 1902), was virtually monopolized by Japanese in connection with Japans territorial designs on both Korea and Manchuria. The records and photographs preserved from the visits by Torii Ryūzo, Sekino Tadashi, Ikeuchi Hiroshi, and Fujita Ryōsaku, among others, remain an invaluable resource for the study of the archaeology of this region. Particularly important at this time was the publication of the two-volume Tsūkō (or Tonggou) by Ikeuchi Hiroshi and Umehara Sueji in 1938 and 1940.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - Jian underwent gradual development during the first half of the twentieth century, mostly under the Japanese-dominated Manchukuo state (1932-1945), during which time a railroad line was run through Jian, connecting central Manchuria with northern Korea (then under Japanese colonial occupation) via the town of Manpo on the south bank of the Yalu. During the early phase of the Korean War (1950–1953), Jian became one of the primary bases for the dispatch of Chinese troops into northern Korea, and it is likely that rapid development and preparation for war during 1950 and 1951 took a toll on the landscape as well as on the archaeological remains there. From the late 1950s and through the 1990s Chinese scholars engaged in archaeological studies of the Koguryõ remains in and around Jian, with a notable lull in scholarly activity during the Cultural Revolution (1966-1976). From the 1960s, however, urban development picked up pace, and a number of archaeological remains appear to have been destroyed or damaged at this time.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - In the early 2000s considerable resources and effort were devoted to intensive archaeological survey and excavation in Jian in preparation for an application to UNESCO for inscription of the remains as World Heritage sites, which was approved in July 2004. Several valuable publications emerged from this work, representing the most detailed analysis of the archaeological remains of Jian yet to appear, far surpassing Ikeuchis work of the late 1930s. Since 2004 urban development has continued and intensified, and Jian has become a thoroughly modern Chinese town with a population of nearly 220,000. Although most of the surviving Koguryõ remains have been designated as protected cultural resources, the effects of urban modernization have taken a toll on them, making the earlier studies and reports of these remains all the more important to research.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - Koguryõ remains in Jian fall into a number of categories, including walled sites, building and settlement features, monumental stelae, and mortuary remains. Of these, the last are by far the most numerous, and some of the tomb mounds were built on a massive scale. Tomb mounds are typically classified into two general types those constructed entirely of stone and those covered by a mound of earth-though both types are in turn divided into a number of subtypes based on specific construction. Stone-mounded tombs take the form of cairns of rock or river stone, some of which have one or more rectangular base platforms constructed in tiers with the cairn on top. Earlier stone tombs have burial chambers in the form of a shaft opening in the center of the top of the mound, while later ones have corridor-style side entrances leading to a tomb chamber. Earth-mounded tombs have corridor-style entrances and one or more burial chambers, some of which are richly decorated with painted murals. The stone-mounded tombs are known to be the earlier form of burial, the transition to earth-mounded tombs spanning the late-fourth to early-fifth centuries. There is no way to estimate how many Koguryo tombs in Jian survived to the early twentieth century, but they would have numbered in the thousands. Many of these, the stone tombs in particular, would have succumbed early to new settlement, as they would have been disassembled and utilized as building material for new dwellings, a destructive process that continued throughout the twentieth century.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - If we visit the primary sites of mineral extraction for computational systems, we find the repressed stories of acid-bleached rivers and deracinated landscapes and the extinction of plant and animal species that were once vital to the local ecology.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - Another process that proved destructive to tombs in Jian was the construction of roads and, especially, the railroad. The primary northern route into the Jian plain runs through a place called Tukouzi in the northeastern part of the plain. Early roads from the north entered Jian in this location through a saddle gap where the elevation between mountains dips to a low point. The main road proceeded southward from this location toward what was the village of Donggang (now part of Taiwang township), while another smaller northern route ran along the lower slopes of Yushan (Yu Mountain) through a particularly dense concentration of tomb mounds. The southern road to Donggang ran through an area largely free of tombs. Some tombs may have been cleared to make way for the northern road, but the early route seems to have wound around tomb clusters where possible. In the late 1930s a railroad was run through the plain roughly parallel to the northern road until it approached the old walled town, where it looped to run eastward in the lower part of the plain. The rail route required a broader clearance than did the older road, and it could not wind to avoid the tombs along its straight path.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - During his brief visit to Jian in 1938, the scholar Fujita Ryōsaku observed the destruction wrought upon the tombs by the railroad construction then underway, by which time hundreds of tombs had been destroyed. By the time the railroad construction was completed in 1939, many more tombs and other archaeological remains must have been lost. Fujita surveyed a number of surviving tombs and their environs and described them in his report.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - The first comprehensive survey of all tombs in the Jian region occurred in 1966 when over forty people were assigned to survey, classify, and register all surviving tombs over a period exceeding two months ending in June. At this time six distinct clusters of tombs were recognized in the area, and they were collectively referred to as the Donggou tomb cluster. These clusters were those at Yushanxia (usually referred to as the Yushan cluster), Shanchengxia, Wanbaoting, Qixingshan, Maxiangou, and Xiajiefang. The 1966 survey resulted in a complete list of 10,782 tombs, which were displayed on a 1:2000-scale distribution map. Because this survey ended just as the Cultural Revolution was gaining momentum, the results were rarely utilized over the next decade, though additional small-scale surveys and excavations took place occasionally. In 1976 and 1979 additional surveys were conducted to update the register and correct errors as well as to account for tombs lost due to development since 1966. By 1984 surviving tombs in Jian numbered 7160, indicating a loss of 3622 tombs since 1966.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - Over two seasons in 1984 and 1985 large-scale excavations and surveys were conducted on 113 tombs in the Yushan cluster prior to their destruction in preparation for the construction of a new road leading to town running immediately to the north of and paralleling the railroad. This replaced the road to Donggang (Taiwang township) as the main route into Jian. The lengthy report resulting from this work, representing the most comprehensive study of Koguryo tomb characteristics to that time, was published in 1993. In 1997 a second comprehensive survey was conducted on all tombs in the Donggou cluster with the intent to correct omissions and errors in earlier registers and to create a better classification system and distribution diagram. The new register and distribution diagram (the latter in the form of 100 maps in 1:2500 scale) were published in bound form in 2002, providing an invaluable resource for the study of the surviving tombs in Jian. This volume includes a table providing data for each tomb, including its registry number, structure type, and physical dimensions. Surviving tombs catalogued in this survey numbered 6854, showing that some 3928 tombs had been destroyed since the 1966 survey. The associated maps show the approximate location of the tombs (indicating in dashed outlines the locations of now-lost tombs that had been included in the 1966 survey) as well as the structure type. The base maps provide contour lines indicating elevation in 2 m increments and show the locations of zones then (1997) occupied by buildings or settlements. The topographical features reflected in these maps represent the situation in the late 1990s, by which time the landscape had undergone significant alteration, a process that accelerated in the early 2000s.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - In 2003 an application was submitted to UNESCO to have the more prominent Koguryõ remains in Jian inscribed collectively as a World Heritage site, in preparation for which modern structures in the vicinity of large tombs and walled sites were cleared away and efforts were made to protect and preserve the surviving remains. Surveys and excavations were conducted on these sites, the results of which were published the following year in three volumes. Today these remains are protected and are the focus of a thriving tourist industry in Jian. Although the publication of the archaeological data is invaluable for the study of these remains, there was little attempt made to describe the encroachment of modern development in their vicinity, which in some cases would have destroyed archaeological information. Based only on presently available materials, archaeologists would be unable to determine whether the absence of archaeological remains in the vicinity of the surviving features indicates that no such remains ever existed or that once-existing remains had been destroyed by modern development. That this is a real problem will be demonstrated shortly.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - As just described, the abundant above-ground remains of the Koguryo kingdom in Jian have been adversely affected by the increasing pace of modern urban development. Such development, along with historical circumstance and lack of documentation, has resulted in the loss of a great deal of information regarding the landscape and archaeological remains of Jian. Researchers appear to be resigned to the fact of this loss and generally consider the information to be un-recoverable. However, during the process of a larger-scale research project I am currently undertaking, I have found that a great deal of precise information regarding lost archaeological remains and the physical landscape of Jian can be recovered through the integrated use of various types of historical imagery in a Geographical Information Systems environment. Through these tools and resources I have been able to document the locations of hundreds of now-lost Koguryo tombs and other remains, including some for which there appear to be no records at all. The same process makes possible the recovery of landscape information that cannot now be observed due to the reworking of land features through quarrying, the building of embankments, or the covering and rerouting of natural drainage channels. All of this recovered information can be of enormous use to the study of the archaeology and history of Jian, as I will shortly illustrate with a few examples.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - The majority of aerial imagery I have used is US reconnaissance photography dating to the early phase of the Korean War (primarily late-1950 and 1951) and consists of both vertical and oblique panchromatic imagery of Jian and its vicinity. Such images vary greatly in quality and coverage, but they are an invaluable resource as they provide very broad and continuous spatial coverage and reveal the features of Jian still largely unspoiled by development. Another type of aerial imagery that has proved to be exceedingly useful is declassified US reconnaissance imagery from U-2 overflights of China and North Korea dating from 1962 to 1965. Photographs from this context are useful in that the majority of missions (those using the Hycon Model B camera set to Mode I operation) provide very high resolution and horizon-to-horizon overlapping spatial coverage and that they typically present oblique views of Jian, which provide a helpful perspective in combination with the Korean War imagery, which offers mostly vertical views of Jian. To my knowledge, the aerial imagery from the Korean War and from U-2 overflights has not previously been extensively exploited for archaeological content in northeast Asia.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - The most useful satellite imagery is the declassified US reconnaissance photography from CORONA satellites. Although typically lower in resolution than the aerial imagery described above, panchromatic CORONA images capture a very large geographic area in a single frame on long negative strips measuring 2.25 in. in width and about 30 in. in length. Although the mechanics of the panoramic scanning camera result in consistent distortions across the imaged swath, increasing with distance from the photographic center, when a relatively small section of the total image is isolated, it can be treated as a single frame with only minor distortion. Further, when the area of interest is near the center of the full negative frame (i.e., near the nadir), the isolated section provides an image with minimal orthographic distortion. Such qualities make CORONA images especially useful as basemaps-once they are georeferenced based on recent orthorectified satellite images, they can be used as a basis for georeferencing the aerial imagery described above (this is often necessary due to the loss of recognizable landmarks visible in recent satellite images, making them unsuitable as a basis for georeferencing older aerial photographs).
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - Although such historical images have been available for some time, their utility in archaeological work had until recently been limited due to the lack of generally available GIS tools and to the fact that the images, CORONA imagery excepted, have been (and often continue to be) very difficult to access and to acquire in digital format. GIS is now a well-developed and expanding field, and its application in combination with overhead imagery in archaeological work has been abundantly demonstrated, including, most recently, the effective use of U-2 imagery in archaeology. In East Asian context, CORONA satellite imagery has been effectively utilized in archaeological studies in recent years in China. Rather less frequently, historical aerial images taken prior to the 1970s have also been used in Chinese, Japanese, and Korean archaeological research, sometimes in conjunction with historical and more recent space-borne imagery. Studies utilizing aerial imagery in China have naturally tended to focus more on Chinas central regions, as have archaeological studies in general. Certain of the peripheral areas, especially the border with North Korea, remain politically sensitive, limiting the possibilities of collecting new data from airborne remote sensing platforms. The most useful existing historical imagery for this border region is that produced by US overflights from the Korean War and by U-2 aircraft in the 1960s, yet this imagery, probably due to difficulty of access, has not to my knowledge been previously used for archaeological research, though such imagery often provides much better resolution than does that from CORONA.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - For the analysis that follows I have used ESRIs ArcMap application as a base for georeferencing historical imagery and maps, making it possible to create multiple overlays allowing one to observe a given location through numerous points in time. For the Jian region I have found it useful first to georeference a section of a CORONA satellite image from the 1960s using ESRIs orthorectified image basemap as a reference, as this provides a helpful bridge when seeking control points for earlier aerial images (as noted above, extensive urban development makes it difficult to locate landscape features common between current and early historical imagery without the use of a chronologically intermediate base image as represented by the CORONA layer). The intermediate layer being established, I then utilized it in tandem with the ESRI basemap to georeference individual aerial images. I also georeferenced a number of historical maps of the Jian region, including the individual tiles of the 1:2500-scale maps generated by the 1997 survey. These layered images permitted me to identify aboveground archaeological features, including many that have been lost and still others that appear to be otherwise unknown. I compiled these features in a database with geocoded references, and when possible and desirable I also created associated shapefiles representing the visible surface outlines of the features. In the following sections I will present a few representative examples of the usefulness of this method in recovering now-lost archaeological information that permits a deeper understanding of Koguryõ mortuary practices and the spatial composition of its capital city.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - Historical records indicate that at least ten Koguryõ rulers should have been buried in Jian, and I have elsewhere discussed the characteristics of Koguryõ elite burials. Of a number of characteristics, including tomb size, marked precincts, and the use of tiled-roof structures on top of the tombs, the single factor that seems to have been reserved for the burials of kings is the inclusion of what archaeologists in Jian have called the ritual altar (jitan). Though the specific function of this feature is not known, it takes the form of a long rectangular platform made of piled stones positioned some distance (10 to 50 m) to the east or northeast of the tomb itself, which is always of the stone-piled variety. There are some ten tombs in Jian for which ritual altar features have been clearly identified, and all of these tombs share other characteristics that seem to indicate royal interments, including their standing in isolation with no other tombs in their immediate vicinity.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - In Baotou, the largest city in Inner Mongolia, there is an artificial lake filled with toxic black mud. It reeks of sulfur and stretches as far as the eye can see, covering more than five and a half miles in diameter. The black lake contains more than 180 million tons of waste powder from ore processing. It was created by the waste runoff from the nearby Bayan Obo mines, which is estimated to contain almost 70 percent of the world’s reserves of rare earth minerals. It is the largest deposit of rare earth elements on the planet.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - In my previous study I pointed out two additional tombs, both in the Maxian section of the Donggou tomb cluster, that are candidates for the list of royal burials despite their presently lacking clear evidence of altar structures. The first, registered as JMM1000 and popularly called the Tomb of a Thousand Autumns, is in all respects certainly the tomb of a king of the late fourth century, but the excavations conducted in 2003 yielded no evidence of an altar structure. The second tomb, JMM2100, is a likely candidate for a slightly earlier kingly burial, but the area in which we would expect to find an altar feature has long been occupied by a modern road and a residential area. The analysis of historical imagery provides useful information that helps to resolve lingering uncertainties with regard to these two tombs.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - JMM1000 is a massive stone-mound tomb measuring 60 m to 70 m per side and presently standing some 11 m in height. Although it is now largely collapsed, it once contained a stone burial chamber decorated with inscribed bricks that gave it the popular name of Tomb of a Thousand Autumns. Excavations in 2003 yielded an inscribed tile fragment that included a date corresponding to 395 or, less likely, 407. On the basis of this tile inscription and supported by the expected chronology based on tomb construction style and roof tile morphology, the tomb is thought to be that of a king named Kogugyang, who died in 391 after a brief reign of seven years (construction of the massive tomb, presumably begun during the kings lifetime, was not completed until at least four years after his death). The apparent absence of an altar feature was the only factor that ran counter to my hypothesis that such altars are the necessary characteristic identifying kingly tombs. In his 1948 report on his observations in 1938, Fujita Ryōsaku noted the existence of some feature to the east of the tomb that he took to be a cluster of attendant tombs, which is how he described what are now known to be altar features associated with other tombs. The excavations in 2003, however, uncovered no evidence of such a feature, though the archaeologists would surely have expected to find one. During the course of my analysis of tomb JMM1000 I quickly resolved the problem of the missing altar and was moreover able to account for its later absence. Multiple images of the Maxian region from Korean War overflights provided clear evidence not only for a large altar feature but also for a smaller associated feature of an unknown nature. Further, a search of early terrestrial images of this tomb clearly show the northern part of the altar feature, though without the broader context provided by the later aerial imagery its nature would be easy to miss. Based on measurements of the georeferenced aerial images, the altar feature was located about 40 m to the east of the tomb (approximately where Fujita placed his attendant tombs) running parallel to the tombs east side. The feature is roughly rectangular, measuring about 30 m by 120 m, and appears to have been constructed of the same stone material as the tomb itself. Approximately 20 m to the south of the altar feature is another raised feature that is roughly square in aspect with rounded corners, measuring about 14 m per side. A straight line running from the northwest corner of the tomb and through the southeast corner would touch the southwestern corner of the smaller feature. Though this may be entirely coincidental, the overall arrangement recalls that of a slightly later tomb numbered JYM0001, located in the northeastern part of the Jian plain and popularly called the Tomb of the General.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - Though JYM0001 is much smaller in scale than JMM1000, it is built of finely worked stone in a pyramidal style, representing the most advanced stage of development of Koguryo stone-piled tombs. This tomb is thought to have been built in the early fifth century for King Changsu, who moved the capital to Pyongyang around 427 and was probably buried in that new capital upon his death in 491, leaving the tomb in Jian unused. To the northeast of JYM0001 is a ritual altar constructed of worked stone, to the southeast of which is an attendant tomb built in pyramidal style like the primary tomb but in a much smaller scale. The arrangement of main tomb, altar feature, and attendant tomb seen at JYM0001 very closely resembles that seen at JMM1000. If this is not mere coincidence, the smaller feature at the latter tomb may likewise have been an attendant tomb.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - The current absence of the ritual altar for the Tomb of a Thousand Autumns can now be easily explained through a look at later imagery. Aerial photographs from 1950 and 1951 show a number of small residential buildings just to the north of the altar feature, which appears to be fully intact. High resolution U-2 imagery from 1962 and 1963, however, reveals that the northeastern part of the altar had by then been occupied by additional buildings, while a road had been constructed along the western edge of the altar leading to a large building placed to the south of the altar next to the attendant tomb feature. Later U-2 imagery from 1965 shows that within a short time the entire altar feature had been replaced by a number of buildings, including a long structure built along what had been the northern half of the altar. The smaller feature to the south still remained, but the modern structures nearby appear to be expanding in its direction, and the entire area around the main tomb was being developed. Between 1965 and 2000 construction in this area continued, and when the area was cleared for the excavation in 2003, no trace of the altar feature remained for the archaeologists to find—it had been removed to make way for new construction, its stones probably reused for the same purpose. Although the above-ground portion of the smaller feature to the south appears to have been razed, it is possible that future excavations will reveal something of its nature.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - Historical imagery thus illustrates how the advance of urban development has resulted in the complete loss of large archaeological features in Jian, and in the same way it can provide useful information about now-lost features that aid in our understanding of surviving remains. In my previous study of Koguryõ royal tombs, I tentatively included JMM2100, lying about 750 m to the north of the Tomb of a Thousand Autumns, in the list of likely royal tombs. Although smaller in scale than most other tombs with altars, JMM2100 stands out from the typical stone-piled tomb due to its size (about 30 m per side), its structure, and the fact that it stands alone with no other tombs in its vicinity. Excavations yielded pottery fragments and tile ends allowing us to date it to the mid-fourth century, upon which basis I suggested that it might have been the tomb of King Sosurim, who ruled from 371 until his death in 384. There are, however, historical records suggesting that Sosurim may have been buried outside of the Jian region. This uncertainty, along with the fact that any ritual altar that may once have existed at this tomb would now lie under either the modern road or the residential buildings nearby, prompted me to consider the identification of JMM2100 as a royal tomb as only tentative. Evidence derived from historical imagery now creates even further doubt that this is the tomb of Sosurim.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - The tomb of the Great King and its walled precinct occupy the southernmost prominence of a low ridge stretching between two major streams, the eastern stream draining the southwestern slopes of Longshan mountain and the western one draining the southeastern slopes of Yushan. To the north of the tomb precinct is a slightly more elevated rise, which was once the site of four large stone-mounded tombs, as will be demonstrated below. Still farther to the north is the lower northern slope of the ridge, which was originally the site of a tight cluster of small stone-piled tombs. Somewhat farther northeastward are the sparse remains of another large tomb designated JYM0030 (also called the Great Tomb at Huangnigang). The mortuary remains in these four zones between the streams are of interest in that they may have been interrelated, as will be discussed below.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - Imagery from 1950 and 1951 show this tomb clearly. Although the road is already present and runs close to the eastern side of the tomb, the area beyond is open field and shows no indication of an altar feature (though it is possible that an existing feature might have been already covered over). More important, however, is the clear presence of a smaller but still substantial tomb lying a very short distance to the northwest of JMM2100. The 1997 register indicates that a stone-piled tomb designated JMM2101 had once existed in this general location but was completely destroyed by the time of that years survey. The published register, evidently based on data from the 1966 survey, suggests that the tomb was a commonly found type with a corridor-style entrance, a type of tomb that is usually quite small in scale. The imagery shows it to have been only about 7 m to the west of JMM2100, measuring about 20 m per side and similar in construction to its neighbor. Moreover, some 20 m to the north of JMM2101 is a much smaller and possibly damaged tomb (JMM2099), and still another (JMM2098) some 15 m farther to the northwest. Both of these tombs are listed in the 1997 register as destroyed and of the same general type as JMM2101, but the imagery shows that JMM2101 is a much larger tomb than the register would suggest. The compilers of the 1966 and 1997 registers may not have had enough information to describe these lost tombs in more detail, but the present instance illustrates why it is necessary to exercise caution when relying on the published 1997 register for information on lost tombs.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - The U-2 imagery from 1962 to 1965 shows that although JMM2101 then still existed next to JMM2100, it was already damaged and there were new buildings abutting it. We may assume that in the following years JMM2101 was completely destroyed, but there was enough of it remaining in 1966 for surveyors to include it in the registry made that year as a stone-piled tomb, the original scale of which was by then indeterminate. Although it is still possible that JMM2100 is a royal tomb, the lack of evidence for an altar feature and the presence of another large tomb in close proximity strongly suggest that this is an elite but non-royal tomb of the mid-fourth century.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - The one tomb in Jian that has never been interpreted as anything other than that of a king is JYM0541, popularly called the Tomb of the Great King on the basis of inscribed bricks found among its stones. This tomb sits on a prominent high ground in the eastern half of the Jian plain at the village formerly named Donggang, now part of the Taiwang township. In addition to the inscribed bricks, other characteristics suggestive of a royal tomb are its immense size (more than 60 m per side), a well-constructed burial chamber, evidence of rich burial goods, and clear traces of a wall marking a large tomb precinct with the massive stone-mounded tomb in its center. Excavations conducted in 2003 revealed the presence of a previously unknown pair of adjacent altar features lying some 40 m to the east of the tomb, as well as various additional building structures within the burial precinct. The walls of the precinct were known to have existed on the basis of early descriptions of the site, but in 2003 only portions of the eastern and southern walls were identified. The tomb itself dates to the early fifth century and is generally understood as belonging to a king called Kwanggaeto, who ruled from 391 to 413.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - Although decades of surveys and close examination have yielded a great deal of information about this tomb and its associated features, historical imagery adds important additional information that would otherwise now be lost. During the first half of the twentieth century several Japanese scholars produced descriptive reports on this tomb and its vicinity, one of the most detailed being that published by Fujita on the basis of his 1938 survey. In his report, Fujita included a rough layout sketch of the tomb vicinity, clearly showing the precinct walls, a long rectangular stone platform to the north of the tomb, which strongly suggests an altar feature, and a large stone-piled tomb immediately to the north of the tomb precinct wall. Interestingly, the rectangular platform described by Fujita, which is no longer extant, is not the altar feature discovered in the 2003 excavation, which lies to the east of the tomb and was already covered with a modern temple complex at the time of Fujitas survey and therefore does not appear in his report.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - Aerial imagery from the Korean War provides valuable coverage of this site, both vertical and oblique, showing it to have been little changed from the time of Fujitas visit a dozen years earlier. The precinct walls, constructed of mixed earth and stone, appear clearly and confirm Fujitas observations, and rectified and georeferenced imagery allows us to plot the location of this now-lost feature with considerable accuracy. Further, whereas Fujita was unable to discern the western portion of the precinct wall from ground level, the aerial imagery suggests the existence of a pair of parallel walls separated by about 10 m running along the slope to the west, though the exact nature of this feature is not clear. The imagery from the early 1950s shows that the piled-stone platform to the north of the tomb, already damaged at the time of Fujitas visit, had suffered further damage due to locals removing the stones to use as building materials. A quarry pit between the tomb and this platform appears to have caused further damage to the precinct, while the construction in the late 1930s of the railroad, which runs a few meters to the south of and parallel with the southern precinct wall, may have caused some damage to the tomb complex as well.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - As noted above, the Korea War imagery shows the area around the Tomb of the Great King to be largely unchanged from the time Fujita made his diagram. The U-2 images from 1962 to 1965, however, indicate extensive development in the intervening years. Numerous buildings had been constructed in the western part of the tomb precinct, modern structures encroaching close upon the tomb itself. The northern and eastern stretches of the precinct walls survived relatively intact as late as 1965, but they had been destroyed by the 1990s (based on my personal observations at that time). Just prior to the clearing of modern structures in preparation for the UNESCO application, the tomb precinct was virtually covered with buildings and other utilities. Although these structures were all removed prior to the excavations in 2003, much of the underlying landscape must have been damaged by the modern encroachments. This should be kept in mind when utilizing the results of the 2003 excavation.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Understanding how the text layer is constructed is thus critical for a scholar to understand how to thoughtfully use a database. Is it double-entry text? In theory, it will be nearly 100 per cent accurate. OCR from the 1990s? A scholar will need to do more contextual skimming. Unfortunately, the platform layer often inhibits this.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - The low hill immediately beyond the northern wall of the precinct (or rather what is left of it, as the northwestern part of the hill has been quarried away) is now densely covered with modern residential structures, but it was largely clear of settlement in 1950. In his narrative description of the region to the north of the walled precinct, Fujita notes the existence of five large stone-piled tombs (and two much smaller ones), though his diagram shows the placement of only the one closest to the precinct wall. These five large tombs appear clearly in the Korean War imagery, and the one lying closest to the northern precinct wall appears precisely where it is placed in Fujitas diagram. Of these five tombs only one survives today, though in a very poor state of preservation. This tomb is referred to in the 1997 survey as JYM0540, a large-scale tiered stone tomb with sides measuring an estimated 40 m each. The tomb had survived relatively intact until the 1960s, at which time the hill where it was situated became densely settled, with portions of the northern side of the hill extensively quarried by a nearby brick factory. Most of the tomb was presumably disassembled so that its materials could be reused for building homes, and what was left of the tomb underwent excavation in 2003, the report being published in 2009. Based on its structure and the remains of roof tiles, the tomb was judged to date to the early fifth century, making it roughly contemporary with the Tomb of the Great King, which sits about 250 m to its south.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - The 1997 survey maps show JYM0540 on the hill north of the Tomb of the Great King, though it is for some reason placed somewhat to the east of its actual location. Of the other tombs described by Fujita for this area, the 1997 survey notes only two, designated JYM0539 (destroyed) and JYM-0540 (damaged), placed to the west and southwest of JYM0540, respectively (tomb numbers preceded by a dash are used in the 1997 survey publication to indicate tombs that were not included in the 1966 survey). However, there appears to have been some confusion with regard to the identity and placement of these two tombs. The tomb designated JYM0539 was extant during the 1966 survey and underwent more focused survey and limited excavation in Spring of 1968, though the results of this work remain unpublished. The tomb designated JYM-0540 seems to be unrepresented in the 1966 survey and was in 1997 regarded as a newly discovered tomb. In fact, JYM0539 is placed precisely where the tomb now referred to as JYM0540 is located today, while JYM-0540 is placed precisely where another tomb, which I refer to below as 0540a, was once located. It seems likely, therefore, that the tomb today designated JYM0540 is actually the tomb labeled as JYM0539 in 1966 and 1968. The compilers of the 1997 survey data evidently misinterpreted data from the 1966 and 1968 surveys, though it is difficult to resolve this problem without direct reference to the records from those earlier surveys.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - Several aerial photographs from the early 1950s provide good coverage of the structures located on the hill to the north of the Tomb of the Great King. The five large stone tombs described by Fujita are readily discernible, as they appear as dark patches against the lighter topsoil. They are more or less aligned along the crest of the hill, and for the purposes of this study I have labeled them JYM0540 (retaining the designator from the 1997 survey) and JYM0540a through JYM0540d, based on increasing distance from the surviving tomb. For the sake of brevity, I will hereafter refer to the latter four tombs as Tomb A through Tomb D. The photographs indicate that JYM0540 actually measured about 30 m per side, somewhat smaller than the scale estimated in the 1997 survey but close to that resulting from the 2003 excavation. There is clear evidence of its tiered structure, and part of the southwestern third of the tomb appears to have been already removed for the construction of nearby residential buildings. The tomb occupied a prominent position near the summit of the hill.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - Tomb A, now completely destroyed, appears as a tomb equal in scale to JYM0540 (about 30 m per side) though placed at a slightly lower elevation, located just to the southwest of the latter tomb, a space of only about 18 m separating the two. There is, however, less evidence for a tiered structure for Tomb A, which appears to have been built lower in profile than its neighbor. Tomb B sits about 38 m to the north of JYM0540 and measured from 20 to 25 m per side, and the photographs show some evidence for a low tiered structure. Immediately to its northeast sits Tomb C, which appears to match the scale and structure of its neighbor, though its central section appears to have been dug out somewhat. A distance of about 14 m separates Tombs B and C, and in fact the relative placement of the four tombs just described suggest that JYM0540 and Tomb A were built as a pair, while Tombs B and C were similarly intended as an associated pair. The latter pair were constructed on the highest summit of the hill.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - Tomb D was located some distance from the other four, sitting just 10 m from the northern wall of the precinct for the Tomb of the Great King. The photographs show it precisely where Fujita placed it in his diagram. It sat about 130 m to the southwest of Tomb A, the intervening space showing no evidence of other tombs of a similar scale. It measured about 22 to 25 m per side, and there is some evidence of a low tiered structure. Its placement suggests some association with the royal tomb to its south, but this must remain speculative, as the smaller tomb appears to have been the first of the tombs on this hill to suffer complete destruction.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - Although the imagery from the early 1950s shows all of these five tombs as relatively intact, U-2 imagery from 1962 shows that Tomb D had been replaced by a quarry and modern buildings, while the other four tombs survived. By 1963 Tomb C appears to have succumbed completely to quarrying activity, and in 1965 Tomb B had been replaced by modern buildings and Tomb A, while still clearly evident, was already damaged by encroaching construction. We may assume that by the time of the survey in the next year, Tomb B had suffered further damage to the point where the surveyors described it only as a stone structure, probably of indeterminate scale. By the early 2000s that part of the hill that survived the quarrying activity had been completely covered with modern buildings, leaving only the central part of JYM0540 to be examined by the excavation team in 2003.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - In recent years, shipping vessels produced 3.1 percent of yearly global carbon dioxide emissions, more than the total produced by Germany. In order to minimize their internal costs, most container shipping companies use low-grade fuel in enormous quantities, which leads to increased amounts of airborne sulfur and other toxic substances. One container ship is estimated to emit as much pollution as fifty million cars, and sixty thousand deaths every year are attributed indirectly to cargo-ship-industry pollution.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - On the lower, northeastern slope of the hill, some 140 m distant from Tomb C, is a lower area of the hill that once was the site of a tight cluster of much smaller tombs distributed in a zone stretching about 200 m from the southwest to northeast and about 70 m from northwest to southeast. The 1997 survey publication records some 33 small tombs in this zone, three being earth-mounded stone-chamber types and the remainder stone-piled corridor-entry types. All are listed as destroyed in 1997, and the data for them were apparently drawn from the 1966 survey as well as two focused (but unpublished) surveys in 1968. Korean War imagery confirms the existence and placement of these tombs (along with what appear to be a few other tombs of similar style that may not have survived to 1966), but their small scale and the insufficient resolution of the imagery make it difficult to determine anything about their specific size, construction, or chronology. U-2 imagery reveals that the area occupied by the small tomb mounds has been developed as fields for planting and, on the southern edge of the zone, for residential structures, suggesting that many tombs might have been damaged or destroyed prior to the 1966 survey. Today the entire area has been covered with modern buildings or otherwise utilized for production.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - The hill occupying the space between the two streams thus appears to have been reserved for mortuary purposes during the late fourth and early fifth centuries, with the large stone-piled tombs occupying the highest central elevation, flanked by the royal tomb complex to the south and the cluster of small tomb mounds to the northeast. Still farther to the northwest another hill rises, and on its southern slope, about 200 m from the cluster of small mounds, sits the only other tomb known to have existed in the region between the two streams. This is the large stone-mounded JYM0030, otherwise called the Great Tomb at Huangnigang. This tomb, measuring roughly 30 m per side, appears to have been much damaged by the time of the Korea War, though its outline is clear and relatively sharp in the imagery from the early 1950s. The tomb suffered further damage in the second half of the twentieth century until it underwent close survey and limited excavation in Autumn of 2003, by which time little other than its perimeter outline remained. Although its scale and evident chronology (fifth century) suggest that it may have some relationship with the mortuary features in the other three zones considered here, lack of more specific information and the fact that it occupies an adjacent hill prompt me to set it aside from the discussion below, which will focus primarily on the large tombs on the hilltop.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - As a result of the destruction caused by modern development and the lack of detailed surveys predating 1966, data concerning the spatial placement and scale of these tombs to the north of the Tomb of the Great King (and in some cases even the fact of their existence) had been almost completely lost. As this study reveals, however, the georeferenced aerial imagery dating from the Korean War allows us to recover much of this information along with important associated topographical data. Although Donggang was the site of a small village in 1950, the modern settlement had not encroached on the ancient remains to the point where, like today, the landscape itself had been significantly altered by human activity. In the area surrounding the village there are two royal tombs, including the Tomb of the Great King (JYM0541) and, on a hilltop across the stream to the east, another massive tomb designated JYM0043, popularly called the River-Viewing Tomb, as it sits on a high bluff overlooking the Yalu valley. The latter is an early royal burial probably dating to the mid-third century. The 2003 excavations in Jian revealed the existence of a large altar feature to the east of the River-Viewing Tomb, which is also clearly visible in the aerial imagery of the early 1950s. There are no other tombs in its immediate vicinity, suggesting the existence of a recognized (but unwalled) tomb precinct, but there are a number of much smaller tombs along the bluff and on the nearby hills surrounding the royal tomb to a distance of about 350 m, the largest of which barely exceed 20 m per side.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - By contrast, the five tombs on the hill to the north of the Tomb of the Great King vary from 22 to 32 m per side, and they are much fewer in number. Their scale and placement suggest a close relationship with the nearby royal burial, possibly indicating members of the kings family or his elite ministers. Another indication of their possible importance in Koguryõ mortuary practice is the fact that about 180 m to their east sits the King Kwanggaeto stele, erected in 414 a year after the death of Kwanggaeto, who is thought to be the occupant of the Tomb of the Great King. Scholars in East Asia have long been troubled by the fact that the placement of this stele reveals no clear alignment with any of the royal tombs in the area, as would be expected in a Chinese context. Evidence, however, suggests that Koguryo did not follow Sinitic modes of burial practice, and the placement of the stele may not reflect orientation toward any particular tomb. The large granite stele stands 6.4 m tall and is inscribed on all four sides in Chinese script, the text extolling the merits of King Kwanggaeto. The long text, nearly 1800 characters in length, begins on the southeastern face and proceeds top to bottom and right to left around the entire circumference of the stone.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - A reader facing the front side of the stele in Koguryo times could have seen the Tomb of the Great King to the left (i.e., the southwest) at a distance of about 350 m. The four large tombs on the hill crest would have been prominently visible almost directly behind and slightly to the left of the stele, provided, of course, that the view was not obstructed by trees or buildings. The fact that these tombs, placed so close to the royal tomb, would have been prominently visible from the site of the stele suggests some significance in their placement that may relate to Koguryõ funerary practice or ritual. The same viewer would have been able to see the cluster of smaller tombs to the right of the stele at a distance of about 180 m, provided, again, that there were no natural or manmade obstructions. Assuming that these small tombs are contemporary with the nearby royal burial (early fifth century), they may represent the interments of non-elite members of the kings extended household. Another possibility is that these are the tombs of some of the so-called tomb guardian households, which are described in the stele inscription as having been tasked with the maintenance of the mausoleum grounds. Although this is speculation, it is not unlikely that the guardian households would have lived in the vicinity of the mausoleum and might also have been buried nearby. In any case, the tight placement of these smaller tombs on the northeastern slope of the hill suggests the possibility of some significant connection with the royal tomb complex. That the tombs discussed above would have been conspicuously visible from the location of the King Kwanggaeto stele is also suggestive of a deliberate placement, possibly reflecting an elite mortuary practice associated with Koguryos high middle period.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - Although we do not know the chronology of most of these hilltop tombs, JYM0540 was dated to the early fifth century, making it roughly contemporary with the royal burial to the south. Knowledge of the original placement of the other tombs discussed here might afford archaeologists an opportunity to explore those areas in the future in an effort to seek any traces that might survive beneath the modern buildings (though Tomb C and possibly Tomb B may have been completely destroyed by quarrying activity). Tomb D would not have been prominently visible from the stele, and its placement so close to the precinct wall of the royal tomb may indicate that it predated the creation of the precinct, suggesting the possibility that still other tombs may have been removed for the construction of the precinct in the late-fourth or early-fifth century. Unfortunately, historical imagery does not provide answers to such questions, but it suggests to archaeologists a broader range of interpretive possibilities when considering Koguryo mortuary practice.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - The research results presented above illustrate how the use of historical imagery in a GIS environment can result in the recovery of archaeological information that would previously have been considered as irretrievably lost. The present study focuses only on mortuary features, but the same methods may be applied to the study of other above-ground archaeological features, such as walled sites. Such studies would be of potential use to researchers who have recently begun to apply GIS and related landscape modeling tools to close analyses of Koguryo fortresses in northeastern China. The methods described above have revealed the existence and placement of important mortuary features in Jian that were either previously unknown or had been lost before they could be subjected to detailed survey. As shown above, the existence or absence of altar features in historical imagery allow us to gauge the likelihood that certain tombs represent royal interments, and the reconstruction of the broader landscape surrounding certain royal tomb precincts enables us for the first time to interpret how the relative placement of tombs and stelae suggest a deliberate arrangement that would be otherwise invisible to us. Such finds represent new information that deepen our understanding of elite mortuary practice and the utilization of the landscape in the Koguryo capital of the fourth to fifth centuries.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - The usefulness of GIS and aerial imagery as an aid to archaeological research has been well attested in recent years, and it has been increasingly utilized in the study of northeast Asian archaeology. However, the relative difficulty of identifying and accessing the relevant imagery for certain regions in East Asia, including the Yalu River region treated in this study, and the fact that most of these resources are located in the United States, where there are relatively few archaeologists who research East Asia, has resulted in some of those valuable resources having been underutilized. Having developed a set of finding aids that diminishes the difficulties in imagery identification and acquisition, I have made much progress in the study of lost archaeological features in northeastern China and northern Korea. The present study represents a sample of finds from the ancient Koguryõ capital of Jian on the north bank of the Yalu River in China.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - The research results described above illustrate the value of historical imagery in identifying and mapping above-ground archaeological features, tomb mounds in particular, that have succumbed to urban development since the mid-twentieth century. Although such research demands a considerable investment of time and energy, the rewards yielded are similarly great. In the present study we see a glimpse of how the mapping of tomb mounds and their associated features, many of which have been lost without adequate documentation, allows us to gain a richer understanding of the mortuary practices observed by Koguryo elites and of how certain significant areas of the capital city were allocated for mortuary purposes. The results of this study also illustrate that researchers must use appropriate caution when utilizing archaeological reports and the comprehensive tomb registers produced in Jian. The uneven documentation of archaeological remains in Jian over the past century and more makes it difficult for researchers today to appreciate how urban development, especially since the 1960s, has erased much of the archaeological landscape of this area. Very often ignorance of the fact that such an erasure has occurred can result in erroneous interpretation of surviving remains.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - This research has shed new light on some limitations and advantages in the basic methodological approach to the study of archaeological remains in Jian and elsewhere. The most important of the limitations involves the published results of the 1997 survey of tombs. Although this is an immensely valuable work for the study of Koguryo archaeology, it cannot always be relied upon for accurate data, particularly where now-destroyed tombs are concerned. A careful analysis of the Jian region through historical imagery should facilitate an improved and more accurate catalogue of the pre-development tomb inventory of Jian. However, while the use of such imagery is useful for identifying lost features, mapping them with precision, and estimating their measurements, the fact that most features of this type can never be analyzed firsthand indicates a certain limitation on the extent to which we may confidently make interpretations. This limiting factor may be ameliorated somewhat by a careful choice of research questions and by making use of comparative analysis using surviving features.
[Author: Lev Manovich; From essay:"The Science of Culture Social Computing Digital Humanities and Cultural Analytics "] - In keeping with the historical orientation of the humanities, digital humanities scholars use computers to analyze mostly historical artifacts created by professionals, whether this be medieval manuscripts produced by learned monks or nineteenth-century novels produced by authors paid for their work. This has meant a comparatively smaller focus on the twentieth- and twenty-first centuries, in part due to copyright restrictions. At the same time, while the use of quantitative analysis in humanities has been steadily growing, it is still comparatively small. More attention has been paid to building archives and databases than to applying new computational techniques to the study of culture.
[Author: Lev Manovich; From essay:"The Science of Culture Social Computing Digital Humanities and Cultural Analytics "] - The field of social computing is thousands of times larger in terms of a number of publications. Here researchers with degrees in computer science study online user-created content and interactions with this content. Note that this research is carried out not only by computer and information scientists who professionally identify themselves with the field of social computing, but also by researchers in a number of other fields such as computer multimedia, computer vision, music information retrieval, natural language processing, and web science. Social computing can thus be used as an umbrella term for all computer science research that analyzes content and activity on social networks.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - The advantages of utilizing historical imagery are numerous, and here I will mention only a few of them. The most basic advantage is that such imagery provides a unique record of now-lost archaeological remains, and without such imagery there is no evident method of identifying or mapping such lost features. Although high resolution satellite data from recent years offers unparalleled multispectral optical data of the region, in the case of Jian we are limited to viewing only the heavily urbanized landscape of recent years, and even the process of identifying individual tomb mounds through such imagery can be quite daunting as they are often drowned out by the encroaching development. By contrast, in historical imagery, especially that from the Korea War, tomb mounds stand out in stark contrast from their surroundings, making the process of mapping individual tombs much easier. Further, there is significant advantage in comparing as many different images as possible of a single area, as each image reveals certain details that others lack as variations of season and angle of illumination provide clear distinctions in surface detail. Careful processing of the imagery, through contrast stretching for example, allows the viewer to draw out selected types of information, a process that facilitates the easy distinction between stone-mounded and earth-mounded tombs. Moreover, as illustrated above, comparison of imagery of a single region over time allows us to map the process of development, which often helps to explain why certain archaeological features have vanished.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - Beyond the fact that historical imagery allows access to now-lost archaeological remains, it further provides valuable information on the basic landscape of the region prior to significant urban development. Besides the construction of embankments to protect the town from flooding and erosion, Jian has seen extensive reworking of the landscape through quarrying activity, damming of rivers, and the covering or rerouting of natural drainage channels. Each of these activities results in the erasure of elements of the premodern landscape, which can impact our understanding of how ancient populations utilized space.
[Author: Mark E Byington; From essay:"Recovery of lost archaeological features on the Yalu River "] - One of the longer-term goals of my work with historical imagery is to reconstruct the pre-development landscape of Jian. Since most of the aerial imagery utilized in this research was designed to produce overlapping photographs, or stereopairs, for the use of photogrammetric measurements, they can also now be used to generate a digital elevation model (DEM), which allows us to create a three-dimensional model of the topography depicted in the imagery. A further advantage of this approach is that the terrain model thus produced represents the pre-development landscape, an advantage not provided by existing satellite-generated DEMs, such as SRTM or ASTER, which represent only the landscape of the 2000s. A further limitation of most existing freely available DEMs is that their best resolution is often insufficient to account for subtle variations in surface contours, which a finer resolution of the area through historical imagery can provide. By draping georeferenced aerial imagery over a DEM, we produce a three-dimensional digital model of the region through which we may move freely, which opens a window for the macro-analysis of the pre-development landscape. The digital integration of archaeological data from imagery, surveys, and excavations, into a three-dimensional surface model provides a rich analytical potential for close study of the pre-development archaeology and landscape of the Jian region. As such an approach has to date been wholly unexplored in the context of this region, we may expect the process to result in considerable yield and value.
[Author: Lev Manovich; From essay:"The Science of Culture Social Computing Digital Humanities and Cultural Analytics "] - I developed the concept of cultural analytics in 2005 to refer to the analysis of massive cultural datasets and flows using computational and visualization techniques. In 2007 we established a research lab (Software Studies Initiative, softwarestudies.com) to start working on practical projects. The following are the examples of theoretical and practical questions that are driving our research:
[Author: Lev Manovich; From essay:"The Science of Culture Social Computing Digital Humanities and Cultural Analytics "] - What does it mean to represent culture by data? What are the unique possibilities offered by computational analysis of large cultural data in contrast to qualitative methods used in humanities and social science? How can we use quantitative techniques to study the key cultural form of our era - interactive media? How can we combine computational analysis and visualization of large cultural data with qualitative methods, including close reading? (In other words, how does one combine analysis of larger patterns with the analysis of individual artifacts and their details?) How can computational analysis do justice to variability and diversity of cultural artifacts and processes, rather than focusing on the typical and most popular?
[Author: Lev Manovich; From essay:"The Science of Culture Social Computing Digital Humanities and Cultural Analytics "] - Eight years later, the work of our lab has become only a tiny portion of a very large body of research. Thousands of researchers have already published tens of thousands of papers analyzing patterns in massive cultural datasets. First, there are data describing the activity on the most popular social networks (Flickr, Instagram, YouTube, Twitter, etc.), user-created content shared on these networks (tweets, images, video, etc.), and also users interactions with this content (likes, favorites, reports, comments). Second, researchers have also started to analyze particular professional cultural areas and historical periods, such as website design, fashion photography, twentieth-century popular music, nineteenth-century literature, etc. This work is carried out in two newly developed fields - social computing and digital humanities.
[Author: Lev Manovich; From essay:"The Science of Culture Social Computing Digital Humanities and Cultural Analytics "] - Where does this leave cultural analytics? Not only does it continue to be relevant as an intellectual program, but in fact it is even more relevant now than it was ten years ago. As we will see, digital humanities and social computing carve their own domains in relation to the types of cultural data they study, but cultural analytics does not have these limitations. We are also not interested in choosing between humanistic versus scientific goals and methodology, or subordinating one to another. Instead, we are interested in combining both in the study of cultures – focusing on the particular, interpretation, and the past from the humanities, while centering on the general, formal models, and predicting the future from the sciences. In this article, I will discuss these and other characteristics of both approaches to the study of large cultural datasets as they have developed until now, pointing out opportunities and ideas that have not yet been explored.
[Author: Lev Manovich; From essay:"The Science of Culture Social Computing Digital Humanities and Cultural Analytics "] - This research deals almost exclusively with data produced after 2004, when social networks and media-sharing services started to become popular. (Since it takes one to two years to do research and publish a paper, typically a paper published in 2015 will use the data collected in 2012-2014.) The datasets are usually much larger than those used in digital humanities. Tens or hundreds of millions of posts, photos, or other items are not uncommon. Since the great majority of user-generated content is created by regular people rather than by professionals, social computing studies the non-professional, vernacular culture by default.
[Author: Lev Manovich; From essay:"The Science of Culture Social Computing Digital Humanities and Cultural Analytics "] - The scale of this research may be surprising to humanities and arts practitioners who may not realize how many people are working in computer science and related fields. For example, the search on Google Scholar for Twitter dataset algorithm returned 102,000 papers, the search for YouTube video dataset returned 27,800 papers, and the search for “Flickr images algorithm” returned 17,400 papers. (I use word dataset” and “algorithms” to limit results to papers that use computational methods.) Even if the actual numbers are much smaller, this is still impressive. Obviously not all these publications directly ask cultural questions, but many do.
[Author: Lev Manovich; From essay:"The Science of Culture Social Computing Digital Humanities and Cultural Analytics "] - Why do computer scientists rarely work with large historical datasets of any kind? Typically they justify their research by reference to already existing industrial applications — for example, search or recommendation systems for online content. The general assumption is that computer science will create better algorithms and other computer technologies useful to industry and government organizations. The analysis of historical artifacts falls outside this goal, and consequently few computer scientists work on historical data (the field of digital heritage being one exception).
[Author: Lev Manovich; From essay:"The Science of Culture Social Computing Digital Humanities and Cultural Analytics "] - However, looking at many examples of computer science papers, it is clear that they are actually doing humanities or communication studies (in relation to contemporary media), but at a much larger scale. Consider, for example, these recent publications: Quantifying Visual Preferences Around the World and What We Instagram: A First Analysis of Instagram Photo Content and User Types. The first study analyzes worldwide preferences for website design using 2.4 million ratings from 40,000 people from 179 countries. Obviously, the study of aesthetics and design traditionally was part of the humanities. The second study analyzes most frequent subjects of Instagram photos — a topic which can be compared to art historical studies of the genres in seventeenth-century Dutch art.
[Author: Lev Manovich; From essay:"The Science of Culture Social Computing Digital Humanities and Cultural Analytics "] - Another example is a paper called What is Twitter, a Social Network or a News Media?. Published in 2010, it has since been cited 3284 times in other social computing publications. It was the first large-scale analysis of the Twitter social network using 106 million tweets by 41.7 million users. The study looked in particular at trending topics, showing “what categories trending topics are classified into, how long they last, and how many users participate. This is a classic question of communication studies, going back to the pioneering work of Paul F. Lazarsfeld and his colleagues in the late 1930s who manually counted the topics of radio broadcasts. But, given that Twitter and other micro-blogging services represent a new form of media – like oil painting, printed books, and photography before — understanding the specificity of Twitter as a medium should also be a topic for the humanities.
[Author: Lev Manovich; From essay:"The Science of Culture Social Computing Digital Humanities and Cultural Analytics "] - In digital humanities, many scholars have already analyzed historical literary texts using a variety of computational methods and algorithms developed originally in computer science. I am not citing any particular examples here since the introduction to this first issue of Journal of Cultural Analytics provides an overview of this quite substantial body of research. Some researchers also applied such methods to other types of historical texts. The methods typically come from the fields of information retrieval and network science.
[Author: Lev Manovich; From essay:"The Science of Culture Social Computing Digital Humanities and Cultural Analytics "] - There is also a small but growing number of publications that analyze historical non-textual media using computer science methods borrowed from the fields of image processing, computer vision and music information retrieval. The prominent examples include Toward Automated Discovery of Artistic Influence (Saleh et al, 2014), Measuring the Evolution of Contemporary Western Popular Music (Joan Serrà, Álvaro Corral, Marián Boguñá, Martín Haro & Josep Ll. Arcos, 2012), and Quicker, faster, darker: Changes in Hollywood film over 75 years. (James E Cutting, Kaitlin L Brunick, Jordan DeLong, Catalina Iricinschi, Ayse Candan, 2011).
[Author: Lev Manovich; From essay:"The Science of Culture Social Computing Digital Humanities and Cultural Analytics "] - Until a few years ago, the only project that analyzed cultural history on a really large scale of millions of texts was carried out by scientists rather than by humanists. I am referring to N-Gram Viewer created in 2010 by Google scientists Jon Orwant and Will Brockman following the prototype by two PhD students from Harvard in Biology and Applied Math. However, more recently, we see people in digital humanities scaling up the size of data they study. For example, in Mapping Mutable Genres in Structurally Complex Volumes literary scholar Ted Underwood and his collaborators analyzed 469,200 volumes from HathiTrust Digital Library. In A network framework of cultural history, Art historian Maximilian Schich and his colleagues analyzed the life trajectories of 120,000 notable historical individuals. And even larger historical datasets are becoming available in the areas of literature, photography, film, and TV — although they remain to be analyzed. In 2012 The New York City Municipal Archives released 870,000 digitized historic photos of the city of New York. In 2015 HathiTrust made available for research, data extracted from 4,801,237 volumes containing 1.8 billion pages. Also in 2015 The Associated Press and British Movietone uploaded to YouTube 550,000 digitized news stories covering the period from 1895 to today.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - The table then identifies the elite type of the deceased (or that of her husband or father in the case of a woman, or that of the father in the case of a child under the age of 21). Elite types include: no office; civil for civil bureaucrats; military for military men; dynastic for imperial princes and princesses, the consorts of princes and princesses, palace women, and eunuchs; and “religious” for monks, nuns, and priests.
[Author: Lev Manovich; From essay:"The Science of Culture Social Computing Digital Humanities and Cultural Analytics "] - What is the importance of having such large cultural datasets? Cant we simply use smaller samples? I believe that there are significant reasons. First, to have a representative sample, we need to have a much larger set of actual items from which to sample, or at least a good understanding of what this larger set includes. So, for example, if we want to create a representative sample of twentieth-century films, we can use IMDb that contains information on 3.4 million films and TV shows (including separate episodes). Similarly, we can create a good sample of historical U.S. newspaper pages using the Historical American Newspaper collection of millions of digitized pages from The Library of Congress. But in many other cultural fields, such larger datasets do not exist, and without them, it may be impossible to construct representative samples.
[Author: Lev Manovich; From essay:"The Science of Culture Social Computing Digital Humanities and Cultural Analytics "] - Here is the second reason: assuming that we can construct a representative sample of a cultural field, we can use it to find general trends and patterns. For example, in the already mentioned paper What We Instagram: A First Analysis of Instagram Photo Content and User Types, three computer scientists analyzed 1000 Instagram photos and came up with eight most frequent categories (selfie, friends, fashion, food, gadget, activity, pet, captioned photos). The sample of 1000 photos was randomly selected from a larger set of photos shared by 95,343 unique users. It is possible that these eight categories are also the most popular among all Instagram photos shared worldwide at the time when the scientists did their study. However, as we saw from our projects where we analyzed Instagram photos in different cities and their parts (for example, the center of Kyiv during 2014 Ukrainian Revolution in The Exceptional and the Everyday), people also share many other types of images. Depending on the geographic area and time period, some of these types may replace the top eight in popularity. In other words, while a small sample allows finding the typical” or “most popular,” it does not reveal what I call content islands — types of coherent content with particular semantic and/or aesthetic characteristics shared in moderate numbers.
[Author: Lev Manovich; From essay:"The Science of Culture Social Computing Digital Humanities and Cultural Analytics "] - When I first started to think about cultural analytics in 2005, both digital humanities and social computing were just getting started as research fields. I felt the need to introduce this new term to signal that our labs work would not be simply a part of digital humanities or social computing, but would cover subject matter studied in both fields. Like digital humanists, we are interested in analyzing historical artifacts — but we are also equally interested in contemporary digital visual culture (e.g., Instagram). Also, we are equally interested in professional culture, artifacts created by dedicated non-professionals and artists outside of the art world (e.g., deviantart.com, “the largest online social network for artists and art enthusiasts”) and accidental creators (for example, people who once in a while upload their photos to social media networks).
[Author: Lev Manovich; From essay:"The Science of Culture Social Computing Digital Humanities and Cultural Analytics "] - Like computational social scientists and computer scientists, we are also attracted to the study of society using social media and social phenomena specific to social networks. An example of the former is finding similar neighborhoods in the city using social media activity, as in The Livehoods Project: Utilizing Social Media to Understand the Dynamics of a City. An example of the latter is analyzing patterns of information diffusion online, as in Delayed information cascades in Flickr: Measurement, analysis, and modeling. However, if social computing focuses on the social in social networks, cultural analytics focuses on the cultural. (Therefore, the most relevant part of social sciences for cultural analytics is sociology of culture, and only after that sociology and economics.)
[Author: Lev Manovich; From essay:"The Science of Culture Social Computing Digital Humanities and Cultural Analytics "] - We believe that the web and social networks content and user activities give us the unprecedented opportunity to describe, model, and simulate the global cultural universe, while questioning and rethinking basic concepts and tools of humanities that were developed to analyze small cultural data (i.e., highly selective and non-representative cultural samples). In the very influential definition by British cultural critic Matthew Arnold (1869), culture is “the best that has been thought and said in the world. Academic humanities have largely followed this definition. And when they started to revolt against their canons and to include the works of previously excluded people (women, non-whites, non-Western authors, queer, etc.), they often included only the best created by those who were previously excluded.
[Author: Lev Manovich; From essay:"The Science of Culture Social Computing Digital Humanities and Cultural Analytics "] - Cultural analytics is interested in everything created by everybody. In this, we are approaching culture the way linguists study languages or biologists who study the life on earth. Ideally, we want to look at every cultural manifestation, rather than selective samples. (This more systematic perspective is not dissimilar to that of cultural anthropology.) The range of projects we have worked on in our lab since 2008 exemplifies the larger inclusive scope combining professional and vernacular, historical and contemporary. We have analyzed historical, professionally created cultural content in all Time magazine covers (1923-2009); paintings by Vincent van Gogh, Piet Mondrian, and Mark Rothko; 20,000 photographs from the collection of Museum of Modern Art in New York (MoMA); one million manga pages from 883 manga series published in the last 30 years. Our analysis of contemporary vernacular content includes Phototrails (the comparison of visual signatures of 13 global cities using 2.3 million Instagram photos), The Exceptional and the Everyday: 144 Hours in Kyiv (the analysis of Instagram images shared in Kyiv during the 2014 Ukrainian Revolution) and On Broadway (the interactive installation exploring Broadway in NYC using 40 million user-generated images and data points). We also have looked at contemporary amateur or semi-professional content (On million artworks shared by 30,000 semi-professional artists on www.deviantart.com.) Currently we are exploring a dataset of 265 million images tweeted worldwide during 2011-2014. In summary, in our work we dont draw a boundary between (smaller) historical professional artifacts and (bigger) online digital content created by non-professionals. Instead, we freely take from both.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - What would a new, virtual, and universal Alexandria library look like? Kahle and his colleagues have forcefully articulated an expansive democratic vision of a past that includes all voices and is open to all. There are about ten to fifteen million peoples voices evident on the Web, he told a reporter. The Net is a peoples medium: the good, the bad and the ugly. The interesting, the picayune and the profane. Its all there. Advocates of the new universal library and archive wax even more eloquently about democratizing access to the historical record. The opportunity of our time is to offer universal access to all of human knowledge, said Kahle.
[Author: Lev Manovich; From essay:"The Science of Culture Social Computing Digital Humanities and Cultural Analytics "] - Obviously, social networks today do not include every human being, and the content shared is sometimes specific to these networks (e.g., Instagram selfies), as opposed to something that existed before. This content is also shaped by the tools and interfaces of technologies used for its creation, capturing, editing, and sharing (e.g., Instagram filters, Instagrams Layout app, etc). The kind of cultural actions available are also defined by these technologies. For example, in social networks you can like,” share, or comment on a piece of content. In other words, like in quantum physics, here the instrument can influence the phenomena we want to study. All this needs to be carefully considered when we study user-generated content and user activities. While social network APIs make it easy to access massive amounts of content, it is not everything by everybody.
[Author: Lev Manovich; From essay:"The Science of Culture Social Computing Digital Humanities and Cultural Analytics "] - When humanities were concerned with “small data (content created by single authors or small groups), the sociological perspective was only one of many options for interpretation — unless you were a Marxist. But once we start studying online content and activities of millions of people, this perspective becomes almost inevitable. In the case of big cultural data, the cultural and the social closely overlap. Large groups of people from different countries and socio-economic backgrounds (sociological perspective) share images, video, texts, and make particular aesthetic choices in doing this (humanities perspective). Because of this overlap, the kinds of questions investigated in sociology of culture of the twentieth century (exemplified by its most influential researcher Pierre Bourdieu) are directly relevant for cultural analytics.
[Author: Lev Manovich; From essay:"The Science of Culture Social Computing Digital Humanities and Cultural Analytics "] - Given that certain demographic categories have become taken for granted in our thinking about society, it appears natural today to group people into these categories and compare them in relation to social, economic, or cultural indicators. For example, Pew Research Center regularly reports the statistics of popular social platform use, breaking their user sample by demographics such as gender, ethnicity, age, education, income, and place of living (urban, suburban, and rural.) So if we are interested in various details of social media activities, such as types of images shared and liked, filters used, or selfie poses, it is logical to study the differences between people from different countries, ethnicities, socioeconomic backgrounds, or levels of technical expertise. The earlier research in social computing did not, and most of the current work still does not consider such differences, treating all users as one undifferentiated pool of “humanity” — but more recently we have started seeing publications that break users into demographic groups. While this is a very good move, we also want to be careful in how far we want to go. Humanistic analysis of cultural phenomena and processes using quantitative methods should not be simply reduced to sociology, i.e. considering common characteristics and behaviors of human groups only.
[Author: Lev Manovich; From essay:"The Science of Culture Social Computing Digital Humanities and Cultural Analytics "] - Sociological tradition is concerned with finding and describing the general patterns in human behavior, rather than with analyzing or predicting the behaviors of particular individuals. Cultural analytics is also interested in patterns that can be derived from the analysis of large cultural datasets. However, ideally the analysis of the larger patterns will also lead us to particular individual cases, i.e. individual creators, their particular creations, or cultural behaviors. For instance, the computational analysis of all photos made by a photographer during her long career may lead us to the outliers – the photos that are most different from all the rest. Similarly, we may analyze millions of Instagram images shared in multiple cities to discover the types of images unique to each city (this example comes from current research in our lab).
[Author: Lev Manovich; From essay:"The Science of Culture Social Computing Digital Humanities and Cultural Analytics "] - In other words, we may combine the concern of social science, and sciences in general, with the general and the regular, and the concern of humanities with individual and particular. (After all, all great artists in the history of art were outliers in comparison to their contemporaries). They just described examples of analyzing massive datasets to zoom in on the unique items illustrate one way of doing this, but it is not the only way.
[Author: Lev Manovich; From essay:"The Science of Culture Social Computing Digital Humanities and Cultural Analytics "] - The goal of science is to explain phenomena and develop compact mathematical models that describe how these phenomena work. The three laws of Newtons physics provide a perfect example of how classical science was approaching this goal. Since the middle of the nineteenth century, a number of new scientific fields adopted a new probabilistic approach. The first example was the statistical distribution describing likely speeds of gas particles presented by Maxwell in 1860 (now it is called the Maxwell-Boltzmann distribution). And what about the social sciences? Throughout the eighteenth and nineteenth centuries, many thinkers were expecting that, similar to physics, the quantitative laws governing societies will also be eventually found. This never happened. (The closest nineteenth-century social thought came to postulating objective laws were in the works of Karl Marx). Instead, when positivist social science started to develop in the late nineteenth and early twentieth century, it adopted a probabilistic approach. So instead of looking for deterministic laws of society, social scientists study correlations between measurable characteristics and model the relations between dependent and independent” variables using various statistical techniques.
[Author: Lev Manovich; From essay:"The Science of Culture Social Computing Digital Humanities and Cultural Analytics "] - After deterministic and probabilistic paradigms in science, the next paradigm was computational simulation — running models on computers to simulate the behavior of systems. The first large-scale computer simulation was created in the 1940s by Manhattan Project to model a nuclear explosion. Subsequently, simulation was adapted in many hard sciences, and in the 1990s it was also taken up in the social sciences.
[Author: Lev Manovich; From essay:"The Science of Culture Social Computing Digital Humanities and Cultural Analytics "] - In the early twenty-first century, the volume of digital online content and user interactions allows us to think of a possible “science of culture.” For example, by the summer of 2015, Facebook users were sharing 400 million photos and sending 45 billion messages daily. This scale is still much smaller than that of atoms and molecules — for example, 1cm³ of water contains 3.33*10²² molecules. However, it is already bigger than the numbers of neurons in the whole nervous system of an average adult estimated at 86 billion. But since science now includes a few fundamental approaches to studying and understanding the phenomena — deterministic laws, statistical models, and simulation — which of them should a hypothetical science of culture adapt?
[Author: Lev Manovich; From essay:"The Science of Culture Social Computing Digital Humanities and Cultural Analytics "] - Looking at the papers of computer scientists who are studying social media datasets, it is clear that their default approach is statistics. They describe social media data and user behavior in terms of probabilities. This includes creation of statistical models — mathematical equations that specify the relations between variables that may be described using probability distributions rather than specific values. A majority of papers today also use supervised machine learning — an automatic creation of models that can classify or predict the values of new data using already existing examples. In both cases, a model can only account for part of the data, and this is typical of the statistical approach.
[Author: Lev Manovich; From essay:"The Science of Culture Social Computing Digital Humanities and Cultural Analytics "] - Computer scientists studying social media use statistics differently than social scientists. The latter want to explain social, economic or political phenomena (for example, the effect of family background on childrens educational performance). Computer scientists are generally not concerned with explaining patterns in social media they discover by referencing some external social, economic or technological factors. Instead, they typically either analyze social media phenomena internally, or try to predict the outside phenomena using information extracted from social media datasets. The example of the former is a statistical description of how many favorites” a photo on Flickr may receive on the average after a certain period of time. The example of the latter is the Google Flu Trends service that predicts flu activity using a combination of Google search data and CDC (U.S. Centers for Disease Control and Prevention) official flu data.
[Author: Lev Manovich; From essay:"The Science of Culture Social Computing Digital Humanities and Cultural Analytics "] - The difference between deterministic laws and non-deterministic models is that the latter only describe probabilities and not certainties. The laws of classical mechanics apply to any macroscopic objects. In contrast, a probabilistic model for predicting number of favorites for a Flickr photo as a function of time since it was uploaded cannot tell us exactly the numbers of favorites for any particular photo. It only describes the overall trend. This seems to be the appropriate method for a science of culture. If instead we start postulating deterministic laws of human cultural activity, what happens to the idea of free will? Even in the case of seemingly pretty automatic cultural behavior (people favoring photos on social networks with certain characteristics such as pretty landscapes, cute pets, or posing young females), we dont want to reduce humans to mechanical automata for the passing of memes.
[Author: Lev Manovich; From essay:"The Science of Culture Social Computing Digital Humanities and Cultural Analytics "] - The current focus on probabilistic models of online activity leaves out the third scientific paradigm - simulation. However, in digital humanities simulation is becoming a new area of interest in the digital humanities. In 2009, scientists at IBMs Almaden research center simulated human visual cortex using 1.6 billion virtual neurons with 9 trillion synapses. Given this, why cant we begin thinking about how to simulate, for instance, all content produced yearly by users of Instagram? Or all content shared by all users of major social networks? Or the categories of images people share? The point of such simulations will be not to get everything right or to precisely predict what people will be sharing next year. Instead, we can follow the authors of the influential textbook Simulation for the Social Scientist when they state that one of the purposes of simulation is to obtain a better understanding of some features of the social world and that simulation can be used as a method of theory development.” (Emphasis mine.) Since computer simulation requires developing an explicit and precise model of the phenomena, thinking of how cultural processes can be simulated can help us to develop more explicit and detailed theories than we use normally. (For the example of how agent-based simulation can be used to study the evolution of human societies, see War, space, and the evolution of Old World complex societies.)
[Author: Lev Manovich; From essay:"The Science of Culture Social Computing Digital Humanities and Cultural Analytics "] - And what about “big data”? Does it not represent a new paradigm in science with its own new research methods? This is a complex question that deserves its own article. However, as a way of conclusion, I do want to mention one concept interesting for humanities that we can borrow from big data analytics and then push in a new direction.
[Author: Lev Manovich; From essay:"The Science of Culture Social Computing Digital Humanities and Cultural Analytics "] - Twentieth-century social science was working on what we can call long data.” That is, the number of cases was typically many times bigger than the number of variables being analyzed. For example, imagine that we surveyed 2000 people asking them about their income, family educational achievement and their years of education. As a result, we have 2000 cases and three variables. We can then examine correlations between these variables, or look for clusters in the data, or perform other types of statistical analysis.
[Author: Lev Manovich; From essay:"The Science of Culture Social Computing Digital Humanities and Cultural Analytics "] - The beginnings of social sciences are characterized by the most extreme asymmetries of this kind. The first positivist sociologist Karl Marx divides all humanity into just two classes: people who own means of production and people who dont (i.e. capitalists and the proletariat). Later sociologists add other divisions. Today these divisions are present in numerous surveys, studies and reports in popular media and academic publications — typically, gender, race, ethnicity, age, educational background, income, place of living, religion, and a few others (the list of additional variables varies from study to study). But regardless of details, the data collected, analyzed and interpreted is still very long.” The full populations or their samples are described using much smaller number of variables.
[Author: Lev Manovich; From essay:"The Science of Culture Social Computing Digital Humanities and Cultural Analytics "] - But why should this be the case? In the fields of computer media analysis and computer vision, computer scientists use algorithms to extract thousands of features from every image, a video, a tweet, an email, and so on. So while, for example, Vincent van Gogh only created about 900 paintings, these paintings can be described according to thousands of separate dimensions. Similarly, we can describe everybody living in a city according to millions of separate dimensions by extracting all kinds of characteristics from their social media activity. For another example, consider our own project On Broadway where we represent Broadway in Manhattan with 40 million data points and images using messages, images and check-ins shared along this street on Twitter, Instagram, and Foursquare, as well as taxi rides data and the U.S. Census indicators for the surrounding areas.
[Author: Lev Manovich; From essay:"The Science of Culture Social Computing Digital Humanities and Cultural Analytics "] - In other words, instead of long data we can have wide data — very large and potentially endless numbers of variables describing a set of cases. Note that if we have more variables than cases, such representation would go against the common sense of both social science and data science. The latter refers to the process of making a large number of variables more manageable as dimension reduction. But for us wide data” offers an opportunity to rethink fundamental assumptions about what society is and how to study it; and similarly, what is culture, an artistic career, a body of images, a group of people with similar aesthetic taste, and so on. Rather than dividing cultural history using one dimension (time), or two (time and geographic location) or a few more (e.g., media, genre), endless dimensions can be put in play. The goal of such “wide data analysis” will be not only to find new similarities, affinities, and clusters in the universe of cultural artifacts, but, first of all, to help us question our common sense view of things, where certain dimensions are taken for granted. And this is one example of general cultural analytics method: estrangement (ostranenie), making strange our basic cultural concepts and ways or organizing and understanding cultural datasets. Using data and techniques for manipulating it to question how we think, see, and ultimately act on our knowledge.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - This paper examines how a large prosopographic database of tens of thousands of individuals in combination with network analysis can provide new insight into the evolution of the political elite over the course of the Tang Dynasty (618–907). In a previous work, I focused specifically on the structure of the elite in the final century of Tang rule, exploring how a group of several dozen old “aristocratic families—whose ancestors had served in office generation after generation for hundreds of years—managed to dominate top bureaucratic offices partly by relying on the social capital embedded in a tightly knit marriage network based at the capital. Here, I expand the temporal scope of my analysis to include the seventh and eighth centuries, looking not at the Tang elite at a single moment in time, but rather at how it evolved over a period of three centuries.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - My hope is that the methodologies I showcase here can fruitfully be used to analyze political power in other periods or places.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - As is well known by historians of the Tang, there is every reason to expect that the political elite changed in fundamental ways between the Sui reunification in 589 and the collapse of the Tang following Huang Chaos sack of the capital cities in 880. To begin with, the unification process itself brought about a reorganization of the elite families that had once served the regional dynasties. Though the Sui and the Tang were both direct successors of the Western Wei and Northern Zhou based in the northwest, they made every effort to welcome into their newly centralized bureaucracies the descendants of men who had in the mid-sixth century served in the northeast (under the Eastern Wei and the Northern Qi) or the south (under the Southern Liang and the Chen). A few decades after the founding of the Tang, the rise to power of Empress Wu in the second half of the seventh century led to a new round of political reshuffling as the monarch sought to bolster her power and legitimacy by purging her opponents and recruiting new men loyal to her. Finally, the An Lushan Rebellion (755-763), which culminated in the complete autonomy of several provinces in the northeast, prompted a total reordering of the northeastern local elite and a significant restructuring of the administration elsewhere.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - Aside from these political developments, there were also institutional innovations that affected the composition of the political elite. In the Sui and early Tang, the implementation of the law of avoidance (which mandated that officials not serve near their places of origin), as well as the elimination of the office of Recommending Legate 中正 (a member of the local elite who recommended local talent for government service), diminished the power of locally entrenched families. The expansion of the civil service examination under Empress Wu and the development in the mid-eighth century of provincial governments—which recruited their staffs without central government oversight-both in principle offered new routes of upward mobility into the officeholding class. Meanwhile, the deregulation of land (with the demise of the equal-field system) in the post-An Lushan Period and a concomitant expansion of the economy plausibly produced new landed and economic elites who could take advantage of these new opportunities to enter officialdom. How did this series of political and institutional developments impact the political elite?
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - Needless to say, the question of what happened during the Tang to the medieval aristocracy has attracted considerable attention in the scholarly literature over the years. Much of the most influential work broaching this topic dates to the golden age of social history, in the 1950s, 1960s, and 1970s. This high-quality scholarship remains a necessary starting point for anyone wishing to examine the Tang elite. It should be noted, however, that some of this scholarship is influenced in no small part by dated meta-narratives and ways of conceptualizing history. Thus, the perspective (perhaps influenced by a Marxist conceptualization of history) that sees social change as driven by the struggle between an older social class and a newly emergent one, in combination with a faith (rooted in traditional Chinese historiography) in the power of institutional change to transform society, has led to the commonly held view that the old “aristocratic families declined inexorably over the entire course of the Tang, as a result of the institutional changes described above, in conjunction with the inability of those families to compete with a newly risen “bureaucratic elite. In fact, empirical evidence suggests that the old families maintained their dominance up until the very end of the dynasty without any serious competition. But this does not mean that one cannot detect other sorts of fundamental changes in the structure of the political elite as a consequence of Tang political and institutional developments.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - Diminishing provincial ties to the highest echelons of political power can also be measured on the basis of the fraction of epitaphs composed for scions of prominent pre-Sui families (defined here as individuals whose ancestors had biographies in the dynastic histories of the Northern and Southern Dynasties), as well as members of the clans of Tang chief ministers, of clans belonging to the capital-based marriage network described above, and of the marriage-ban clans. As shown in Table 6, one recognizes a rapid decline in the provinces of such well-connected families after the founding of the Tang, though it remains unclear if it was the fall of the Sui or the rise of Empress Wu that made the greater impact.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - New sources of biographical data (notably the vast troves of funerary epitaphs excavated in recent decades), the digitization of this data into large datasets composed of tens of thousands of individuals, and new digital techniques together offer the possibility of reconsidering the evolution of the Tang political elite from a fresh perspective. Whereas earlier scholarship was often based on the synthesis of anecdotal evidence culled from a broad assortment of sources, or on detailed case studies of individual clans, the very large biographical datasets now available permit taking a more systematic and holistic view of the entire sociopolitical elite in its entirety. Moreover, the fact that much of the new data is based on excavated epitaphs (which dig deeper into society than dynastic history biographies and other traditional sources) means one can now take into consideration a much broader swathe of elite society. Network analysis also permits a reconceptualization of social groupings. Earlier scholarship typically classified people into categories based on apparent class background, or classified families into regional blocks based on a claimed ancestral place of origin, then (whether due to source or methodological limitations or to ideological proclivity) assumed that these categories constituted cohesive social realities. By analyzing very large quantities of genealogical data in the aggregate and reconstructing networks of kin and marriage ties, one can shift ones analysis of social groupings from a theoretical to an empirical basis. One no longer needs to assume that feelings of solidarity existed between individuals sharing common characteristics; one can reconstruct categories of people based on demonstrable social connections.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - Most of the data used here comes from two sources: the August 2017 release of the China Biographical Database (CBDB), and version 1 of the Prosopographic and Social Network Database of the Tang and Five Dynasties (TBDB), compiled as part of an earlier book project and largely incorporated into the recent versions of CBDB. Of particular relevance here, CBDB incorporates genealogical and marriage data taken from the several thousand Tang epitaphs included in the two-volume Tangdai muzhi huibian 唐代墓誌彙編 and its sequel volume. My own TBDB focuses primarily on ninth-century epitaphs, but includes epitaphs found in a much broader assortment of publications. It also incorporates the genealogical tables of the imperial clan and families of chief ministers included in the eleventh-century New Tang History (新唐書). For the present article, I added large amounts of new data culled from recently published collections of epitaphs, with particular emphasis on three time slices (650 to 660, 720 to 730, and 800 to 810) described in more detail below, on epitaphs for members of families that produced chief ministers, and on epitaphs spanning the Sui-Tang period from provincial north China. One consequence of combining datasets in this way was that it was then necessary for me to disambiguate (i.e., identify and merge) hundreds of duplicate entries. Correctly identifying duplicate entries—e.g., determining conclusively that the father-in-law mentioned in one epitaph was the same man as a member of one of the large patrilines with the same name-was particularly important in order to place individuals correctly within large family trees and identify as many marriage ties between families as possible. Finally, let me note that, for the readers reference, version 1.5 of TBDB (available for download) includes the queries used to produce the figures and tables of the present study.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - The basic building blocks of the Tang marriage network of political elites consisted of what I have previously termed patrilines. A patriline is defined to be the largest multi-generational family tree of patrilineal kin that can be reconstructed on the basis of father-son and father-daughter relations that are attested in historical sources. Unlike much past scholarship, which has often depended primarily on claims to particular great clan choronyms (i.e., the place of ancestral origin of a famous family of the same surname) as a way of identifying clan membership, the patrilines in this study have all been defined empirically on the basis of extant genealogical data. Given the fragmentary nature of much of the underlying source material, the majority of reconstructed patrilines-especially those based on epitaph data described below-contain only a handful of individuals (e.g., a great grandfather, a grandfather, a father, and two sons). But other patrilines were much larger, sometimes with hundreds of known members spread across a dozen or more generations.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - The major source of data for patrilineal ties remains the New Tang History genealogical tables, which identify over 17,000 father-son ties. Consequently, it is no surprise that the largest reconstructed patrilines are families represented in these tables. But genealogical data culled from excavated tomb epitaphs (which in my database consists to date of nearly 15,000 discrete father-son and father-daughter ties) will soon surpass the New Tang History tables in importance, and they have already permitted the reconstruction of a few previously unknown large patrilines.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - Tomb epitaphs are also much richer sources of marriage data, since dynastic histories and other traditional sources of Tang history usually identify only the marriage partners of emperors and imperial princesses. Unfortunately, epitaphs do not always provide sufficient details to confirm the identities of a womans husband or a mans father-in-law, making disambiguation difficult. By surveying several thousand epitaphs, it has nevertheless been possible to identify hundreds of marriage connections between patrilines.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - Figures 1 and 2 depict the core of a large marriage network of Tang political elites. The estimated dates of the marriages—each calculated based on the known or estimated birth dates of the groom-span the entire Sui-Tang period. The figures thus constitute a reformulation of the ninth-century marriage network in my previous study, with the addition of seventh- and eighth-century data. Each node in the network represents a patriline; the thickness of lines between nodes is proportional to the number of known marriages between the two patrilines in question. Large numbers of leaves and minor branches of the network (i.e. small patrilines with few demonstrable ties to the center of the marriage network) have been eliminated to reduce clutter. In all, 87 patrilines-representing only 7 percent of the patrilines in the total network but 60 percent of known individuals and 384 marriages are depicted in the figures.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - As suggested in Figure 1, the network was capital-based—that is, the home base of most patrilines was either the Western Capital of Changan (denoted with a “C”), the Eastern Capital of Luoyang (“L”), or split between the two capitals and/or the Capital Corridor linking the two cities together (“CL”). In addition, by Tang times, most patrilines had served in office for many generations, going back to the pre-Sui period (gray and black nodes in Figure 2). This network embodied a capital-based bureaucratic aristocracy.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - Much like in the ninth-century case, and as analyzed in my book, one recognizes two prominent marriage cliques. The clique on the right was more diffuse, lacking any single dominant patriline. It was composed almost exclusively of Luoyang-based families (Figure 1), most of which had served northeastern regimes in pre-Sui times (Figure 2). Moreover, the families at the core of this marriage clique were all branches of the very most eminent marriage-ban clans (black nodes)-clans that were in early Tang times forbidden to intermarry in an ultimately unsuccessful attempt by the throne to diminish their prestige. In contrast to the marriage-ban clique, the clique on the left was organized around the imperial clan (represented by the node at the center of the clique). It was more heterogeneous, containing many families based in Changan, but also some families based in Luoyang or other parts of the Capital Corridor. The clique was dominated by families that had served the northwestern dynasties, but also included northeastern families. A few patrilines with southern origins also belonged to this clique, though overall the southern families occupied a relatively peripheral position in the marriage network.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - Needless to say, by aggregating three centuries of marriage data, Figures 1 and 2 tell us nothing about the evolution of the marriage network over time. To provide a sense of chronological change, Figure 3 depicts the network as it existed in three different time periods: 600–700 CE, 700–760 CE, and 780–880 CE. The two cliques are recognizable in all time periods, though there is a greater density of marriage ties within the marriage-ban clique. A possible explanation is that the marriage-ban clans preferred increasingly to intermarry with each other. Alternatively, perhaps due to a heightened sense that good marriages defined the most prestigious families, later epitaphs made more of a point to identify such good marriages explicitly.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - One can also assess temporal change statistically rather than graphically. Looking just at marriages involving clique members, one finds long-term continuities in the preference for intra-clique marriages. In all three periods, individuals were on the order of ten times more likely to marry within their clique than with members of the opposite clique. Though the two cliques were never entirely isolated from each other, there is no evidence of a trend towards greater integration. But the two cliques did differ from each other in one measure.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - clique marriages within the marriage-ban clique involved two patrilines that intermarried in all three time periods spanning the Tang, whereas such marriages were relatively less common (13 percent; n = 136) in the case of the imperial-clan clique. This latter clique evidently evolved more over time, as new families—such as the descendants of Guo Ziyi, hero in the fight against the An Lushan rebels-began to intermarry with the imperial clan over the course of the Tang Dynasty. By contrast, the marriage-ban clique consisted of an established marriage network with origins in the sixth-century or earlier.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - In addition to using network analysis to analyze a marriage network of patrilines, one can also treat individual patrilines themselves as an empirically reconstructed network-roughly speaking a family tree-of people sharing certain common characteristics.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - One example of a shared characteristic involves the home base” of a patriline, which—as used in Figure 1 and Table 1-is defined as the place where the family buried its dead. Given the desirability to bury kin together, one can surmise the home base of most individual members of a patriline in a particular period of time based on the discovery of only a small number of family epitaphs. A second example involves marriage ties. Though one usually does not know the marriage partner of a given individual, one can-as we have seen-get a sense of a patrilines pattern of marriage alliances based on the sample of known marriages. On this basis, one can surmise with what type of family an individual kinsman is likely to have intermarried. In sum, reconstructing patrilineal kinship networks helps one to fill in inevitable gaps in the historical record.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - Here I will apply a network-based approach to investigate the evolution in the composition of chief ministers during the Tang period. The 364 men from 246 different patrilines known to have served as chief minister at some point in the Tang constitute a useful sample of political elites. The list of chief ministers is probably comprehensive, so one need not worry excessively about biases regarding whose name survives. In addition, as a result of the hard work of eleventh-century genealogists, the families of chief ministers are relatively easy to reconstruct using the tables contained in the New Tang History. Finally, it is important to bear in mind that about half of the 246 families of chief ministers supplied only a single chief minister during the entire course of the dynasty. Yet, at the same time, each of these families had an average of about forty-eight documented Tang-era officeholders (a number that will likely increase as new epitaphs are excavated), including men who served in a broad range of different offices. Thus, one can think of the patrilines of chief ministers as representing a broad cross-section of the Tang political elite. Undoubtedly, there were many other very similar families that, due to the vagaries of court politics, never themselves produced a chief minister.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - Perhaps the most important conclusion to draw from this data is that, whereas the Changan elite remained relatively stable in terms of its composition throughout the dynasty, the Luoyang elite changed considerably, especially between the 650s and the 720s. In the early Tang, less than half of Luoyang epitaphs were for individuals from families with strong officeholding traditions, whereas three-quarters to four-fifths were from such families in the 720s and 800s. In addition, over two-fifths (43 percent) of epitaphs from the early period were for individuals who did not themselves hold office (or whose husbands or fathers did not hold office in the case of women and children), whereas the figure drops to 5 percent in the 720s and 12 percent in the early 800s. Simply put, the Luoyang elite had fewer ties to the state in the first decades of the Tang dynasty than it would in later times. Finally, one can look at the kin and marriage networks to which the individuals belonged. Here, too, one notes less change over time among the Changan elite. The Luoyang elite, by comparison, evolved considerably. Only a tiny percentage had demonstrable ties to the political elite marriage network in the early Tang, whereas a clear majority did so by the ninth century. In addition, the forty-one patrilines belonging to the marriage-ban clique came to occupy a disproportionate position in Luoyang society by the late Tang, whereas their presence was barely measurable in the 650s.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - Tables 1 and 2 tabulate data concerning chief ministers divided into four sixty-year periods spanning the Tang Dynasty. The figures are in terms of percentages of tenures in office. In many ways, the pool of chief ministers changed very little over the course of the dynasty, corroborating an observation made long ago by David Johnson. Whereas Johnson focused specifically on the “great clan origins of these men, the two tables presented here suggest continuities in terms of a somewhat different set of metrics. In the case of a substantial majority of chief ministers from all four periods, it is possible to trace their ancestries back to officeholders serving prior to the founding of the Tang, or even prior to the founding of the Sui. The apparent decline in percentages over time (from 87 percent to 70 percent and from 81 percent to 68 percent respectively) is probably attributable in large part to the increased difficulty of tracing genealogies over an ever greater number of generations. In addition, throughout the dynasty, a substantial majority of chief ministers came from patrilines known to have been based in the capital region, with only a tiny percentage in the earlier periods from provincially based patrilines. Indeed, as we shall see below, very few provincial epitaphs have been discovered for such politically powerful families. Finally, most chief ministers in all time periods came from families with ties to the marriage network described above. It is plausible that all of these long-term continuities in the Tang political elite will become even more evident in the future as more data from excavated epitaphs becomes available, permitting more comprehensive reconstructions of kin and marriage ties.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - Aside from these continuities in the characteristics of chief ministers, two long-term changes stand out in Table 1. First, the Luoyang-based marriage-ban clique provided relatively few chief ministers at the beginning of the dynasty, perhaps a consequence of the early Tang emperors animosity towards the marriage-ban clans. By the late eighth century, however, this clique had become as important as the clique surrounding the imperial clan. This trend corroborates an argument made previously by Maeda Aiko regarding the resurgence of the marriage-ban clans as a consequence of the rule of Empress Wu. Second, patrilines whose ancestors had served the Southern Dynasties in pre-Tang times became increasingly insignificant as sources of chief ministers. The possible significance of these two developments will be examined in the Conclusion.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - Although the data suggests that chief ministers throughout the dynasty were overwhelmingly from capital-based patrilines, the home base of a patriline was determined here based on aggregate burial data from the entire dynasty. Such a methodology does not take into consideration when a patriline first established itself in the capital. Previous scholarship has roughly dated the relocation to the capital of the most powerful families to the first half of the dynasty. A more precise understanding of the timing of the relocation is important for understanding the evolution of the geography of political power during the Tang, and, more specifically, for determining to what extent the capital elite had a stranglehold on the upper echelons of the bureaucracy as early as the seventh century. Figure 4 charts the change in the difference between the start of a chief ministers tenure in office and the earliest known capital burial of a member of the chief ministers patriline. A negative number indicates that the start of the ministers tenure preceded the earliest known burial; a positive number indicates that it came after the earliest capital burial. The upward trend (from left to right) in the figure indicates that, by the eighth century, chief ministers were increasingly frequently from patrilines well established in the capital at the time of their elevation to the top office.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - It is tempting to conclude from the trendline, especially given its slope of roughly 1:1, that the late seventh century-that is, between where the trend line and data line cross the x-axis and roughly representing the time of Empress Wus reign—was a key moment when many patrilines relocated from elsewhere to the capital. But we should be careful not to draw such a conclusion prematurely. A smaller number of epitaphs relatively speaking have been discovered dating to the first half of the seventh century. Moreover, as noted below, epitaphs prior to the eighth century tend to identify only one or two generations of ancestors. The result is that kinswomen and kinsmen of chief ministers, as well as descendants of the great families, are somewhat more difficult to identify in seventh-century epitaphs. It is perhaps telling that, as of yet, the only epitaphs that I have encountered demonstrating earlier provincial burials of members of capital-based chief minister patrilines date to the 620s or earlier. It is plausible that epitaphs excavated in the future will reveal that nearly all “capital-based patrilines in Table 1, even those of seventh-century chief ministers, were well established in the capital many years before the respective chief ministers tenure in office.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - A third way to assess the evolution of the Tang elite is to take cross-sections of it during different periods of time, tabulating the composition of each cross-section based on a comprehensive survey of excavated epitaphs. Here, I focus on epitaphs from the two capital cities of Changan and Luoyang that date to three time periods: 650–660, 720–730, and 800–810. These particular time slices allow one to assess the social landscape at the capitals both pre- and post-Empress Wu, as well as pre- and post-An Lushan. Unlike the survey of chief ministers, this survey looks well beyond the pinnacle of the political elite. Indeed, not all epitaphs were composed for officeholders. Elsewhere, I have argued that a survey of excavated epitaphs from a single region provides a relatively representative sample of the economic elite based there. At the capital, the economic elite largely overlapped with the political elite, but it did not do so entirely.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - Table 3 presents data on the careers of the deceased individuals and of their immediate family members, as well as on the broader kin and marriage network to which the deceased belonged. The table begins by tabulating the “officeholding tradition,” a measure of the share of close relatives of the deceased who held office. A strong tradition indicates that most generations (including those of the grandfather, father, deceased or deceaseds spouse, and deceaseds children) held office. “None” indicates there are no known officeholders among the deceaseds close relatives—that is, that the deceased in essence did not belong to the political elite no matter how it is defined.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - For the historian, the principle drawback of relying upon choronyms (and the reason I have preferred to focus on empirically reconstructed patrilines) is that prestigious choronyms were easily concocted (either by the deceaseds family or the author of the epitaph), albeit fictive claims were probably more rampant in the provinces than at the capital. Nevertheless, a study of choronyms does offer an additional perspective on the composition of the political elite, because it involves an alternative sample of individuals-namely marriage partners and other affines mentioned in an epitaph by choronym and surname but whose patrilines cannot be conclusively identified. In addition, analyzing choronym use-especially the contexts in which choronyms were invoked-sheds light on strategies used by elites to maintain their social distinction within society.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - The first four rows of Table 4 tabulate the frequency of claims to choronyms located in four regions: the northeast (defined as lying north of 33°N latitude and east of 113°E longitude, corresponding to Hebei and eastern Hedong and Henan); the northwest (north of 33°N latitude and west of 108°E longitude, corresponding to western Shaanxi, as well as the Gansu Corridor); the south (south of 33°N latitude, i.e. roughly speaking south of the Huai River); and Hebei (a sub-region of the northeast). It should be evident that clans claiming northeastern, northwestern, or southern choronyms were not necessarily affiliated with the pre-Sui regimes based in the northeast, northwest, and south, respectively, though many were. The data suggests that claims to northwestern choronyms (like Longxi) remained stable throughout the dynasty, whereas there was a decline in the use of southern choronyms, probably reflecting the decline in political significance of southern families, a trend alluded to above. There was also a substantial surge in the use of northeastern choronyms, largely attributable to the greater numbers of claims to Hebei ones, including notably those used by marriage-ban clans, such as Qinghe, Boling, and Zhaojun. Indeed, among claims to specific choronym-surname combinations, one sees a marked increase in references to the marriage-ban clans in particular (row 5 of Table 4).
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - The next three rows of the table refer to two Tang-era lists of great clans found in manuscript form among the Dunhuang documents. These lists, well known to historians, appear to identify the most famous surnames from each prefecture. One sees a steady rise over the course of the dynasty in claims to choronym-surname combinations that appear on these lists. If one takes into consideration other reconstructed lists of Tang great clans that attempt to fill in the gaps in the manuscript lists (row 9 of the table), one finds a sharp decline in claims of descent from unrecognized clans. A plausible explanation is that these trends reflect a heightened awareness among epitaph authors of which choronym-surname combinations were most prestigious and most worthy of recording. The last two rows of the table are also suggestive of a greater interest over time in an individuals officeholding relatives. Whereas in the mid-seventh century, the typical Changan and Luoyang epitaph identified only two immediate ancestors, it was common by the 720s to identify three immediate ancestors—usually the great grandfather, grandfather, and father (see row 10). This trend coincided with a greater tendency in epitaphs to identify the deceaseds officeholding maternal grandfather or father-in-law (data not shown), reflecting the importance of good marriages as well as patrilineal ancestry in defining the worth of an individual.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - Finally, row 11 of the table calculates the frequency with which choronyms precede the surname of the deceased in the title of the epitaph, a measure of the choronyms prominence within the text of the epitaph. Taken together, I propose that the data in rows 6 to 11 point to a growing sense among scions of the most famous families that it was necessary to reassert explicitly the prestige of the family.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - What repercussions did the aforementioned developments have on provincial elites? In this final section, I shift focus from what we can learn about the structure of the Tang elite from capital epitaphs to what we can learn from provincial ones. The sample consulted for the discussion that follows consists of most published Sui-Tang-era epitaphs from regions of north China beyond the Capital Corridor, excluding the prefecture of Luzhou. It is beyond the scope of this brief survey to examine regional differences between the provinces, so I treat this sample of 800 or so inscriptions as representing a generalized cross-section of the empires provincial elite. In terms of temporal distribution, one notes an important lull in epitaph production in the 620s through 650s (Figure 5). Unfortunately, this means that, although one can discern much about the Sui Period and the period accompanying the rise of Empress Wu, one has far less data on the decades immediately following the Tang founding.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - What can we learn from this sample of epitaphs? One of the most important trends transforming provincial elite society of the Sui and Tang involved declining ties between the provinces and the center of imperial power. The diminished political importance of provincials can be measured using a variety of different metrics. One can see it in the ever smaller fraction of epitaphs composed for members of the high imperial elite-that is, individuals with immediate family members serving in high-ranking positions in the civil bureaucracy—as well as the increased fraction of epitaphs composed for individuals with no officeholders in their immediate family (Table 5).
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - The sharpest drop came sometime after the founding of the Tang. It is conceivable that there would have been a further decline in ties to officialdom in the post-An Lushan period had the provincial governments not become major employers of provincial elites (as we shall see below). Of course, “non-officeholders” were not necessarily individuals with no connections to the state. Some undoubtedly served in the army—but without holding an office-or they assisted the local administration as a sort of liaison to local society. Such men appear to have sometimes received honorary titles from the state in compensation for their contributions, titles that did not confer any political authority. There were also individuals eligible for service, who may have sought office, but who were never assigned a substantive post. And then there were those who received special titles granted to elderly members of the local elite who had reached the age of 80. Such connections to the state among non-officeholders does much to suggest the hegemony of imperial power in local society. But one should not assume these non-officeholders held sway beyond their own local society; they had nowhere near the political clout of the capital-based bureaucratic elite.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - How does one account for the exceptional cases of Tables 5 and 6? Who were the provincials who apparently enjoyed ties to the heights of imperial power? Among the fifteen ninth-century epitaphs for provincials participating in the capital-based marriage network, six involved Hebei governors or their close relatives, families known to have intermarried with the imperial clan. One finds, for example, epitaphs of one of the governors of the autonomous Hebei province of Chengde and of his daughter. The governor had married the granddaughter of Emperor Xianzong-in what was essentially a dynastic marriage meant to draw the governor back into the Tang fold. Of the remaining nine cases (a group that included all three ninth-century family members of chief ministers represented in Table 6), the majority were probably individuals participating in a well-documented survival strategy involving outward migration from the capital. Facing difficulty surviving in the hypercompetitive environment at the capital, some scions of prominent political families took advantage of appointments in the provinces to obtain land and reestablish themselves in the provinces. In so doing, they were in a good position to remain important in local society, even if they rapidly lost the connections necessary to secure offices at the center for themselves and their descendants. Thus, for example, there is the epitaph of Wei Xiang 魏湘 (804–854), who served in office in autonomous Weibo Province, where his father had also served. Though he was a direct sixth-generation descendant of the famous early Tang chief minister Wei Zheng 魏徵 (580–643), by the time of his fathers generation, his branch of the family had already committed themselves to service in Weibo. Men like Wei Xiang, though they may appear to have maintained enduring ties to the high political elite, in fact represented families with rapidly dwindling connections to the capital and capital society.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - A second major trend affecting Sui-Tang provincial elite society involved the impact of a new layer of regional administration established in the post-An Lushan Period, that of provincial governments. Whereas prefectures had constituted the highest level of regional government in the early Tang, provinces were established in the post-An Lushan Period that now played an oversight role over their subordinate prefectures and also oversaw regional armed forces. This province-level of administration-headed by a governor (usually jiedushi or guanchashi)-rapidly became a major employer of provincial elites (Table 7).
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - As is now well known, provincial governments also employed capital elites, including ambitious men early in their careers. However, unlike provincials, capital elites took positions as upper-echelon civilian bureaucrats in the provincial governments, positions that, according to court directives, provided them with fast-track promotion opportunities into the regular bureaucracy. Many of these men were initially hired by the governor at the capital (i.e., selected from among the capital-based elite) even before he left for his tenure in office. Provincial elites by contrast-with the notable exception of those inhabiting the autonomous provinces in Hebei—took either military offices or lower-echelon civilian positions, positions that precluded any hope of rising to higher office.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - A third major trend evident from the survey of provincial epitaphs accompanied the creation of provincial governments in the immediate post-An Lushan Period. In brief, provincial officeholders increasingly came to serve in their home locales (Table 8). In earlier times, the majority of officeholders served in the regular provincial bureaucracy, or in offices in the fubing “militia system-a system in which soldiers were rotated between the Sixteen Guards at the capital and provincial garrisons attached to one of the capital guards. These appointments seem to have adhered strictly to the law of avoidance, such that individuals traveled away from home to take up their posts. In the case of fubing appointments (particularly common among Hedong and Xiazhou elites), ordinary militia soldiers may have been locals, but epitaph data suggests that officers generally served away from home, either at an appointment at the capital or at a provincial garrison hundreds of kilometers away. The establishment of provincial governments ushered in a radical departure from earlier practice. With the law of avoidance not enforced in provincial governments—which hired their staffs without direct central government involvement-new employment opportunities became available for locals to serve at home. Except in the autonomous provinces, upper-echelon civilian officials (including the governor himself) were typically outsiders, usually selected from among the capital-based elite, but the remainder of the staff consisted of men recruited locally.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - This paper has sought via four mini-studies-on the capital-based marriage network of political elites, on the backgrounds of the chief ministers, on the composition of the capital elite during three time slices, and on the makeup of the provincial elite-to offer new insight on how the political elite evolved over the course of the Tang Dynasty. One conclusion one can draw from this analysis is that the political elite was marked as much by continuities over time as by discontinuities.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - Throughout the dynasty, one recognizes the same marriage network of politically influential patrilines, patrilines that had served in office generation after generation since before the founding of the dynasty. These patrilines supplied most of the Tang chief ministers, and presumably the lions share of other top officials. The network constituted a capital elite insofar as the families were almost all based in one of the two capital cities by no later than the late seventh century—even earlier than has often been assumed. Moreover, throughout the dynasty, the network was structured in a similar way, with two prominent marriage cliques at its core: one consisting primarily of the preeminent “marriage-ban clans and one organized around the Tang imperial Li clan.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - Much has been made of attempts by the Sui and early Tang to weaken the power of entrenched provincial families through institutional innovations. But one should not conclude, as many have in the past, that the medieval aristocratic families declined as a consequence. Quite to the contrary, it was precisely by diminishing their influence that institutional developments helped ensure that local elites (except those in the prefectures immediately adjacent to the capital) competed poorly for high office with their capital-based counterparts. Though they continued to serve in low-ranking offices, it was rare already by the mid-seventh century to find provincials serving in high-ranking offices at the capital. Meanwhile, scions of the old pre-Tang elite, now established in the capital cities, were able to benefit from their connections in the capital, and from what was in essence an alliance with the dynasty and the imperial clan. As it adapted to new circumstances through relocation to the capital, aristocratic families were strengthened rather than diminished by the institutional changes and this to the very end of the dynasty.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - The political divide separating the capital from the provinces was further accentuated after the An Lushan Rebellion as a consequence of the establishment of new provincial governments. In the first half of the dynasty, officeholders among local elites tended to serve away from home, in county- or prefectural-level positions all over the empire, or in an office in a fubing militia. In all appearance, official appointments adhered to the law of avoidance,” which forbade men from serving near their home bases. The staffing of the new provincial governments fundamentally changed the situation. It did so not by opening up new avenues of upward mobility for locals, as has been previously argued, but rather by assigning them to offices near their home locales. It was as a consequence of this reorganization of the geography of political power that the provinces came to have a “colonial” relationship with the metropolitan center. By the ninth century, agents of the center were dispatched from the capital to serve in county- or prefectural-level offices, as well as in upper-echelon civilian posts in the provincial governments, where they would remain in office for a fixed term (usually no more than three years) before returning to the capital. Meanwhile, local elites served exclusively in lower-echelon posts or in the local provincial military.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - However, though it was the same “aristocratic families that continued to dominate power throughout the dynasty, there were discontinuities over time in the structure and makeup of the capital elite. Perhaps the most important change involved the rise to much greater political prominence of Luoyang families. Whereas Luoyang had been a secondary political center in the 650s, it grew by the 720s to be on par with Changan as a place where powerful patrilines concentrated. Most notable among the Luoyang-based families acquiring a new influence at court were the marriage-ban clans-that is, those patrilines at the core of one of the two marriage cliques. A more systematic analysis of Luoyang epitaphs dating to the intermediary decades between the 650s and 720s may clarify when precisely the Luoyang elites acquired their new prominence (and when and to what extent families of the marriage-ban clique relocated there from Hebei at this time). But, in all likelihood, Empress Wus reign played a critical role. Not only did she move the imperial court to Luoyang, but she also famously sought to staff the bureaucracy with men loyal to her rather than to the Tang imperial clan. And what better group to elevate than a cluster of prestigious old families that, unlike the families of their rival marriage clique, had not intermarried extensively with the Tang imperial house?
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - The data here also sheds new light on the pre-Sui regional elites. In Chen Yinkes classic account, court politics of the seventh and eighth centuries were driven by factional struggles between three regional blocks—a northwestern, a northeastern, and a southern elite—with origins in the regional regimes of the sixth century. The present study suggests that, though he had access to far less empirical data, Chen was not far off the mark. Indeed, the marriage-ban clique can be thought of as a block of families with predominantly northeastern origins. But one should not overstate the significance of regional origin as a source of political solidarity. The imperial-clan clique was less monolithic than the marriage-ban clique, and it included families from the northeast (and south) as well. Moreover, throughout the dynasty, the two cliques intermarried with each other at a low but relatively consistent rate. Finally, although Chen and others have argued that Empress Wu ushered in a class struggle between an old elite and a newly risen class of bureaucrats, it is more likely given available data that the old aristocratic families faced no significant new rivals. Right into the ninth century, the primary actors in competition with each other for political power came from a single large marriage network of patrilines, nearly all of which had officeholding traditions going back to pre-dynastic times.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - If both cliques and the bulk of the broader capital-based marriage network were composed of northwestern or northeastern families, what might one surmise about the southern elite in the Tang? Historians have long ago noted the dominance of the northern elite at the early Tang court. An analysis of the pre-Sui origins of elite Tang patrilines, as well as of choronym usage in capital epitaphs, suggests that southerners further declined in influence over the course of the Tang Dynasty. Their decline may reflect in part how social capital contributes to political reproduction. Southern patrilines were peripheral members of the marriage network of political elites, putting them at a disadvantage in competing with the core patrilines of the network. There may have been pull factors in addition, encouraging southern families to shun office and remain in the south. As I have previously noted, in the provinces (i.e. away from the capitals and Capital Corridor), it was only in the south that one could still find as late as the ninth century scions of the old aristocratic families inhabiting their places of ancestral origin. These families had no ties to officialdom, yet still managed to survive as a local elite. It is possible that, far from the political center and the much greater concentration of military power in the north, it was easier for families to persist for centuries entrenched in local society, where they perhaps took advantage of new economic opportunities.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - One last point to consider in light of the data presented above is somewhat more speculative, and concerns how cultural capital was deployed as a strategy for maintaining long-term political influence. As others have noted, the Sui and early Tang courts compiled lists of eminent clans from across the realm, identifying these clans by surname and choronym. Though such lists did not guarantee a family easier access to office (as they may have had under earlier dynasties), their very compilation suggests they may have played an informal role in the selection of bureaucrats. It has been argued that the courts decision to abandon such genealogical projects by the mid-eighth century reflected the end of the aristocratic age, especially given that it coincided with a moderate expansion in the use of the civil service examination for bureaucratic recruitment.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - It took centuries for users of print materials to develop the web of trust that now undergirds our current system of publication, dissemination, and preservation, notes Abby Smith, a leading figure in library and preservation circles. Digital documents are disrupting that carefully wrought system by undercutting our expectations of what constitutes a trusted and authentic document and repository. But to make the transition to a new system requires not just technical measures (such as digital signatures and watermarks) but, as Clifford Lynch, the executive director of the Coalition for Networked Information, observes, also figuring out responsibility for guaranteeing claims of authorship and financing for a system of authentication and integrity management.
[Author: Nicolas Tackett; From essay:"The Evolution of the Tang Political Elite and its Marriage Network "] - How then would certain illustrious patrilines with long histories of bureaucratic service have adapted effectively to changing circumstances, as the empirical data would suggest they did? The greater density of intermarriages between marriage-ban clans in the post-An Lushan period reflected the more extensive deployment of social capital as a resource for political reproduction. This strategy was combined with concerted efforts by the old families to articulate clearly their distinction from the rest of society. Choronyms became a more explicit component of their identity, as reflected in their greater use in the title lines of epitaphs. But choronyms were insufficient in themselves to define the most powerful elites, as increasing numbers of descendants of the great families could claim such illustrious pedigrees. Thus, simultaneously, the most eminent and successful branches of the aristocracy made renewed efforts in their funerary biographies (and presumably in other public writings) to lionize the officeholders among their immediate ancestors (including three generations up the patriline) and relatives by marriage (especially maternal grandfathers and fathers-in-law). Not coincidentally, it was in the ninth century (and, as far as I can tell, not earlier) that one encounters a number of epitaphs with long and elaborate celebrations of the deceased for her or his familys generations of officeholding and generations of good marriages. These ways of asserting one familys eminence became part of a multi-pronged strategy for maintaining political power under changing circumstances.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - ON OCTOBER 11, 2001, THE SATIRIC Bert Is Evil web site, which displayed photographs of the furry Muppet in Zelig-like proximity to villains such as Adolf Hitler (see Figure 1), disappeared from the web-a bit of collateral damage from the September 11th attacks. Following the strange career of Bert Is Evil shows us possible futures of the past in a digital era-futures that historians need to contemplate more carefully than they have done so far.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - In 1996, Dino Ignacio, a twenty-two-year-old Filipino web designer, created Bert Is Evil (brought to you by the letter H and the CIA), which became a cult favorite among early tourists on the World Wide Web. Two years later, Bert Is Evil won a Webby as the best weird site. Fan and mirror sites appeared with some embellishing on the Bert Is Evil theme. After the bombing of the U.S. embassies in Kenya and Tanzania in 1998, sites in the Netherlands and Canada paired Bert with Osama bin Laden.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - This image made a further global leap after September 11. When Mostafa Kamal, the production manager of a print shop in Dhaka, Bangladesh, needed some images of bin Laden for anti-American posters, he apparently entered the phrase Osama bin Laden in Googles image search engine. The Osama and Bert duo was among the top hits. Sesame Street being less popular in Bangladesh than in the Philippines, Kamal thought the picture a nice addition to an Osama collage. But when this transnational circuit of imagery made its way back to more Sesame Street-friendly parts of the world via a Reuters photo of anti-American demonstrators (see Figure 2), a storm of indignation erupted. Childrens Television Workshop, the shows producers, threatened legal action. On October 11, 2001, a nervous Ignacio pushed the delete key, imploring all fans [sic] and mirror site hosts of Bert is Evil to stop the spread of this site too.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - Ignacios sudden deletion of Bert should capture our interest as historians since it dramatically illustrates the fragility of evidence in the digital era. If Ignacio had published his satire in a book or magazine, it would sit on thousands of library shelves rather than having a more fugitive existence as magnetic impulses on a web server. Although some historians might object that the Bert Is Evil web site is of little historical significance, even traditional historians should worry about what the digital era might mean for the historical record. U.S. government records, for example, are being lost on a daily basis. Although most government agencies started using e-mail and word processing in the mid-1980s, the National Archives still does not require that digital records be retained in that form, and governmental employees profess confusion over whether they should be preserving electronic files. Future historians may be unable to ascertain not only whether Bert is evil, but also which undersecretaries of defense were evil, or at least favored the concepts of the evil empire or the axis of evil. Not only are ephemera like Bert and government records made vulnerable by digitization, but so are traditional works- books, journals, and film-that are increasingly being born digitally. As yet, no one has figured out how to ensure that the digital present will be available to the futures historians.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - But, as we shall see, tentative efforts are afoot to preserve our digital cultural heritage. If they succeed, historians will face a second, profound challenge-what would it be like to write history when faced by an essentially complete historical record? In fact, the Bert Is Evil story could be used to tell a very different tale about the promiscuity and even persistence of digital materials. After all, despite Ignacios pleas and Childrens Television Workshops threats, a number of Bert mirror sites persist. Even more remarkably, the Internet Archive-a private organization that began archiving the web in 1996-has copies of Bert Is Evil going back to March 30, 1997. To be sure, this extraordinary archive is considerably more fragile than one would like. The continued existence of the Internet Archive rests largely on the interest and energy of a single individual, and its collecting of copyrighted material is on even shakier legal ground. It has put the future of the past-traditionally seen as a public patrimony-in private hands.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - Still, the astonishingly rapid accumulation of digital data-obvious to anyone who uses the Google search engine and gets 300,000 hits-should make us consider that future historians may face information overload. Digital information is mounting at a particularly daunting rate in science and government. Digital sky surveys, for example, access over 2 billion images. Even a dozen years ago, NASA already had 1.2 million magnetic tapes (many of them poorly maintained and documented) with space data. Similarly, the Clinton White House, by one estimate, churned out 6 million e-mail messages per year. And NARA is contemplating archiving military intelligence records that include more than 1 billion electronic messages, reports, cables, and memorandums.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - While some interfaces, such as the Internet Archive, promote skimming and context when exploring collections, most do not. Slow loading times, page-by-page PDF downloads, and results lists all convey that a platform is designed to be primarily searched via keyword rather than systemic skimming or reading. This approach brings risks as the following example demonstrates.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - Thus historians need to be thinking simultaneously about how to research, write, and teach in a world of unheard-of historical abundance and how to avoid a future of record scarcity. Although these prospects have occasioned enormous commentary among librarians, archivists, and computer scientists, historians have almost entirely ignored them. In part, our detachment stems from the assumption that these are technical problems, which are outside the purview of scholars in the humanities and social sciences. Yet the more important and difficult issues about digital preservation are social, cultural, economic, political, and legal issues that humanists should excel at. The system for preserving the past that has evolved over centuries is in crisis, and historians need to take hand in building a new system for the coming century. Historians also tend to assume a professional division of responsibility, leaving these matters to archivists. But the split of archivists from historians is a relatively recent one. In the early twentieth century, historians saw themselves as having a responsibility for preserving as well as researching the past. At that time, the vision and membership of the American Historical Association- embracing archivists, local historians, and amateurs as well as university scholars-was considerably broader than it later became.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - Ironically, the disruption to historical practice (to what Thomas Kuhn called normal science) brought by digital technology may lead us back to the future. The struggle to incorporate the possibilities of new technology into the ancient practice of history has led, most importantly, to questioning the basic goals and methods of our craft. For example, the Internet has dramatically expanded and, hence, blurred our audiences. A scholarly journal like this one is suddenly much more accessible to high school students and history enthusiasts. And the work of history buffs is similarly more visible and accessible to scholars. We are forced, as a result, to rethink who our audiences really are. Similarly, the capaciousness of digital media means that the page limits of journals like this one are no longer fixed by paper and ink costs. As a result, we are led to question the nature and purpose of the scholarly journals-why do they publish articles with particular lengths and structures? Why do they publish particular types of articles? The simultaneous fragility and promiscuity of digital data requires yet more rethinking-about whether we should be trying to save everything, who is responsible for preserving the past, and how we find and define historical evidence.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - Historians, in fact, may be facing a fundamental paradigm shift from a culture of scarcity to a culture of abundance. Not so long ago, we worried about the small numbers of people we could reach, pages of scholarship we could publish, primary sources we could introduce to our students, and documents that had survived from the past. At least potentially, digital technology has removed many of these limits: over the Internet, it costs no more to deliver the AHR to 15 million people than 15,000 people; it costs less for our students to have access to literally millions of primary sources than a handful in a published anthology. And we may be able to both save and quickly search through all of the products of our culture. But will abundance bring better or more thoughtful history?
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - Historians are not unaware of these challenges to the ways that we work. Yet, paradoxically, these fundamental questions are often relegated to more marginal professional spaces-to casual lunchtime conversations or brief articles in association newsletters. But in this time of rapid and perplexing changes, we need to engage with issues about access to scholarship, the nature of scholarship, the audience for scholarship, the sources for scholarship, and the nature of scholarly training in the central places where we practice our craft-scholarly journals, scholarly meetings, and graduate classrooms. That scholarly engagement should also lead us, I believe, to public action to advocate the preservation of the past as a public responsibility-one that historians share. But I hope to persuade even those who do not share my particular political stance that professional historians need to shift at least some of their attention from the past to the present and future and reclaim the broad professional vision that was more prevalent a century ago. The stakes are too profound for historians to ignore the futures of the past.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - ALTHOUGH HISTORIANS HAVE MOSTLY BEEN SILENT, archivists, librarians, public officials, and others have loudly warned about the threatened loss of digital records and publications for at least two decades. Words such as disaster and crisis echo through their reports and conference proceedings. As early as 1985, the Committee on the Records of Government declared, the United States is in danger of losing its memory. More than a dozen years later, a project called Time and Bits: Managing Digital Continuity brought together archivists, librarians, and computer scientists to address the problem once again. Conferees watched the Terry Sanders film Into the Future: On the Preservation of Knowledge in the Electronic Age, and some likened it to Rachel Carsons Silent Spring and themselves to the environmentalists of the 1960s and 1970s. A Time and Bits web site assembled conference materials and promoted ongoing digital dialogue. But, as if to prove the conferences point, the site disappeared in less than a year. Computer scientist Jeff Rothenberg may have been over-optimistic when he quipped, Digital documents last forever-or five years, whichever comes first.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - Those worried about a problem like digital preservation that lacks public attention are prone to exaggerate. Probably the greatest distortion has been the implicit suggestion that we have somehow fallen from a golden age of preservation in which everything of importance was saved. But much-really, most-of the record of previous historical eras has disappeared. The members of prehistoric societies did not think they lived in prehistoric times, Washington Post writer Joel Achenbach observes. They merely lacked a good preservation medium. And non-digital records that have survived into this century-from Greek and Chinese antiquities to New Guinean folk traditions to Hollywood films are also seriously threatened.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - Such questions are particularly hard to answer since digitization also undercuts our sense of who owns such materials and, thus, who has the right and responsibility to preserve them. Consumers (including libraries) have traditionally purchased books and magazines under the first sale doctrine, which gives those who buy something the right to make any use of it, including lending or selling it to others. But most digital goods are licensed rather than sold. Because contract law governs licenses, vendors of digital content can set any restrictions they choose-they can say that the contents may not be copied or cannot be viewed by more than one person at a time. Adobes eBook reader even includes a warning that a book may not be read aloud.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - Another exaggeration involves stories about the grievous losses that never occurred. One widely repeated story is that computers can no longer read the data tapes from the 1960 U.S. Census. In truth, as Margaret Adams and Thomas Brown from the National Archives have shown, the Census Bureau had by 1979 successfully copied almost all the records to newer industry-compatible tapes. Yet, even in debunking one of the persistent myths of the digital age, Adams and Brown reveal some of the key problems. In just a decade and a half, migrating the census tapes to a readable format represented a major engineering challenge-hardly something we expect to face with historical records originating from within our own lifetimes. And although only 1,575 records ... could not be copied because of deterioration, the absolute nature of digital corrosion is sobering. Print books and records decline slowly and unevenly-faded ink or a broken-off corner of a page. But digital records fail completely-a single damaged bit can render an entire document unreadable. Here is the key difference from the paper era: we need to take action now because digital items very quickly become unreadable, or recoverable only at great expense.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - This has already happened-albeit not as much as sometimes suggested. Ten to twenty percent of vital data tapes from the Viking Mars mission, notes Deanna Marcum, the president of the Council on Library Information Resources, have significant errors because magnetic tape is too susceptible to degradation to serve as an archival storage medium. Often, records lack sufficient information about their organization and coding to make them usable. According to Kenneth Thibodeau, director of the National Archive and Record Administrations Electronic Records Archives program, NARA lacked adequate documentation to make sense of several hundred reels of computer tapes from the Department of Health and Human Resources and data files from the National Commission on Marijuana and Drug Abuse. Some records could be recovered by future digital archaeologists but sometimes only through an unaffordable major engineering challenge. The greatest concern is not over what has already been lost but what historians in fifty years may find that they cant read.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - Many believe-incorrectly-the central problem to be that we are storing information on media with surprisingly short life spans. To be sure, acid-free paper and microfilm last a hundred to five hundred years, whereas digital and magnetic media deteriorate in ten to thirty years. But the medium is far from the weakest link in the digital preservation chain. Well before most digital media degrade, they are likely to become unreadable because of changes in hardware (the disk or tape drives become obsolete) or software (the data are organized in a format destined for an application program that no longer works). The life expectancy of digital media may be as little as ten years, but very few hardware platforms or software programs last that long. Indeed, Microsoft only supports its software for about five years.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - The most vexing problems of digital media are the flipside of their greatest virtues. Because digital data are in the simple lingua franca of bits, of ones and zeros, they can be embodied in magnetic impulses that require almost no physical space, be transmitted over long distances, and represent very different objects (for instance, words, pictures, or sounds as well as text). But the ones and zeros lack intrinsic meaning without software and hardware, which constantly change because of technological innovation and competitive market forces. Thus this lingua franca requires translators in every computer application, which, in turn, operate only on specific hardware platforms. Compounding the difficulty is that the languages being translated keep changing every few years.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - The problem is still worse because of the ability of digital media to create and represent complex, dynamic, and interactive objects-another of their great virtues. Even relatively simple documents that appear to have direct print analogs turn out to be more complex. Printing out e-mail messages makes rapid searches of them impossible and often jettisons crucial links to related messages and attachments. In addition, multimedia programs, which generally rely on complicated combinations of hardware and software, quickly become obsolete. Nor is there any good way to preserve interactive and experiential digital creations. That is most obviously true of computer games and digital art, but even a large number of ordinary web pages are generated out of databases, which means that the specific page you view is your own creation and the system can create an infinite number of pages. Preserving hypertextually linked web pages poses the further problem that to save a single page in its full complexity could ultimately require you to preserve the entire web, because virtually every web page is linked to every other. And the dynamic nature of databases destabilizes mundane business and governmental records since they are often embedded in systems that automatically replace old data with new-a changeability that, notes archival educator Richard Cox, threatens the records of any modern day politician, civic leader, businessperson, military officer, or leader.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - WHILE THESE TECHNICAL DIFFICULTIES ARE IMMENSE, the social, economic, legal, and organizational problems are worse. Digital documents-precisely because they are in a new medium-have disrupted long-evolved systems of trust and authenticity, ownership, and preservation. Reestablishing those systems or inventing new ones is more difficult than coming up with a long-lived storage mechanism.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - How, for example, do we ensure the authenticity of preserved digital information and trust in the repository? Paper documents and records also face questions about authenticity, and forgeries are hardly unknown in traditional archives. The science of diplomatics, in fact, emerged in the seventeenth century as a way to authenticate documents when scholars confronted rampant forgeries in medieval documents. But digital information-because it is so easily altered and copied, lacks physical marks of its origins, and, indeed, even the clear notion of an original-cannot be authenticated as physical documents and objects can. We have, for example, no way of knowing that forwarded e-mail messages we receive daily have not been altered. In fact, the public archive of Usenet discussion groups contains hundreds of deliberately and falsely attributed messages. Fakery, write David Bearman and Jennifer Trant, has not been a major issue for most researchers in the past, both because of the technical barriers to making plausible forgeries, and because of the difficulty with which such fakes entered an authoritative information stream. Digital media, tools, and networks have altered the balance.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - But if libraries dont own digital content, how can they preserve it? The problem will become even worse if publishers widely adopt copy protection schemes as they are seriously considering doing for electronic books. Even a library that had the legal right to preserve the content would have no reason to assume that it would be able to do so; meanwhile, the publisher would have little incentive to keep the protection system functioning in a new software environment. In general, digital rights management systems and other forms of trusted computing undercut preservation efforts by embedding centralized control in proprietary systems. If Microsoft, or the U.S. government, does not like what you said in a document you wrote, speculates Free Software advocate Richard Stallman, they could post new instructions telling all computers to refuse to let anyone read that document.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - Licensed and centrally controlled digital content not only erodes the ability of libraries to preserve the past, it also undercuts their responsibility. Why should a library worry about the long-term preservation of something it does not own? But then, who will? Publishers have not traditionally assumed preservation responsibility since there is no obvious profit to be made in ensuring that something will be available or readable in a hundred years when it is in the public domain and cant be sold or licensed.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - The digital era has not only unsettled questions of ownership and preservation for traditional copyrighted material, it has also introduced a new, vast category of what could be called semi-published works, which lack a clear preservation path.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - The free content available on the web is protected by copyright even though it has not been formally registered with the Library of Congress Copyright Office or sold by a publisher. That means that a library that decided to save a collection of web pages-say, those posted by abortion rights organizations-would technically be violating copyright. The absence of this process is the most fundamental problem facing digital preservation. Over centuries, a complex (and imperfect) system for preserving the past has emerged. Digitization has unsettled that system of responsibility for preservation, and an alternative system has not yet emerged. In the meantime, cultural and historical objects are being permanently lost.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - Four different systems generally preserve cultural and historical documents and objects. Research libraries take responsibility for books, magazines, and other published cultural works, including moving images and recorded sound. Government records fall under the jurisdiction of the National Archives and a network of state and local archives. Systems for maintaining other cultural and historical materials are less formal or centralized. Records and papers from businesses, voluntary associations, and individuals have found their way into local historical societies, specialized archives, and university special collections. Finally, the semi-published body of material we have called ephemera has been most often saved by enthusiastic individuals-for example, postcard and comic book collectors-who might later deposit their hoard in a permanent repository.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - While research libraries have tried to save relatively complete sets of published works, other historical sources have generally only been preserved in a highly selective and sometimes capricious fashion-what archivists call preservation through neglect. Materials that lasted fifty or one hundred years found their way into an archive, library, or museum. Although this inexact system has resulted in many grievous losses to the historical record, it has also given us many rich collections or personal and organizational papers and ephemera.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - But this system will not work in the digital era because preservation cannot begin twenty-five years after the fact. What might happen, for example, to the records of a writer active in the 1980s who dies in 2003 after a long illness? Her heirs will find a pile of unreadable 5¼ floppy disks with copies of letters and poems written in WordStar for the CP/M operating system or one of the more than fifty now-forgotten word-processing programs used in the late 1980s.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - Government archives similarly continue to rely on the unwarranted assumption that records can be appraised and accessioned many years after their creation. A recent study, Current Recordkeeping Practices within the Federal Government, which surveyed more than forty federal agencies, found widespread confusion about policies and procedures for managing, storing, and disposing of electronic records and systems. Government employees, it concluded, do not know how to solve the problem of electronic records-whether the electronic information they create constitutes records and, if so, what to do with the records. Electronic files that qualify as records-particularly in the form of e-mail, and also word processing and spreadsheet documents are not being kept at all as records in many cases.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - This uncertainty and disarray would not be so serious if we could assume that it could be simply sorted out in another thirty years. But if we hope to preserve the present for the future, then the technical problems facing digital preservation as well as the social and political questions about authenticity, ownership, and preservation policy need to be confronted now.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - AT LEAST INITIALLY, archivists and librarians tended to assume that a technical change the rise of digital media-required a technical solution. The simplest technical solution has been to translate digital information into something more familiar and reassuring like paper or microfilm. But, as Rothenberg points out, this is a rear-guard action that destroys unique functionality (such as dynamic interaction, nonlinearity, and integration) and core digital attributes (perfect copying, access, distribution, and so forth) and sacrifices the original form, which may be of unique historical, contextual, or evidential interest.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - By February 2002, the Internet Archive (IA) had gathered a monumental collection of more than 100 terabytes of web data-about 10 billion web pages or five times all the books in the Library of Congress-and was gobbling up 12 terabytes more each month. That same fall, it began offering public access to most of the collection through what Kahle called the Wayback Machine-a wry reference to the device used by the time-traveling Mr. Peabody in the Rocky and Bullwinkle cartoons of the 1960s. Astonishingly, a single individual with a very small staff has created the worlds largest database and library in just five years.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - Another backward-looking solution is to preserve the original equipment. If you have files created on an Apple II, then why not keep one in case you need it? Well, sooner or later, a disk drive breaks or a chip fails, and unless you have a computer junkyard handy and a talent for computer repair, you are out of luck. Technological preservation, moreover, requires intervention before it is too late to save not just the files but also the original equipment. The same can be said of what is probably the most widely accepted current method of digital preservation-data migration, or moving the documents from a medium, format, or computer technology that is becoming obsolete to one that is becoming more common. When the National Archives saved the 1960 U.S Census tapes, they used migration, and large organizations use this strategy all the time-moving from one accounting system to another. Because we have lots of experience migrating data, we also know that it is time consuming and expensive. One estimate is that data migration is equivalent to photocopying all the books in a library every five years.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - Some like Rothenberg also worry, for example, about the loss of functionality in migrating digital files. Moreover, the process cant be automated because migration requires a unique new solution for each new format or paradigm and each type of document that is to be converted into that new form. Rothenberg is also derisive about the practice of translating documents into standardized formats and then re-translating as new formats emerge, which he finds analogous to translating Homer into modern English by way of every intervening language that has existed during the past 2,500 years.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - Rothenbergs favored alternative is emulation-developing a system that works on later generations of hardware and software but mimics the original. In principle, a single emulation solution could preserve a vast store of digital documents. In addition, it holds the greatest promise for preserving interactive and multimedia digital creations. But critics of emulation tellingly note that it is only a theoretical solution. Probably the best strategy is to reject the all-or-nothing, magic-bullet approaches implicit in the proposals of the most passionate advocates of any particular strategy-whether creating hard copies, preserving old equipment, migrating formats, or emulating hardware and software. Margaret Hedstrom, one of the leading figures in digital preservation research, argues persuasively that the search for the Holy Grail of digital archiving is premature, unrealistic, and possibly counter-productive. Instead, we need to develop solutions that are appropriate, effective, affordable and acceptable to different classes of digital objects that live in different technological and organizational contexts.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - But even the most calibrated mix of technical solutions will not save the past for the future because, as we have seen, the problems are much more than technical and involve difficult social, political, and organizational questions of authenticity, ownership, and responsibility. Multiple experiments and practices are under way-more than can be discussed here. But I want to focus on some widely discussed approaches or experiments as illustrative of some of the possibilities and continuing problems.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - ONE OF THE EARLIEST AND MOST INFLUENTIAL approaches to digital preservation (and digital authenticity) was what archivists call the Pitt Project, a three-year (1993-1996) research effort funded by the National Historical Publications and Records Commission (NHPRC) and centered at the University of Pittsburgh School of Information and Library Studies. For historians, what is most interesting (and sometimes puzzling) about the Pitt Project approach is the way that it simultaneously narrows and broadens the role of archives and archivists through its focus on records as evidence rather than information. Records, David Bearman and Jennifer Trant explain, are that which was created in the conduct of business and provide evidence of transactions. Data or information, by contrast, Bearman dismisses as non-archival and unworthy of the archivists attention. From this point of view, the governments record of your Social Security account is vital but not the information contained in letters that you and others might have written complaining about the idea of privatizing Social Security.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - The Pitt Project produced a pathbreaking set of functional requirements for evidence in electronic record keeping-in effect, strategies and tactics to ensure that electronic records produce legally or organizationally acceptable evidence of their transactions. Such a focus responds particularly well to worries about the authenticity of electronic records. But for historians (and for some archivists), the focus on records as evidence rather than records as sources of information, history, or memory seems disappointingly narrow. Moreover, as Canadian archivist Terry Cook points out, the emphasis on redesigning computer systems functional requirements to preserve the integrity and reliability of records and assigning long-term custodial control... to the creator of archival records privileges the powerful, relatively stable, and continuing creators of records capable of such reengineering and ignores artists, activists, and marginalized and weaker members of society who have neither the resources nor inclination to produce business acceptable communications.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - While the Pitt Project emphasizes archival professionalism, a narrowing of the definition of recordkeeping, a rejection of the custodial tradition in archives, and planning for more careful collecting in the future rather than action in the present, the Internet Archive has taken precisely the opposite approach. It represents a grass-roots, immediate, enthusiast response to the crisis of digital preservation that both expands and further centralizes archival responsibility in ways that were previously unimaginable. Starting in September 1996, Brewster Kahle and a small staff sent crawlers out to capture the web by moving link-by-link and completing a full snapshot every two months. Although in part a philanthropic venture funded by Kahle, the Internet Archive also has a commercial side. Kahles for-profit web navigation service, Alexa Internet (bought by Amazon in 1999 for $300 million), is what actually gathers the web snapshots, which it uses to analyze patterns of web use, and then donates them to the Internet Archive.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - Even industry-friendly sources like the World Shipping Council admit that thousands of containers are lost each year, sinking to the ocean floor or drifting loose. Some carry toxic substances that leak into the oceans; others release thousands of yellow rubber ducks that wash ashore around the world over decades. Typically, workers spend almost six months at sea, often with long working shifts and without access to external communications.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - In December 2001, shortly after the Wayback Machine became public, the search engine company Google unveiled Google Groups, another massive digital archive-this one under purely commercial auspices. Google Groups provides access to more than 650 million messages posted over the past two decades to Usenet, the online discussion forums that predate even the Internet. Although ownership seems like a dubious concept in relation to a public discussion forum, Google purchased the archive from Deja.com, which had brought the groups to the web but then collapsed in the Internet bust. Despite Deja.coms failure, Google sees the Usenet Archive as another attractive feature in its stable of online information resources and tools.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - Both IA and Google Groups are libraries organized on principles that are more familiar to computer scientists than to librarians, as Peter Lyman, who knows both worlds as the head of the University of California at Berkeley library and as a member of the IA board, points out. The library community has focused on developing sophisticated cataloging strategies. But computer scientists, including Kahle, have been more interested in developing sophisticated search engines that operate directly on the data we see (the web pages) rather than on the metadata (the cataloging information). Whereas archival and library projects focus on high-quality collections built around select themes and make the unit of cataloging the web page, the computer science paradigm allows for archiving the entire Web as it changes over time, then uses search engines to retrieve the necessary information.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - Projects designed by librarians and archivists generally have the advantages of precision and standardization. They favor careful protocols and standards such as the Dublin Core, the OAIS (Open Archival Information System), and the EAD (Encoded Archival Description). But the expense and difficulty of the protocols and procedures mean that less well funded and staffed archives and libraries often ignore them. Responding to presentations by advocates of standards at a conference, computer scientist Jim Miller warned that if archivists push for too much cataloging metadata they might end up with none.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - The Internet Archive, which is the child of the search engines and the computer scientists, is an extraordinarily valuable resource. Most historians will not be interested now, but in twenty-five or fifty years they will delight in searching it. A typical college history assignment in 2050 might be to compare web depictions of Muslim Americans in 1998 and 2008. But any appreciation of the IA must acknowledge its limitations. For example, large numbers of web pages do not exist as static HTML pages; rather, they are stored in databases, and the pages are generated on the fly by search queries. As a result, the IAs crawlers do not capture much of the so-called deep web that is stored in databases. Multimedia files-streaming media and flash-also do not seem to be captured. In addition, the Internet Archives crawls cannot go on forever; at some point, they stop, since, as one of the computer scientists who manages them acknowledges, the Web is essentially infinite in size. Anyone who browses the IA regularly encounters such messages as Not in Archive and File Location Error or even closed for maintenance.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - Some pages are missing for legal and economic as well as technical reasons. Private, gated sites are off-limits to the Internet Archives crawlers. And many ungated sites also discourage the crawlers. The New York Times allows free access to its current contents, but charges for articles more than one week old. If the IA gathered up and preserved the Timess content, there would be no reason for anyone to pay the Times for access to its proprietary archive. As a result, the Times includes a robots exclusion file on its site, which the IA respects. Even those sites without the robots exclusion file and without any formal copyright are still covered by copyright law and could challenge the IAs archiving of their content. To avoid trouble, the IA simply purges the pages of anyone who complains. It is as if Julie Nixon could write to the National Archives and tell them to delete her fathers tapes or an author could withdraw an early novel from circulation.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - Thus the Internet Archive is very far from the complete solution to the problem of digital preservation. It does not deal with the digital records that vex the National Archives and other repositories because they lack the public accessibility and minimal standardization in HTML of web pages. Nor does it include much formally published literature-e-books and journals-which is sold and hence gated from view. And even for what it has gathered, it has not yet hatched a long-term preservation plan, which would have to incorporate a strategy for continuing access to digital data that are in particular (and time-bound) formats. Even more troubling, it has no plan for how it will sustain itself into the future. Will Kahle continue to fund it indefinitely? What if Amazon and Alexa no longer find it worthwhile to gather the data, especially since acquisition costs are doubling every year?
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - Similar questions could be raised about Google Groups. What if the company decides that there is no prospect of gaining adequate advertising revenue by making old newsgroup messages available (as, indeed, Deja.com previously determined)? While appreciating Googles entrepreneurial energy in preserving and making available an enormous body of historical documents, we should also look carefully at the way private corporations have suddenly entered into a realm-archives-that was previously part of the public sector-a reflection of the privatization sweeping across the global economy. At least so far, our most important, and most imaginatively constructed, digital collections are in private hands.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - GIVEN THAT THE PRESERVATION OF cultural heritage and national history are arguably social goods, why shouldnt the government take the lead in such efforts? One reason is that at least some key aspects of the digital present the Bert story, for example-do not follow national boundaries and, indeed, erode them. If national archives were part of the projects of state-building and nationalism, then why should states support post-national digital archives? The declining significance of state-based national archives may mirror the decline of the contemporary national state. So far, the Smithsonian Institution and the Library of Congress have worked with the Internet Archive only where they needed its help in documenting some particularly national stories-the elections of 1996 and 2000 and the September 11th attacks.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - Another reason for the limited government role is that the digital preservation crisis emerged most dramatically during the anti-statist Reagan revolution of the 1980s. In the 1970s, for example, the electronic records program of the National Archives made a modest, promising start. But, as archivist Thomas E. Brown writes, it went into a near total collapse in the 1980s. The staff dropped to seven people by 1983, and, amazingly, this beleaguered group charged with guarding the nations electronic records had no access to computer facilities. Things began to improve in the early 1990s, but, after 1993, the electronic records program suffered from further cutbacks in the federal work force. An underfunded and understaffed National Archives was hardly in a position to develop a solution to the daunting and mounting problem of electronic federal records.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - The Library of Congress also initially eschewed a leading role in preserving digital materials, as the National Research Council later complained. Here, too, one could detect the weakening influence of the state. The librarys high-profile effort in the digital realm was American Memory, which digitized millions of items from its collections and placed them online. Teachers, students, and researchers love American Memory, but it did nothing to preserve the growing number of born digital objects. Not coincidentally, American Memory was a project that could attract large numbers of private and corporate donors, who often saw sponsorship as good advertising and who paid for three-quarters of the project.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - Better developed state-centered approaches to digital preservation have, not surprisingly, emerged outside the United States-in Australia and Scandinavia, for example. Norway requires that digital materials be legally deposited with the national library in return for copyright protection. One of the key ways that the Library of Congress could help preserve the future of digital materials would be to aggressively assert its copyright deposit claims, which would finesse some of the legal and ownership issues troubling the Internet Archive.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - Nevertheless, the National Archives and the Library of Congress have very recently begun-prodded by outside critics and supported belatedly by Congress-to take a more aggressive approach on digital preservation. The archives is proposing a Redesign of Federal Records Management to respond to the reality that a large majority of electronic record series of continuing value are not coming into archival custody. It is also working closely with the San Diego Supercomputing Center on developing persistent object preservation (POP), which creates a description of a digital object (and groups of digital objects) in simple tags and schemas that will be understandable in the future; the records would be self-describing and, hence, independent of specific hardware and software. The computer scientists maintain that records in this format will last for three hundred to four hundred years.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - In December 2000, the Library of Congress launched the most important initiative, the National Digital Information Infrastructure Program (NDIIP). Even this massive and important federal initiative bore the marks of the anti-statist, privatization politics of the 1980s. Congress gave the library $5 million for planning and promised another $20 million when it approved the plan. But the final $75 million will only be distributed as a match against an equal amount in private funds.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - Although the future of the digital present remains perilous, these recent initiatives suggest some encouraging strategies for preserving the range of digital materials. A combination of technical and organizational approaches promises the greatest chance of success, but privatization poses grave dangers for the future of the past. Advocates of digital preservation need to mobilize state funding and state power (such as the assertion of eminent domain over copyright materials) but infuse it with the experimental and ad hoc spirit of the Internet Archive. And we need to recognize that, for many digital materials (especially the web), the imperfect computer-science paradigm probably has more to recommend it than the more careful and systematic approach of the librarians and archivists. What is often said of military strategy seems to apply to digital preservation: the greatest enemy of a good plan is the dream of a perfect plan. We have never preserved everything; we need to start preserving something.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - GIVEN THE ENORMOUS BARRIERS to saving digital records and information, it comes as something of a surprise that many continue to insist that a perfect plan-or at least a pretty good plan-will eventually emerge. Techno-optimists such as Brewster Kahle dream most vividly of the perfect plan and its startling consequences. For the second time in history, Kahle writes with two collaborators, people are laying plans to collect all information-the first time involved the Greeks which culminated in the Library of Alexandria... Now....many [are] once again to take steps in building libraries that hold complete collections. Digital technology, they explain, has gotten to the point where scanning all books, digitizing all audio recordings, downloading all websites, and recording the output of all TV and radio stations is not only feasible but less costly than buying and storing the physical versions.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - Librarians and archivists remain skeptical of such predictions, pointing out the enormous costs of cataloging and making available what has been preserved, and that we have never saved more than a fraction of our cultural output. But, whatever our degree of skepticism, it is still worth thinking seriously about what a world in which everything was saved might look like.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - Most obviously, archives, libraries, and other record repositories would suddenly be freed from the tyranny of shelf space that has always shadowed their work. Digitization also removes other long-term scourges of historical memory such as fire and war. The 1921 fire that destroyed the 1890 census records provided a crucial spark that finally led to the creation of the National Archives. But what if there had been multiple copies of the census? The ease-almost inevitability-of the copying of digital files means that it is considerably less likely today that things exist in only a single copy.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - Kahles vision of cultural and historical abundance merges the traditional democratic vision of the public library with the resources of the research library and the national archive. Previously, few had the opportunity to come to Washington to watch early Thomas Edison films at the Library of Congress. And the library could not have served them if they had. Democratized access is the real payoff in electronic records and materials. It may be harder to preserve and organize digital materials than it is paper records, but, once that is accomplished, they can be made accessible to vastly greater numbers of people. To open up the archives and libraries in this way democratizes historical work. Already, people who had never had direct access to archives and libraries can now enter. High school students are suddenly doing primary source research; genealogy has exploded in popularity because you no longer have to travel to distant archives.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - This vision of democratic access also promises direct and unmediated access to the past. Electronic commerce enthusiasts tout disintermediation-which is the elimination of the insurance and real estate broker and other intermediaries-and the emergence of one like eBay made up of only buyers and sellers. In theory, the universal digital library might bring a similar cultural disintermediation in which people interested in history make direct contact with the documents and artifacts of the past without the mediation of cultural brokers like librarians, archivists, and historians. Sociologist Mike Featherstone speculates on the emergence of a new culture of memory in which the existing hierarchical controls over access would disappear. This direct access to cultural records and resources from those outside cultural institutions could lead to a decline in intellectual and academic power in which the historian, for example, no longer stands between people and their pasts. The Wayback Machine encapsulates this vision of disintermediation by suggesting that everyone, like Mr. Peabody and his boy Sherman, can jump in a time machine and find out what Columbus or Edison was really like. Of course, most historians would argue that, while digital collections may put the novice in the archive, he or she is not so likely to know what to do there. Still, the balance of power may shift. Ask any travel agent how the widespread access to information undercuts professional control.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - Most historians have not embraced this vision in which everyone becomes his or her own historian. Nor have they enthusiastically endorsed the vision of a universal library that contains all voices and all records. In my informal polling, most historians recoil at the thought that they would need to write history with even more sources. Historians are not particularly hostile to new technology, but they are not ready to welcome fundamental changes to their cultural position or their modes of work. Having lived our professional careers in a culture of scarcity, historians find that a world of abundance can be unsettling.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - Abundance, after all, can be overwhelming. How do we find the forest when there are so many damned trees? Psychologist Aleksandr Luria made this point in his famous study of a Russian journalist, S (S. V. Shereshevskii), who had an amazingly photographic memory; he could reproduce complex tables of numbers and long lists of words that had been shown to him years earlier. But this gift turned out to be a curse. He could not recognize people because he remembered their faces so precisely; a slightly different expression would register as a different person. Grasping the larger point of a passage or abstract idea became a tortuous ... struggle against images that kept rising to the surface in his mind. He lacked, as psychologist Jerome Bruner notes, the capacity to convert encounters with the particular into instances of the general.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - If historians are to set themselves against forgetting (in Milan Kunderas resonant phrase), then they may need to figure out new ways to sort their way through the potentially overwhelming digital record of the past. Contemporary historians are already groaning under the weight of their sources. Robert Caro has spent twenty-six years working his way through just the documents on Lyndon B. Johnsons pre-vice-presidential years including 2,082 boxes of Senate papers. Surely, the injunction of traditional historians to look at everything cannot survive in a digital era in which everything has survived.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - The historical narratives that future historians write may not actually look much different from those that are crafted today, but the methodologies they use may need to change radically. If we have, for example, a complete record of everything said in 2010, can we offer generalizations about the nature of discourse on a topic simply by reading around? Wouldnt we need to engage in some more methodical sampling in the manner of, say, sociology? Would this revive the social-scientific approaches with which historians flirted briefly in the 1970s? Wouldnt historians need to learn to write complex searches and algorithms that would allow them to sort through this overwhelming record in creative, but systematic, ways? The future gurus of historical research methodology may be the computer scientists at Google who have figured out how to search the equivalent of a 100-mile-high pile of paper in half a second. To be able to find things with high accuracy and high reliability has an incredible impact on the world-and, one might add, future historians. Future graduate programs will probably have to teach such social-scientific and quantitative methods as well as such other skills as digital archaeology (the ability to read arcane computer formats), digital diplomatics (the modern version of the old science of authenticating documents), and data mining (the ability to find the historical needle in the digital hay). In the coming years, contemporary historians may need more specialized research and language skills than medievalists do.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - HISTORIANS HAVE TIME TO THINK ABOUT changing their methods to meet the challenge of a cornucopia of historical sources. But they need to act more immediately on preserving the digital present or that reconsideration will be moot; they will be struggling with a scarcity, not an overabundance, of sources. Surprisingly, however, historians themselves have been scarce on this issue. Archivists and librarians have intensely debated and discussed digitization and digital presentation for more than a decade. They have written hundreds of articles and reports, undertaken research projects, and organized conferences and workshops. Academic and teaching historians have taken almost no part in these conferences and have contributed almost nothing to this burgeoning literature. Historical journals have published nothing on the topic.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - Part of the reason is that preserving the born-digital materials for future historians seems like a theoretical and technical issue, tomorrows problem or at least someone elses problem. Another reason for this disinterest is the divorce of archival concerns from the historical profession-a part of the general narrowing of the concerns of professional historians over the past century. In the late nineteenth and early twentieth centuries, historians and archivists were closely aligned. Perhaps the most important committee of the American Historical Association in the 1890s was the Historical Manuscripts Commission, which led to the AHAs influential Public Archives Commission. Archival concerns found a regular place in the AHAs Annual Meeting, the American Historical Review, and especially the voluminous AHA annual reports. Most important, the AHA led the fight to establish the National Archives. But in 1936 (in the midst of an earlier technological upheaval that came with the emergence of microfilm), the Conference of Archivists left the AHA to create the Society of American Archivists. The professions charged with writing about the past and preserving the records of the past have sharply diverged in the past seven decades. Today, only 82 of the 14,000 members of the AHA identify themselves as archivists.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - But historians ignore the future of digital data at their own peril. What, for example, about the long-term preservation of scholarship that is increasingly- originating in digital form? Not only do historians need to ensure the future of their own scholarship, but linking directly from footnotes to electronic texts-an exciting prospect for scholars-will only be possible if a stable archiving system emerges. For the foreseeable future, librarians and archivists will be making decisions about priorities in digital preservation. Historians should be at the table when those decisions are made. Do they wish to endorse, for example, the Pitt Projects emphasis on preserving records of business transactions rather than information more broadly?
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - One of the most vexing and interesting features of the digital era is the way that it unsettles traditional arrangements and forces us to ask basic questions that have been there all along. Some are about the relationship between historians and archival work. Should the work of collecting, organizing, editing, and preserving of primary sources receive the same kind of recognition and respect that it did in earlier days of the profession? Others are about whose overall responsibility it is to preserve the past. For example, should the National Archives expand its role in preservation beyond official records? For many years, historians have taken a hands-off approach to archival questions. With the unsettling of the status quo, they should move back more actively into this realm. If the web page is the unit of analysis for the digital librarian and the link the unit of analysis for the computer scientists, what is the appropriate unit of analysis for historians? What would a digital archival system designed by historians look like? And how might we alter and enhance our methodologies in a digital realm? For example, in a world where all sources were digitized and universally accessible, arguments could be more rigorously tested. Currently, many arguments lack such scrutiny because so few scholars have access to the original sources-a problem that has arisen especially sharply in the recent controversies over Michael A. Bellesiles Arming America: The Origins of a National Gun Culture (2000). In a new digital world, would historians then be held to the same standard of reproducible results as scientists?
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - Of course, when historians get to the preservation table, they will discover a cultural and professional clash between their own impulses, which are to save everything, and those of librarians and archivists who believe that selection, whether passive or active, is inevitable. The National Archives, for example, only permanently accessions 2 percent of government records. This conflict surfaced in the 1980s and 1990s, when librarians tried to bring in scholars to discuss priorities in preserving books that were deteriorating because of acidic paper. Librarians found the discussion frustrating. Many scholars, recalls Deanna Marcum, declared that everything had to be saved and they could not make choices. Not surprisingly, scholars have responded very differently to Nicholson Bakers sharp attack on the microfilming and disposal of aging books and newspapers in Double Fold than have archivists and librarians. Whereas many scholars have shared Bakers outrage that books and newspapers have been destroyed, archivists and librarians have responded in outrage to what they see as his failure to understand the pressures that make it impossible to save everything. Whereas historians with their gaze fixed on the past worry about information scarcity (the missing letter or diary), archivists and librarians recognize that we now live in a world of overwhelming information abundance. If historians are going to join in preservation discussions, they will have to make themselves better informed about the simultaneous abundance of historical sources and scarcity of financial resources that lead archivists and librarians to respond with exasperation to scholars blithe insistence that everything must be saved.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - Preservation of the past is, in the end, often a matter of allocating adequate resources. Perhaps the largest problem facing the preservation of electronic government records has nothing to do with technology; it is, as various reports have noted, the low priority traditionally given to federal records management. In the absence of new resources, the costs of preservation will come from the money that our society, in the aggregate, allocates for history and culture. Richard Cox, for example, has argued that a greater portion of the budget of the National Historical Publications and Records Commission (NHPRC) should go to electronic records preservation and management and correspondingly less money should go to the letterpress Documentary Editions that the commission also funds, since most of the records represented by the documentary editions are not immediately threatened. This stance does not endear him to documentary editors, who are much better represented among professional historians than are archivists.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - How, at this moment in the twenty-first century, is AI conceptualized and constructed? What is at stake in the turn to artificial intelligence, and what kinds of politics are contained in the way these systems map and interpret the world? What are the social and material consequences of including AI and related algorithmic systems into the decision-making systems of social institutions like education and health care, finance, government operations, workplace interactions and hiring, communication systems, and the justice system? This book is not a story about code and algorithms or the latest thinking in computer vision or natural language processing or reinforcement learning. Many other books do that. Neither is it an ethnographic account of a single community and the effects of AI on their experience of work or housing or medicine — although we certainly need more of those.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - The alternative to squabbling over inadequate resources that are appropriated for these purposes is joint action to secure further funds. When Shirley Baker, president of the Association of Research Libraries, challenged historian Robert Darntons favorable review of Bakers book and noted choices have always had to be made in the absence of greater public commitment to the preservation of the historical record, Darnton responded by urging the establishment of a new kind of national library dedicated to the preservation of cultural artifacts (including disappearing digital records) and funded by income generated by the sale or rental of bandwidth. Such state-based solutions return us to the kind of alliance between historians and archivists that led to building of the National Archives in the 1930s, an era of growing rather than waning confidence in the nation-state. Historians need to join in lobbying actively for adequate funding for both current historical work and preservation of future resources. They should also argue forcefully for the democratized access to the historical record that digital media make possible. And they must add their voices to those calling for expanding copyright deposit-and opposing copyright extension, for that matter of digital materials so as to remove some of the legal clouds hanging over efforts like the Internet Archive and to halt the ongoing privatization of historical resources. Even in the absence of state action, historians should take steps individually and within their professional organizations to embrace the culture of abundance made possible by digital media and expand the public space of scholarship-for example, making their own work available for free on the web, cross-referencing other digital scholarship, and perhaps depositing their sources online for other scholars to use. A vigorous public domain today is a prerequisite for a healthy historical record.
[Author: Roy Rosenzweig; From essay:"Scarcity or Abundance Preserving the Past in a Digital Era "] - More than a century ago, Justin Winsor, the third president of the AHA, concluded his Presidential Address-focused on a topic that would be considered odd today, that of preserving manuscript sources for the study of history-with a plea to the AHA to convince the National Legislature to support a scheme before it is too late to preserve and make known what there is still left to us of the historical manuscripts of the country. For founders of the historical profession such as Winsor, the need to engage with history broadly defined-not just how it was researched but also how it was taught in the schools or preserved in archives-came naturally; it was part of creating a historical profession. In the early twenty-first century, we are likely to be faced with recreating the historical profession, and we will be well served by such a broad vision of our mission. If the past is to have an abundant future, if the story of Bert Is Evil and hundreds of other stories are to be fully told, then historians need to act in the present.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - At the end of the nineteenth century, Europe was captivated by a horse called Hans. “Clever Hans” was nothing less than a marvel: he could solve math problems, tell time, identify days on a calendar, differentiate musical tones, and spell out words and sentences. People flocked to watch the German stallion tap out answers to complex problems with his hoof and consistently arrive at the right answer. “What is two plus three?” Hans would diligently tap his hoof on the ground five times. “What day of the week is it?” The horse would then tap his hoof to indicate each letter on a purpose-built letter board and spell out the correct answer. Hans even mastered more complex questions, such as, “I have a number in mind. I subtract nine and have three as a remainder. What is the number?” By 1904, Clever Hans was an international celebrity, with the New York Times championing him as “Berlin’s Wonderful Horse; He Can Do Almost Everything but Talk.”
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - Hans’s trainer, a retired math teacher named Wilhelm von Osten, had long been fascinated by animal intelligence. Von Osten had tried and failed to teach kittens and bear cubs cardinal numbers, but it wasn’t until he started working with his own horse that he had success. He first taught Hans to count by holding the animal’s leg, showing him a number, and then tapping on the hoof the correct number of times. Soon Hans responded by accurately tapping out simple sums. Next von Osten introduced a chalkboard with the alphabet spelled out, so Hans could tap a number for each letter on the board. After two years of training, von Osten was astounded by the animal’s strong grasp of advanced intellectual concepts. So he took Hans on the road as proof that animals could reason. Hans became the viral sensation of the belle époque.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - But many people were skeptical, and the German board of education launched an investigative commission to test Von Osten’s scientific claims. The Hans Commission was led by the psychologist and philosopher Carl Stumpf and his assistant Oskar Pfungst, and it included a circus manager, a retired schoolteacher, a zoologist, a veterinarian, and a cavalry officer. Yet after extensive questioning of Hans, both with his trainer present and without, the horse maintained his record of correct answers, and the commission could find no evidence of deception. As Pfungst later wrote, Hans performed in front of “thousands of spectators, horse-fanciers, trick-trainers of first rank, and not one of them during the course of many months’ observations are able to discover any kind of regular signal” between the questioner and the horse.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - The commission found that the methods Hans had been taught were more like “teaching children in elementary schools” than animal training and were “worthy of scientific examination.” But Strumpf and Pfungst still had doubts. One finding in particular troubled them: when the questioner did not know the answer or was standing far away, Hans rarely gave the correct answer. This led Pfungst and Strumpf to consider whether some sort of unintentional signal had been providing Hans with the answers.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - As Pfungst would describe in his 1911 book, their intuition was right: the questioner’s posture, breathing, and facial expression would subtly change around the moment Hans reached the right answer, prompting Hans to stop there. Pfungst later tested this hypothesis on human subjects and confirmed his result. What fascinated him most about this discovery was that questioners were generally unaware that they were providing pointers to the horse. The solution to the Clever Hans riddle, Pfungst wrote, was the unconscious direction from the horse’s questioners. The horse was trained to produce the results his owner wanted to see, but audiences felt that this was not the extraordinary intelligence they had imagined.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - Instead, this is an expanded view of artificial intelligence as an extractive industry. The creation of contemporary AI systems depends on exploiting energy and mineral resources from the planet, cheap labor, and data at scale. To observe this in action, we will go on a series of journeys to places that reveal the makings of AI.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - The story of Clever Hans is compelling from many angles: the relationship between desire, illusion, and action, the business of spectacles, how we anthropomorphize the nonhuman, how biases emerge, and the politics of intelligence. Hans inspired a term in psychology for a particular type of conceptual trap, the Clever Hans Effect or observer-expectancy effect, to describe the influence of experimenters’ unintentional cues on their subjects. The relationship between Hans and von Osten points to the complex mechanisms by which biases find their ways into systems and how people become entangled with the phenomena they study. The story of Hans is now used in machine learning as a cautionary reminder that you can’t always be sure of what a model has learned from the data it has been given. Even a system that appears to perform spectacularly in training can make terrible predictions when presented with novel data in the world.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - This opens a central question of this book: How is intelligence “made,” and what traps can that create? At first glance, the story of Clever Hans is a story of how one man constructed intelligence by training a horse to follow cues and emulate humanlike cognition. But at another level, we see that the practice of making intelligence was considerably broader. The endeavor required validation from multiple institutions, including academia, schools, science, the public, and the military. Then there was the market for von Osten and his remarkable horse—emotional and economic investments that drove the tours, the newspaper stories, and the lectures. Bureaucratic authorities were assembled to measure and test the horse’s abilities. A constellation of financial, cultural, and scientific interests had a part to play in the construction of Hans’s intelligence and a stake in whether it was truly remarkable.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - We can see two distinct mythologies at work. The first myth is that nonhuman systems (be it computers or horses) are analogues for human minds. This perspective assumes that with sufficient training, or enough resources, humanlike intelligence can be created from scratch, without addressing the fundamental ways in which humans are embodied, relational, and set within wider ecologies. The second myth is that intelligence is something that exists independently, as though it were natural and distinct from social, cultural, historical, and political forces. In fact, the concept of intelligence has done inordinate harm over centuries and has been used to justify relations of domination from slavery to eugenics.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - These mythologies are particularly strong in the field of artificial intelligence, where the belief that human intelligence can be formalized and reproduced by machines has been axiomatic since the mid-twentieth century. Just as Hans’s intelligence was considered to be like that of a human, fostered carefully like a child in elementary school, so AI systems have repeatedly been described as simple but humanlike forms of intelligence. In 1950, Alan Turing predicted that “at the end of the century the use of words and general educated opinion will have altered so much that one will be able to speak of machines thinking without expecting to be contradicted.” The mathematician John von Neumann claimed in 1958 that the human nervous system is “prima facie digital.” MIT professor Marvin Minsky once responded to the question of whether machines could think by saying, “Of course machines can think; we can think and we are ‘meat machines.’” But not everyone was convinced. Joseph Weizenbaum, early AI inventor and creator of the first chatbot program, known as ELIZA, believed that the idea of humans as mere information processing systems is far too simplistic a notion of intelligence and that it drove the “perverse grand fantasy” that AI scientists could create a machine that learns “as a child does.”
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - This has been one of the core disputes in the history of artificial intelligence. In 1961, MIT hosted a landmark lecture series titled “Management and the Computer of the Future.” A stellar lineup of computer scientists participated, including Grace Hopper, J. C. R. Licklider, Marvin Minsky, Allen Newell, Herbert Simon, and Norbert Wiener, to discuss the rapid advances being made in digital computing. At its conclusion, John McCarthy boldly argued that the differences between human and machine tasks were illusory. There were simply some complicated human tasks that would take more time to be formalized and solved by machines.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - But philosophy professor Hubert Dreyfus argued back, concerned that the assembled engineers “do not even consider the possibility that the brain might process information in an entirely different way than a computer.” In his later work What Computers Can’t Do, Dreyfus pointed out that human intelligence and expertise rely heavily on many unconscious and subconscious processes, while computers require all processes and data to be explicit and formalized. As a result, less formal aspects of intelligence must be abstracted, eliminated, or approximated for computers, leaving them unable to process information about situations as humans do.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - Much in AI has changed since the 1960s, including a shift from symbolic systems to the more recent wave of hype about machine learning techniques. In many ways, the early fights over what AI can do have been forgotten and the skepticism has melted away. Since the mid-2000s, AI has rapidly expanded as a field in academia and as an industry. Now a small number of powerful technology corporations deploy AI systems at a planetary scale, and their systems are once again hailed as comparable or even superior to human intelligence.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - Yet the story of Clever Hans also reminds us how narrowly we consider or recognize intelligence. Hans was taught to mimic tasks within a very constrained range: add, subtract, and spell words. This reflects a limited perspective of what horses or humans can do. Hans was already performing remarkable feats of interspecies communication, public performance, and considerable patience, yet these were not recognized as intelligence. As author and engineer Ellen Ullman puts it, this belief that the mind is like a computer, and vice versa, has “infected decades of thinking in the computer and cognitive sciences,” creating a kind of original sin for the field. It is the ideology of Cartesian dualism in artificial intelligence: where AI is narrowly understood as disembodied intelligence, removed from any relation to the material world.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - Let’s ask the deceptively simple question, What is artificial intelligence? If you ask someone in the street, they might mention Apple’s Siri, Amazon’s cloud service, Tesla’s cars, or Google’s search algorithm. If you ask experts in deep learning, they might give you a technical response about how neural nets are organized into dozens of layers that receive labeled data, are assigned weights and thresholds, and can classify data in ways that cannot yet be fully explained. In 1978, when discussing expert systems, Professor Donald Michie described AI as knowledge refining, where “a reliability and competence of codification can be produced which far surpasses the highest level that the unaided human expert has ever, perhaps even could ever, attain.” In one of the most popular textbooks on the subject, Stuart Russell and Peter Norvig state that AI is the attempt to understand and build intelligent entities. “Intelligence is concerned mainly with rational action,” they claim. “Ideally, an intelligent agent takes the best possible action in a situation.”
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - Each way of defining artificial intelligence is doing work, setting a frame for how it will be understood, measured, valued, and governed. If AI is defined by consumer brands for corporate infrastructure, then marketing and advertising have predetermined the horizon. If AI systems are seen as more reliable or rational than any human expert, able to take the “best possible action,” then it suggests that they should be trusted to make high-stakes decisions in health, education, and criminal justice. When specific algorithmic techniques are the sole focus, it suggests that only continual technical progress matters, with no consideration of the computational cost of those approaches and their far-reaching impacts on a planet under strain.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - In contrast, in this book I argue that AI is neither artificial nor intelligent. Rather, artificial intelligence is both embodied and material, made from natural resources, fuel, human labor, infrastructures, logistics, histories, and classifications. Al systems are not autonomous, rational, or able to discern anything without extensive, computationally intensive training with large datasets or predefined rules and rewards. In fact, artificial intelligence as we know it depends entirely on a much wider set of political and social structures. And due to the capital required to build AI at scale and the ways of seeing that it optimizes AI systems are ultimately designed to serve existing dominant interests. In this sense, artificial intelligence is a registry of power.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - In this book we’ll explore how artificial intelligence is made, in the widest sense, and the economic, political, cultural, and historical forces that shape it. Once we connect AI within these broader structures and social systems, we can escape the notion that artificial intelligence is a purely technical domain. At a fundamental level, AI is technical and social practices, institutions and infrastructures, politics and culture. Computational reason and embodied work are deeply interlinked: AI systems both reflect and produce social relations and understandings of the world.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - It’s worth noting that the term “artificial intelligence” can create discomfort in the computer science community. The phrase has moved in and out of fashion over the decades and is used more in marketing than by researchers. “Machine learning” is more commonly used in the technical literature. Yet the nomenclature of AI is often embraced during funding application season, when venture capitalists come bearing checkbooks, or when researchers are seeking press attention for a new scientific result. As a result, the term is both used and rejected in ways that keep its meaning in flux. For my purposes, I use AI to talk about the massive industrial formation that includes politics, labor, culture, and capital. When I refer to machine learning, I’m speaking of a range of technical approaches (which are, in fact, social and infrastructural as well, although rarely spoken about as such).
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - But there are significant reasons why the field has been focused so much on the technical—algorithmic breakthroughs, incremental product improvements, and greater convenience. The structures of power at the intersection of technology, capital, and governance are well served by this narrow, abstracted analysis. To understand how AI is fundamentally political, we need to go beyond neural nets and statistical pattern recognition to instead ask what is being optimized, and for whom, and who gets to decide. Then we can trace the implications of those choices.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - How can an atlas help us to understand how artificial intelligence is made? An atlas is an unusual type of book. It is a collection of disparate parts, with maps that vary in resolution from a satellite view of the planet to a zoomed-in detail of an archipelago. When you open an atlas, you may be seeking specific information about a particular place—or perhaps you are wandering, following your curiosity, and finding unexpected pathways and new perspectives. As historian of science Lorraine Daston observes, all scientific atlases seek to school the eye, to focus the observer’s attention on particular telling details and significant characteristics. An atlas presents you with a particular viewpoint of the world, with the imprimatur of science—scales and ratios, latitudes and longitudes—and a sense of form and consistency.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - Yet an atlas is as much an act of creativity — a subjective, political, and aesthetic intervention — as it is a scientific collection. The French philosopher Georges Didi-Huberman thinks of the atlas as something that inhabits the aesthetic paradigm of the visual and the epistemic paradigm of knowledge. By implicating both, it undermines the idea that science and art are ever completely separate. Instead, an atlas offers us the possibility of rereading the world, linking disparate pieces differently and “reediting and piecing it together again without thinking we are summarizing or exhausting it.”
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - Perhaps my favorite account of how a cartographic approach can be helpful comes from the physicist and technology critic Ursula Franklin: “Maps represent purposeful endeavors: they are meant to be useful, to assist the traveler and bridge the gap between the known and the as yet unknown; they are testaments of collective knowledge and insight.”
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - Maps, at their best, offer us a compendium of open pathways—shared ways of knowing—that can be mixed and combined to make new interconnections. But there are also maps of domination, those national maps where territory is carved along the fault lines of power: from the direct interventions of drawing borders across contested spaces to revealing the colonial paths of empires. By invoking an atlas, I’m suggesting that we need new ways to understand the empires of artificial intelligence. We need a theory of AI that accounts for the states and corporations that drive and dominate it, the extractive mining that leaves an imprint on the planet, the mass capture of data, and the profoundly unequal and increasingly exploitative labor practices that sustain it. These are the shifting tectonics of power in AI. A topographical approach offers different perspectives and scales, beyond the abstract promises of artificial intelligence or the latest machine learning models. The aim is to understand AI in a wider context by walking through the many different landscapes of computation and seeing how they connect.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - There’s another way in which atlases are relevant here. The field of AI is explicitly attempting to capture the planet in a computationally legible form. This is not a metaphor so much as the industry’s direct ambition. The Al industry is making and normalizing its own proprietary maps, as a centralized God’s-eye view of human movement, communication, and labor. Some AI scientists have stated their desire to capture the world and to supersede other forms of knowing. AI professor Fei-Fei Li describes her ImageNet project as aiming to “map out the entire world of objects.” In their textbook, Russell and Norvig describe artificial intelligence as “relevant to any intellectual task; it is truly a universal field.” One of the founders of artificial intelligence and early experimenter in facial recognition, Woody Bledsoe, put it most bluntly: “in the long run, AI is the only science.” This is a desire not to create an atlas of the world but to be the atlas—the dominant way of seeing. This colonizing impulse centralizes power in the AI field: it determines how the world is measured and defined while simultaneously denying that this is an inherently political activity.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - Instead of claiming universality, this book is a partial account, and by bringing you along on my investigations, I hope to show you how my views were formed. We will encounter well-visited and lesser-known landscapes of computation: the pits of mines, the long corridors of energy-devouring data centers, skull archives, image databases, and the fluorescent-lit hangars of delivery warehouses. These sites are included not just to illustrate the material construction of AI and its ideologies but also to “illuminate the unavoidably subjective and political aspects of mapping, and to provide alternatives to hegemonic, authoritative — and often naturalized and reified — approaches,” as media scholar Shannon Mattern writes.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - Models for understanding and holding systems accountable have long rested on ideals of transparency. As I’ve written with the media scholar Mike Ananny, being able to see a system is sometimes equated with being able to know how it works and how to govern it. But this tendency has serious limitations. In the case of AI, there is no singular black box to open, no secret to expose, but a multitude of interlaced systems of power. Complete transparency, then, is an impossible goal. Rather, we gain a better understanding of Al’s role in the world by engaging with its material architectures, contextual environments, and prevailing politics and by tracing how they are connected.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - My thinking in this book has been informed by the disciplines of science and technology studies, law, and political philosophy and from my experience working in both academia and an industrial AI research lab for almost a decade. Over those years, many generous colleagues and communities have changed the way I see the world: mapping is always a collective exercise, and this is no exception. I’m grateful to the scholars who created new ways to understand sociotechnical systems, including Geoffrey Bowker, Benjamin Bratton, Wendy Chun, Lorraine Daston, Peter Galison, Ian Hacking, Stuart Hall, Donald MacKenzie, Achille Mbembé, Alondra Nelson, Susan Leigh Star, and Lucy Suchman, among many others. This book benefited from many in-person conversations and reading the recent work by authors studying the politics of technology, including Mark Andrejevic, Ruha Benjamin, Meredith Broussard, Simone Browne, Julie Cohen, Sasha Costanza-Chock, Virginia Eubanks, Tarleton Gillespie, Mar Hicks, Tung-Hui Hu, Yuk Hui, Safiya Umoja Noble, and Astra Taylor.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - As with any book, this one emerges from a specific lived experience that imposes limitations. As someone who has lived and worked in the United States for the past decade, my focus skews toward the AI industry in Western centers of power. But my aim is not to create a complete global atlas — the very idea invokes capture and colonial control. Instead, any author’s view can be only partial, based on local observations and interpretations, in what environmental geographer Samantha Saville calls a “humble geography” that acknowledges one’s specific perspectives rather than claiming objectivity or mastery.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - Just as there are many ways to make an atlas, so there are many possible futures for how AI will be used in the world. The expanding reach of AI systems may seem inevitable, but this is contestable and incomplete. The underlying visions of the AI field do not come into being autonomously but instead have been constructed from a particular set of beliefs and perspectives. The chief designers of the contemporary atlas of AI are a small and homogenous group of people, based in a handful of cities, working in an industry that is currently the wealthiest in the world. Like medieval European mappae mundi, which illustrated religious and classical concepts as much as coordinates, the maps made by the Al industry are political interventions, as opposed to neutral reflections of the world. This book is made against the spirit of colonial mapping logics, and it embraces different stories, locations, and knowledge bases to better understand the role of AI in the world.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - Artificial intelligence, then, is an idea, an infrastructure, an industry, a form of exercising power, and a way of seeing; it’s also a manifestation of highly organized capital backed by vast systems of extraction and logistics, with supply chains that wrap around the entire planet. All these things are part of what artificial intelligence is — a two-word phrase onto which is mapped a complex set of expectations, ideologies, desires, and fears.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - In chapter 1, we begin in the lithium mines of Nevada, one of the many sites of mineral extraction needed to power contemporary computation. Mining is where we see the extractive politics of AI at their most literal. The tech sector’s demand for rare earth minerals, oil, and coal is vast, but the true costs of this extraction is never borne by the industry itself. On the software side, building models for natural language processing and computer vision is enormously energy hungry, and the competition to produce faster and more efficient models has driven computationally greedy methods that expand Al’s carbon footprint. From the last trees in Malaysia that were harvested to produce latex for the first transatlantic undersea cables to the giant artificial lake of toxic residues in Inner Mongolia, we trace the environmental and human birthplaces of planetary computation networks and see how they continue to terraform the planet.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - Chapter 2 shows how artificial intelligence is made of human labor. We look at the digital pieceworkers paid pennies on the dollar clicking on microtasks so that data systems can seem more intelligent than they are. Our journey will take us inside the Amazon warehouses where employees must keep in time with the algorithmic cadences of a vast logistical empire, and we will visit the Chicago meat laborers on the disassembly lines where animal carcasses are vivisected and prepared for consumption. And we’ll hear from the workers who are protesting against the way that AI systems are increasing surveillance and control for their bosses.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - Labor is also a story about time. Coordinating the actions of humans with the repetitive motions of robots and line machinery has always involved a controlling of bodies in space and time. From the invention of the stopwatch to Google’s TrueTime, the process of time coordination is at the heart of workplace management. AI technologies both require and create the conditions for ever more granular and precise mechanisms of temporal management. Coordinating time demands increasingly detailed information about what people are doing and how and when they do it.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - Chapter 3 focuses on the role of data. All publicly accessible digital material—including data that is personal or potentially damaging—is open to being harvested for training datasets that are used to produce AI models. There are gigantic datasets full of people’s selfies, of hand gestures, of people driving cars, of babies crying, of newsgroup conversations from the 1990s, all to improve algorithms that perform such functions as facial recognition, language prediction, and object detection. When these collections of data are no longer seen as people’s personal material but merely as infrastructure, the specific meaning or context of an image or a video is assumed to be irrelevant. Beyond the serious issues of privacy and ongoing surveillance capitalism, the current practices of working with data in AI raise profound ethical, methodological, and epistemological concerns.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - And how is all this data used? In chapter 4, we look at the practices of classification in artificial intelligence systems, what sociologist Karin Knorr Cetina calls the “epistemic machinery.” We see how contemporary systems use labels to predict human identity, commonly using binary gender, essentialized racial categories, and problematic assessments of character and credit worthiness. A sign will stand in for a system, a proxy will stand for the real, and a toy model will be asked to substitute for the infinite complexity of human subjectivity. By looking at how classifications are made, we see how technical schemas enforce hierarchies and magnify inequity. Machine learning presents us with a regime of normative reasoning that, when in the ascendant, takes shape as a powerful governing rationality.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - From here, we travel to the hill towns of Papua New Guinea to explore the history of affect recognition, the idea that facial expressions hold the key to revealing a person’s inner emotional state. Chapter 5 considers the claim of the psychologist Paul Ekman that there are a small set of universal emotional states which can be read directly from the face. Tech companies are now deploying this idea in affect recognition systems, as part of an industry predicted to be worth more than seventeen billion dollars. But there is considerable scientific controversy around emotion detection, which is at best incomplete and at worst misleading. Despite the unstable premise, these tools are being rapidly implemented into hiring, education, and policing systems.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - In chapter 6 we look at the ways in which AI systems are used as a tool of state power. The military past and present of artificial intelligence have shaped the practices of surveillance, data extraction, and risk assessment we see today. The deep interconnections between the tech sector and the military are now being reined in to fit a strong nationalist agenda. Meanwhile, extralegal tools used by the intelligence community have now dispersed, moving from the military world into the commercial technology sector, to be used in classrooms, police stations, workplaces, and unemployment offices. The military logics that have shaped AI systems are now part of the workings of municipal government, and they are further skewing the relation between states and subjects.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - The concluding chapter assesses how artificial intelligence functions as a structure of power that combines infrastructure, capital, and labor. From the Uber driver being nudged to the undocumented immigrant being tracked to the public housing tenants contending with facial recognition systems in their homes, AI systems are built with the logics of capital, policing, and militarization—and this combination further widens the existing asymmetries of power. These ways of seeing depend on the twin moves of abstraction and extraction: abstracting away the material conditions of their making while extracting more information and resources from those least able to resist.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - But these logics can be challenged, just as systems that perpetuate oppression can be rejected. As conditions on Earth change, calls for data protection, labor rights, climate justice, and racial equity should be heard together. When these interconnected movements for justice inform how we understand artificial intelligence, different conceptions of planetary politics become possible.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - It’s important for us to contend with these many aspects of artificial intelligence—its malleability, its messiness, and its spatial and temporal reach. The promiscuity of AI as a term, its openness to being reconfigured, also means that it can be put to use in a range of ways: it can refer to everything from consumer devices like the Amazon Echo to nameless backend processing systems, from narrow technical papers to the biggest industrial companies in the world. But this has its usefulness, too. The breadth of the term “artificial intelligence” gives us license to consider all these elements and how they are deeply imbricated: from the politics of intelligence to the mass harvesting of data; from the industrial concentration of the tech sector to geopolitical military power; from the deracinated environment to ongoing forms of discrimination.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - The task is to remain sensitive to the terrain and to watch the shifting and plastic meanings of the term “artificial intelligence”—like a container into which various things are placed and then removed—because that, too, is part of the story.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - Simply put, artificial intelligence is now a player in the shaping of knowledge, communication, and power. These reconfigurations are occurring at the level of epistemology, principles of justice, social organization, political expression, culture, understandings of human bodies, subjectivities, and identities: what we are and what we can be. But we can go further. Artificial intelligence, in the process of remapping and intervening in the world, is politics by other means—although rarely acknowledged as such. These politics are driven by the Great Houses of AI, which consist of the half-dozen or so companies that dominate large-scale planetary computation.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - Many social institutions are now influenced by these tools and methods, which shape what they value and how decisions are made while creating a complex series of downstream effects. The intensification of technocratic power has been under way for a long time, but the process has now accelerated. In part this is due to the concentration of industrial capital at a time of economic austerity and outsourcing, including the defunding of social welfare systems and institutions that once acted as a check on market power. This is why we must contend with AI as a political, economic, cultural, and scientific force. As Alondra Nelson, Thuy Linh Tu, and Alicia Headlam Hines observe, “Contests around technology are always linked to larger struggles for economic mobility, political maneuvering, and community building.”
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - We are at a critical juncture, one that requires us to ask hard questions about the way AI is produced and adopted. We need to ask: What is AI? What forms of politics does it propagate? Whose interests does it serve, and who bears the greatest risk of harm? And where should the use of AI be constrained? These questions will not have easy answers. But neither is this an irresolvable situation or a point of no return—dystopian forms of thinking can paralyze us from taking action and prevent urgently needed interventions. As Ursula Franklin writes, “The viability of technology, like democracy, depends in the end on the practice of justice and on the enforcement of limits to power.”
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - This book argues that addressing the foundational problems of AI and planetary computation requires connecting issues of power and justice: from epistemology to labor rights, resource extraction to data protections, racial inequity to climate change. To do that, we need to expand our understanding of what is under way in the empires of AI, to see what is at stake, and to make better collective decisions about what should come next.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - The Boeing 757 banks right over San Jose on its final approach to San Francisco International Airport. The left wing drops as the plane lines up with the runway, revealing an aerial view of the tech sector’s most iconic location. Below are the great empires of Silicon Valley. The gigantic black circle of Apple’s headquarters is laid out like an uncapped camera lens, glistening in the sun. Then there’s Google’s head office, nestled close to NASA’s Moffett Federal Airfield. This was once a key site for the U.S. Navy during World War II and the Korean War, but now Google has a sixty-year lease on it, and senior executives park their private planes here. Arrayed near Google are the large manufacturing sheds of Lockheed Martin, where the aerospace and weapons manufacturing company builds hundreds of orbital satellites destined to look down on the activities of Earth. Next, by the Dumbarton Bridge, appears a collection of squat buildings that are home to Facebook, ringed with massive parking lots close to the sulfuric salt ponds of the Ravenswood Slough. From this vantage point, the nondescript suburban cul-de-sacs and industrial midrise skyline of Palo Alto betray little of its true wealth, power, and influence. There are only a few hints of its centrality in the global economy and in the computational infrastructure of the planet.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - I’m here to learn about artificial intelligence and what it is made from. To see that, I will need to leave Silicon Valley altogether.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - From the airport, I jump into a van and drive east. I cross the San Mateo–Hayward Bridge and pass by the Lawrence Livermore National Laboratory, where Edward Teller directed his research into thermonuclear weapons in the years after World War II. Soon the Sierra Nevada foothills rise beyond the Central Valley towns of Stockton and Manteca. Here the roads start winding up through the tall granite cliffs of the Sonora Pass and down the eastern side of the mountains toward grassy valleys dotted with golden poppies. Pine forests give way to the alkaline waters of Mono Lake and the parched desert landforms of the Basin and Range. To refuel, I pull into Hawthorne, Nevada, site of the world’s biggest ammunition depot, where the U.S. Army stores armaments in dozens of dirt-covered ziggurats that populate the valley in neat rows. Driving along Nevada State Route 265 I see a lone VORTAC in the distance, a large bowling pin–shaped radio tower that was designed for the era before GPS. It has a single function: it broadcasts “I am here” to all passing aircraft, a fixed point of reference in a lonely terrain.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - My destination is the unincorporated community of Silver Peak in Nevada’s Clayton Valley, where about 125 people live, depending on how you count. The mining town, one of the oldest in Nevada, was almost abandoned in 1917 after the ground was stripped bare of silver and gold. A few gold rush buildings still stand, eroding under the desert sun. The town may be small, with more junked cars than people, but it harbors something exceedingly rare. Silver Peak is perched on the edge of a massive underground lake of lithium. The valuable lithium brine under the surface is pumped out of the ground and left in open, iridescent green ponds to evaporate. From miles away, the ponds can be seen when they catch the light and shimmer. Up close, it’s a different view. Alien-looking black pipes erupt from the ground and snake along the salt-encrusted earth, moving in and out of shallow trenches, ferrying the salty cocktail to its drying pans.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - Here, in a remote pocket of Nevada, is a place where the stuff of AI is made.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - Clayton Valley is connected to Silicon Valley in much the way that the nineteenth-century goldfields were to early San Francisco. The history of mining, like the devastation it leaves in its wake, is commonly overlooked in the strategic amnesia that accompanies stories of technological progress. As historical geographer Gray Brechin points out, San Francisco was built from the gains of pulling gold and silver out of the lands of California and Nevada in the 1800s. The city is made from mining. Those same lands had been taken from Mexico under the Treaty of Guadalupe Hidalgo in 1848 at the end of the Mexican-American War, when it was already clear to the settlers that these would be highly valuable goldfields. It was a textbook example, Brechin observes, of the old adage that “commerce follows the flag, but the flag follows the pick.” Thousands of people were forced from their homes during this substantial territorial expansion of the United States. After America’s imperial invasion, the miners moved in. The land was stripped until the waterways were contaminated and the surrounding forests destroyed.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - Since antiquity, the business of mining has only been profitable because it does not have to account for its true costs: including environmental damage, the illness and death of miners, and the loss to the communities it displaces. In 1555, Georgius Agricola, known as the father of mineralogy, observed that “it is clear to all that there is greater detriment from mining than the value of the metals which the mining produces.” In other words, those who profit from mining do so only because the costs must be sustained by others, those living and those not yet born. It is easy to put a price on precious metals, but what is the exact value of a wilderness, a clean stream, breathable air, the health of local communities? It was never estimated, and thus an easy calculus emerged: extract everything as rapidly as possible. It was the “move fast and break things” of a different time. The result was that the Central Valley was decimated, and as one tourist observed in 1869, “Tornado, flood, earthquake and volcano combined could hardly make greater havoc, spread wider ruin and wreck than [the] gold-washing operations. . . . There are no rights which mining respects in California. It is the one supreme interest.”
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - As San Francisco drew enormous wealth from the mines, it was easy for its populace to forget where it all came from. The mines were located far from the city they enriched, and this remoteness allowed city dwellers to remain ignorant of what was happening to the mountains, rivers, and laborers that fed their fortunes. But small reminders of the mines are all around. The city’s new buildings used the same technology that came from deep within the Central Valley for transport and life support. The pulley systems that carried miners down into the mine shafts were adapted and turned upside down to transport people in elevators to the top of the city’s high-rises. Brechin suggests that we should think of the skyscrapers of San Francisco as inverted minescapes. The ores extracted from holes in the ground were sold to create the stories in the air; the deeper the extractions went, the higher the great towers of office work stretched into the sky.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - San Francisco is enriched once more. Once it was gold ore that underwrote fortunes; now it is the extraction of substances like white lithium crystal. It’s known in mineral markets as “gray gold.” The technology industry has become a new supreme interest, and the five biggest companies in the world by market capitalization have offices in this city: Apple, Microsoft, Amazon, Facebook, and Google. Walking past the start-up warehouses in the SoMa district where miners in tents once lived, you can see luxury cars, venture capital–backed coffee chains, and sumptuous buses with tinted windows running along private routes, carrying workers to their offices in Mountain View or Menlo Park. But only a short walk away is Division Street, a multilane thoroughfare between SoMa and the Mission district, where rows of tents have returned to shelter people who have nowhere to go. In the wake of the tech boom, San Francisco now has one of the highest rates of street homelessness in the United States. The United Nations special rapporteur on adequate housing called it an “unacceptable” human rights violation, due to the thousands of homeless residents denied basic necessities of water, sanitation, and health services in contrast to the record number of billionaires who live nearby. The greatest benefits of extraction have been captured by the few.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - In this chapter we’ll traverse across Nevada, San Jose, and San Francisco, as well as Indonesia, Malaysia, China, and Mongolia: from deserts to oceans. We’ll also walk the spans of historical time, from conflict in the Congo and artificial black lakes in the present day to the Victorian passion for white latex. The scale will shift, telescoping from rocks to cities, trees to megacorporations, transoceanic shipping lanes to the atomic bomb. But across this planetary supersystem we will see the logics of extraction, a constant drawdown of minerals, water, and fossil fuels, undergirded by the violence of wars, pollution, extinction, and depletion. The effects of large-scale computation can be found in the atmosphere, the oceans, the earth’s crust, the deep time of the planet, and the brutal impacts on disadvantaged populations around the world. To understand it all, we need a panoramic view of the planetary scale of computational extraction.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - I’m driving through the desert valley on a summer afternoon to see the workings of this latest mining boom. I ask my phone to direct me to the perimeter of the lithium ponds, and it replies from its awkward perch on the dashboard, tethered by a white USB cable. Silver Peak’s large, dry lake bed was formed millions of years ago during the late Tertiary Period. It’s surrounded by crusted stratifications pushing up into ridgelines containing dark limestones, green quartzites, and gray and red slate. Lithium was discovered here after the area was scoped for strategic minerals like potash during World War II. This soft, silvery metal was mined in only modest quantities for the next fifty years, until it became highly valuable material for the technology sector.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - In 2014, Rockwood Holdings, Inc., a lithium mining operation, was acquired by the chemical manufacturing company Albemarle Corporation for $6.2 billion. It is the only operating lithium mine in the United States. This makes Silver Peak a site of intense interest to Elon Musk and the many other tech tycoons for one reason: rechargeable batteries. Lithium is a crucial element for their production. Smartphone batteries, for example, usually contain about three-tenths of an ounce of it. Each Tesla Model S electric car needs about one hundred thirty-eight pounds of lithium for its battery pack. These kinds of batteries were never intended to supply a machine as power hungry as a car, but lithium batteries are currently the only mass-market option available. All of these batteries have a limited lifespan; once degraded, they are discarded as waste.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - About two hundred miles north of Silver Peak is the Tesla Gigafactory. This is the world’s largest lithium battery plant. Tesla is the number-one lithium-ion battery consumer in the world, purchasing them in high volumes from Panasonic and Samsung and repackaging them in its cars and home chargers. Tesla is estimated to use more than twenty-eight thousand tons of lithium hydroxide annually—half of the planet’s total consumption. In fact, Tesla could more accurately be described as a battery business than a car company. The imminent shortage of such critical minerals as nickel, copper, and lithium poses a risk for the company, making the lithium lake at Silver Peak highly desirable. Securing control of the mine would mean controlling the U.S. domestic supply.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - As many have shown, the electric car is far from a perfect solution to carbon dioxide emissions. The mining, smelting, export, assemblage, and transport of the battery supply chain has a significant negative impact on the environment and, in turn, on the communities affected by its degradation. A small number of home solar systems produce their own energy. But for the majority of cases, charging an electric car necessitates taking power from the grid, where currently less than a fifth of all electricity in the United States comes from renewable energy sources. So far none of this has dampened the determination of auto manufacturers to compete with Tesla, putting increasing pressure on the battery market and accelerating the removal of diminishing stores of the necessary minerals.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - Global computation and commerce rely on batteries. The term “artificial intelligence” may invoke ideas of algorithms, data, and cloud architectures, but none of that can function without the minerals and resources that build computing’s core components. Rechargeable lithium-ion batteries are essential for mobile devices and laptops, in-home digital assistants, and data center backup power. They undergird the internet and every commerce platform that runs on it, from banking to retail to stock market trades. Many aspects of modern life have been moved to “the cloud” with little consideration of these material costs. Our work and personal lives, our medical histories, our leisure time, our entertainment, our political interests—all of this takes place in the world of networked computing architectures that we tap into from devices we hold in one hand, with lithium at their core.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - The mining that makes AI is both literal and metaphorical. The new extractivism of data mining also encompasses and propels the old extractivism of traditional mining. The stack required to power artificial intelligence systems goes well beyond the multilayered technical stack of data modeling, hardware, servers, and networks. The full-stack supply chain of AI reaches into capital, labor, and Earth’s resources—and from each, it demands an enormous amount. The cloud is the backbone of the artificial intelligence industry, and it’s made of rocks and lithium brine and crude oil.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - In his book A Geology of Media, theorist Jussi Parikka suggests we think of media not from Marshall McLuhan’s point of view—in which media are extensions of the human senses— but rather as extensions of Earth. Computational media now participate in geological (and climatological) processes, from the transformation of the earth’s materials into infrastructures and devices to the powering of these new systems with oil and gas reserves. Reflecting on media and technology as geological processes enables us to consider the radical depletion of non-renewable resources required to drive the technologies of the present moment. Each object in the extended network of an AI system, from network routers to batteries to data centers, is built using elements that required billions of years to form inside the earth.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - From the perspective of deep time, we are extracting Earth’s geological history to serve a split second of contemporary technological time, building devices like the Amazon Echo and the iPhone that are often designed to last for only a few years. The Consumer Technology Association notes that the average smartphone life span is a mere 4.7 years. This obsolescence cycle fuels the purchase of more devices, drives up profits, and increases incentives for the use of unsustainable extraction practices. After a slow process of development, these minerals, elements, and materials then go through an extraordinarily rapid period of excavation, processing, mixing, smelting, and logistical transport—crossing thousands of miles in their transformation. What begins as ore removed from the ground, after the spoil and the tailings are discarded, is then made into devices that are used and discarded. They ultimately end up buried in e-waste dumping grounds in places like Ghana and Pakistan. The lifecycle of an AI system from birth to death has many fractal supply chains: forms of exploitation of human labor and natural resources and massive concentrations of corporate and geopolitical power. And all along the chain, a continual, large-scale consumption of energy keeps the cycle going.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - The extractivism on which San Francisco was built is echoed in the practices of the tech sector based there today. The massive ecosystem of AI relies on many kinds of extraction: from harvesting the data made from our daily activities and expressions, to depleting natural resources, and to exploiting labor around the globe so that this vast planetary network can be built and maintained. And AI extracts far more from us and the planet than is widely known. The Bay Area is a central node in the mythos of AI, but we’ll need to traverse far beyond the United States to see the many-layered legacies of human and environmental damage that have powered the tech industry.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - The lithium mines in Nevada are just one of the places where the materials are extracted from the earth’s crust to make AI. There are many such sites, including the Salar in southwest Bolivia—the richest site of lithium in the world and thus a site of ongoing political tension—as well as places in central Congo, Mongolia, Indonesia, and the Western Australia deserts. These are the other birthplaces of AI in the greater geography of industrial extraction. Without the minerals from these locations, contemporary computation simply does not work. But these materials are in increasingly short supply.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - In 2020, scientists at the U.S. Geological Survey published a short list of twenty-three minerals that are a high “supply risk” to manufacturers, meaning that if they became unavailable, entire industries — including the tech sector—would grind to a halt. The critical minerals include the rare earth elements dysprosium and neodymium, which are used inside iPhone speakers and electric vehicle motors; germanium, which is used in infrared military devices for soldiers and in drones; and cobalt, which improves performance for lithium-ion batteries.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - There are seventeen rare earth elements: lanthanum, cerium, praseodymium, neodymium, promethium, samarium, europium, gadolinium, terbium, dysprosium, holmium, erbium, thulium, ytterbium, lutetium, scandium, and yttrium. They are processed and embedded in laptops and smartphones, making those devices smaller and lighter. The elements can be found in color displays, speakers, camera lenses, rechargeable batteries, hard drives, and many other components. They are key elements in communication systems, from fiber-optic cables and signal amplification in mobile communication towers to satellites and GPS technology. But extracting these minerals from the ground often comes with local and geopolitical violence. Mining is and always has been a brutal undertaking. As Lewis Mumford writes, “Mining was the key industry that furnished the sinews of war and increased the metallic contents of the original capital hoard, the war chest: on the other hand, it furthered the industrialization of arms, and enriched the financier by both processes.” To understand the business of AI, we must reckon with the war, famine, and death that mining brings with it.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - Recent U.S. legislation that regulates some of those seventeen rare earth elements only hints at the devastation associated with their extraction. The 2010 Dodd-Frank Act focused on reforming the financial sector in the wake of the 2008 financial crisis. It included a specific provision about so-called conflict minerals, or natural resources extracted in a conflict zone and then sold to fund the conflict. Companies using gold, tin, tungsten, and tantalum from the region around the Democratic Republic of the Congo now had a reporting requirement to track where those minerals came from and whether the sale was funding armed militia in the region. Like “conflict diamonds,” the term “conflict minerals” masks the profound suffering and prolific killing in the mining sector. Mining profits have financed military operations in the decades-long Congo-area conflict, fueling the deaths of thousands and the displacement of millions. Furthermore, working conditions inside the mines have often amounted to modern slavery.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - It took Intel more than four years of sustained effort to develop basic insight into its own supply chain. Intel’s supply chain is complex, with more than sixteen thousand suppliers in over a hundred countries providing direct materials for the company’s production processes, tools, and machines for their factories, as well as their logistics and packaging services. In addition, Intel and Apple have been criticized for auditing only smelters—not the actual mines—to determine the conflict-free status of minerals. The tech giants were assessing smelting plants outside of Congo, and the audits were often performed by locals. So even the conflict-free certifications of the tech industry are now under question.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - Dutch-based technology company Philips has also claimed that it was working to make its supply chain “conflict-free.” Like Intel, Philips has tens of thousands of suppliers, each of which provides component parts for the company’s manufacturing processes. Those suppliers are themselves linked downstream to thousands of component manufacturers acquiring treated materials from dozens of smelters. The smelters in turn buy their materials from an unknown number of traders who deal directly with both legal and illegal mining operations to source the various minerals that end up in computer components.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - According to the computer manufacturer Dell, the complexities of the metals and mineral supply chains pose almost insurmountable challenges to the production of conflict-free electronics components. The elements are laundered through such a vast number of entities along the chain that sourcing their provenance proves impossible—or so the end-product manufacturers claim, allowing them a measure of plausible deniability for any exploitative practices that drive their profits.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - Just like the mines that served San Francisco in the nineteenth century, extraction for the technology sector is done by keeping the real costs out of sight. Ignorance of the supply chain is baked into capitalism, from the way businesses protect themselves through third-party contractors and suppliers to the way goods are marketed and advertised to consumers. More than plausible deniability, it has become a well-practiced form of bad faith: the left hand cannot know what the right hand is doing, which requires increasingly lavish, baroque, and complex forms of distancing.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - China supplies 95 percent of the world’s rare earth minerals. China’s market domination, as the writer Tim Maughan observes, owes far less to geology than to the country’s willingness to take on the environmental damage of extraction. Although rare earth minerals like neodymium and cerium are relatively common, making them usable requires the hazardous process of dissolving them in large volumes of sulfuric and nitric acid. These acid baths yield reservoirs of poisonous waste that fill the dead lake in Baotou. This is just one of the places that are brimming with what environmental studies scholar Myra Hird calls “the waste we want to forget.”
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - To date, the unique electronic, optical, and magnetic uses of rare earth elements cannot be matched by any other metals, but the ratio of usable minerals to waste toxins is extreme. Natural resource strategist David Abraham describes the mining in Jiangxi, China, of dysprosium and terbium, which are used in a variety of high-tech devices. He writes, “Only 0.2 percent of the mined clay contains the valuable rare earth elements. This means that 99.8 percent of earth removed in rare earth mining is discarded as waste, called ‘tailings,’ that are dumped back into the hills and streams,” creating new pollutants like ammonium. In order to refine one ton of these rare earth elements, “the Chinese Society of Rare Earths estimates that the process produces 75,000 liters of acidic water and one ton of radioactive residue.”
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - About three thousand miles south of Baotou are the small Indonesian islands of Bangka and Belitung, off the coast of Sumatra. Bangka and Belitung produce 90 percent of Indonesia’s tin, used in semiconductors. Indonesia is the world’s second-largest producer of the metal, behind China. Indonesia’s national tin corporation, PT Timah, supplies companies such as Samsung directly, as well as solder makers Chernan and Shenmao, which in turn supply Sony, LG, and Foxconn — all suppliers for Apple, Tesla, and Amazon.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - On these small islands, gray-market miners who are not officially employed sit on makeshift pontoons, using bamboo poles to scrape the seabed before diving underwater to suck tin from the surface by drawing their breath through giant, vacuumlike tubes. The miners sell the tin they find to middlemen, who also collect ore from miners working in authorized mines, and they mix it together to sell to companies like Timah. Completely unregulated, the process unfolds beyond any formal worker or environmental protections. As investigative journalist Kate Hodal reports, “Tin mining is a lucrative but destructive trade that has scarred the island’s landscape, bulldozed its farms and forests, killed off its fish stocks and coral reefs, and dented tourism to its pretty palm-lined beaches. The damage is best seen from the air, as pockets of lush forest huddle amid huge swaths of barren orange earth. Where not dominated by mines, this is pockmarked with graves, many holding the bodies of miners who have died over the centuries digging for tin.” The mines are everywhere: in backyards, in the forest, by the side of the road, on the beaches. It is a landscape of ruin.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - It is a common practice of life to focus on the world immediately before us, the one we see and smell and touch every day. It grounds us where we are, with our communities and our known corners and concerns. But to see the full supply chains of AI requires looking for patterns in a global sweep, a sensitivity to the ways in which the histories and specific harms are different from place to place and yet are deeply interconnected by the multiple forces of extraction.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - We can see these patterns across space, but we can also find them across time. Transatlantic telegraph cables are the essential infrastructure that ferries data between the continents, an emblem of global communication and capital. They are also a material product of colonialism, with its patterns of extraction, conflict, and environmental destruction. At the end of the nineteenth century, a particular Southeast Asian tree called Palaquium gutta became the center of a cable boom. These trees, found mainly in Malaysia, produce a milky white natural latex called gutta-percha. After English scientist Michael Faraday published a study in the Philosophical Magazine in 1848 about the use of this material as an electrical insulator, gutta-percha rapidly became the darling of the engineering world. Engineers saw gutta-percha as the solution to the problem of insulating telegraphic cables to withstand harsh and varying conditions on the ocean floor. The twisted strands of copper wire needed four layers of the soft, organic tree sap to protect them from water incursion and carry their electrical currents.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - As the global submarine telegraphy business grew, so did demand for Palaquium gutta tree trunks. The historian John Tully describes how local Malay, Chinese, and Dayak workers were paid little for the dangerous work of felling the trees and slowly collecting the latex. The latex was processed and then sold through Singapore’s trade markets into the British market, where it was transformed into, among other things, lengths upon lengths of submarine cable sheaths that wrapped around the globe. As media scholar Nicole Starosielski writes, “Military strategists saw cables as the most efficient and secure mode of communication with the colonies—and, by implication, of control over them.” The routes of submarine cables today still mark out the early colonial networks between the centers and the peripheries of empire.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - A mature Palaquium gutta could yield around eleven ounces of latex. But in 1857, the first transatlantic cable was around eighteen hundred miles long and weighed two thousand tons—requiring about 250 tons of gutta-percha. To produce just one ton of this material required around nine hundred thousand tree trunks. The jungles of Malaysia and Singapore were stripped; by the early 1880s, the Palaquium gutta had vanished. In a last-ditch effort to save their supply chain, the British passed a ban in 1883 to halt harvesting the latex, but the tree was all but extinct.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - The Victorian environmental disaster of gutta-percha, at the dawn of the global information society, shows how the relations between technology and its materials, environments, and labor practices are interwoven. Just as Victorians precipitated ecological disaster for their early cables, so do contemporary mining and global supply chains further imperil the delicate ecological balance of our era.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - There are dark ironies in the prehistories of planetary computation. Currently large-scale AI systems are driving forms of environmental, data, and human extraction, but from the Victorian era onward, algorithmic computation emerged out of desires to manage and control war, population, and climate change. The historian Theodora Dryer describes how the founding figure of mathematical statistics, English scientist Karl Pearson, sought to resolve uncertainties of planning and management by developing new data architectures including standard deviations and techniques of correlation and regression. His methods were, in turn, deeply imbricated with race science, as Pearson—along with his mentor, the statistician and founder of eugenics Sir Francis Galton—believed that statistics could be “the first step in an enquiry into the possible effect of a selective process upon any character of a race.”
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - As Dryer writes, “By the end of the 1930s, these data architectures—regression techniques, standard deviation, and correlations—would become dominant tools used in interpreting social and state information on the world stage. Tracking the nodes and routes of global trade, the interwar ‘mathematical-statistics movement’ became a vast enterprise.” This enterprise kept expanding after World War II, as new computational systems were used in domains such as weather forecasting during periods of drought to eke out more productivity from large-scale industrial farming. From this perspective, algorithmic computing, computational statistics, and artificial intelligence were developed in the twentieth century to address social and environmental challenges but would later be used to intensify industrial extraction and exploitation and further deplete environmental resources.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - Minerals are the backbone of AI, but its lifeblood is still electrical energy. Advanced computation is rarely considered in terms of carbon footprints, fossil fuels, and pollution; metaphors like “the cloud” imply something floating and delicate within a natural, green industry. Servers are hidden in nondescript data centers, and their polluting qualities are far less visible than the billowing smokestacks of coal-fired power stations. The tech sector heavily publicizes its environmental policies, sustainability initiatives, and plans to address climate-related problems using AI as a problem-solving tool. It is all part of a highly produced public image of a sustainable tech industry with no carbon emissions. In reality, it takes a gargantuan amount of energy to run the computational infrastructures of Amazon Web Services or Microsoft’s Azure, and the carbon footprint of the AI systems that run on those platforms is growing.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - As Tung-Hui Hu writes in A Prehistory of the Cloud, “The cloud is a resource-intensive, extractive technology that converts water and electricity into computational power, leaving a sizable amount of environmental damage that it then displaces from sight.” Addressing this energy-intensive infrastructure has become a major concern. Certainly, the industry has made significant efforts to make data centers more energy efficient and to increase their use of renewable energy. But already, the carbon footprint of the world’s computational infrastructure has matched that of the aviation industry at its height, and it is increasing at a faster rate. Estimates vary, with researchers like Lotfi Belkhir and Ahmed Elmeligi estimating that the tech sector will contribute 14 percent of global greenhouse emissions by 2040, while a team in Sweden predicts that the electricity demands of data centers alone will increase about fifteenfold by 2030.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - By looking closely at the computational capacity needed to build AI models, we can see how the desire for exponential increases in speed and accuracy is coming at a high cost to the planet. The processing demands of training AI models, and thus their energy consumption, is still an emerging area of investigation. One of the early papers in this field came from AI researcher Emma Strubell and her team at the University of Massachusetts Amherst in 2019. With a focus on trying to understand the carbon footprint of natural language processing (NLP) models, they began to sketch out potential estimates by running Al models over hundreds of thousands of computational hours. The initial numbers were striking. Strubell’s team found that running only a single NLP model produced more than 660,000 pounds of carbon dioxide emissions, the equivalent of five gas-powered cars over their total lifetime (including their manufacturing) or 125 round-trip flights from New York to Beijing.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - Worse, the researchers noted that this modeling is, at minimum, a baseline optimistic estimate. It does not reflect the true commercial scale at which companies like Apple and Amazon operate, scraping internet-wide datasets and feeding their own NLP models to make AI systems like Siri and Alexa sound more human. But the exact amount of energy consumption produced by the tech sector’s AI models is unknown; that information is kept as highly guarded corporate secrets. Here, too, the data economy is premised on maintaining environmental ignorance.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - In the AI field, it is standard practice to maximize computational cycles to improve performance, in accordance with a belief that bigger is better. As Rich Sutton of DeepMind describes it: “Methods that leverage computation are ultimately the most effective, and by a large margin.” The computational technique of brute-force testing in AI training runs, or systematically gathering more data and using more computational cycles until a better result is achieved, has driven a steep increase in energy consumption. OpenAI estimated that since 2012, the amount of compute used to train a single AI model has increased by a factor of ten every year. That’s due to developers “repeatedly finding ways to use more chips in parallel, and being willing to pay the economic cost of doing so.” Thinking only in terms of economic cost narrows the view on the wider local and environmental price of burning computation cycles as a way to create incremental efficiencies. The tendency toward “compute maximalism” has profound ecological impacts.
[Author: John Unsworth; From essay:"What is Humanities Computing and What is Not "] - selecting a representation means making a set of ontological commitments. The commitments are in effect a strong pair of glasses that determine what we can see, bringing some part of the world into sharp focus, at the expense of blurring other parts. These commitments and their focusing/blurring effect are not an incidental side effect of a representation choice; they are of the essence: a KR is a set of ontological commitments. It is unavoidably so because of the inevitable imperfections of representations. It is usefully so because judicious selection of commitments provides the opportunity to focus attention on aspects of the world we believe to be relevant.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - Data centers are among the world’s largest consumers of electricity. Powering this multilevel machine requires grid electricity in the form of coal, gas, nuclear, or renewable energy. Some corporations are responding to growing alarm about the energy consumption of large-scale computation, with Apple and Google claiming to be carbon neutral (which means they offset their carbon emissions by purchasing credits) and Microsoft promising to become carbon negative by 2030. But workers within the companies have pushed for reductions in emissions across the board, rather than what they see as buying indulgences out of environmental guilt. Moreover, Microsoft, Google, and Amazon all license their AI platforms, engineering workforces, and infrastructures to fossil fuel companies to help them locate and extract fuel from the ground, which further drives the industry most responsible for anthropogenic climate change.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - Beyond the United States, more clouds of carbon dioxide are rising. China’s data center industry draws 73 percent of its power from coal, emitting about 99 million tons of CO2 in 2018. And electricity consumption from China’s data center infrastructure is expected to increase by two-thirds by 2023. Greenpeace has raised the alarm about the colossal energy demands of China’s biggest technology companies, arguing that “China’s leading tech companies, including Alibaba, Tencent, and GDS, must dramatically scale up clean energy procurement and disclose energy use data.” But the lasting impacts of coal-fired power are everywhere, exceeding any national boundaries. The planetary nature of resource extraction and its consequences goes well beyond what the nation-state was designed to address.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - Water tells another story of computation’s true cost. The history of water use in the United States is full of battles and secret deals, and as with computation, the deals made over water are kept close. One of the biggest U.S. data centers belongs to the National Security Agency (NSA) in Bluffdale, Utah. Open since late 2013, the Intelligence Community Comprehensive National Cybersecurity Initiative Data Center is impossible to visit directly. But by driving up through the adjacent suburbs, I found a cul-de-sac on a hill thick with sagebrush, and from there I was afforded a closer view of the sprawling 1.2-million-square-foot facility. The site has a kind of symbolic power of the next era of government data capture, having been featured in films like Citizenfour and pictured in thousands of news stories about the NSA. In person, though, it looks nondescript and prosaic, a giant storage container combined with a government office block.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - The struggle over water began even before the data center was officially open, given its location in drought-parched Utah. Local journalists wanted to confirm whether the estimated consumption of 1.7 million gallons of water per day was accurate, but the NSA initially refused to share usage data, redacted all details from public records, and claimed that its water use was a matter of national security. Antisurveillance activists created handbooks encouraging the end of material support of water and energy to surveillance, and they strategized that legal controls over water usage could help shut down the facility. But the city of Bluffdale had already made a multiyear deal with the NSA, in which the city would sell water at rates well below the average in return for the promise of economic growth the facility might bring to the region. The geopolitics of water are now deeply combined with the mechanisms and politics of data centers, computation, and power—in every sense. From the dry hillside that overlooks the NSA’s data repository, all the contestation and obfuscation about water makes sense: this is a landscape with a limit, and water that is used to cool servers is being taken away from communities and habitats that rely on it to live.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - Just as the dirty work of the mining sector was far removed from the companies and city dwellers who profited most, so the majority of data centers are far removed from major population hubs, whether in the desert or in semi-industrial exurbs. This contributes to our sense of the cloud being out of sight and abstracted away, when in fact it is material, affecting the environment and climate in ways that are far from being fully recognized and accounted for. The cloud is of the earth, and to keep it growing requires expanding resources and layers of logistics and transport that are in constant motion.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - So far, we have considered the material stuff of AI, from rare earth elements to energy. By grounding our analysis in the specific materialities of AI—the things, places, and people— we can better see how the parts are operating within broader systems of power. Take, for example, the global logistical machines that move minerals, fuel, hardware, workers, and consumer Al devices around the planet. The dizzying spectacle of logistics and production displayed by companies like Amazon would not be possible without the development and widespread acceptance of a standardized metal object: the cargo container. Like submarine cables, cargo containers bind the industries of global communication, transport, and capital, a material exercise of what mathematicians call “optimal transport”—in this case, as an optimization of space and resources across the trade routes of the world.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - Standardized cargo containers (themselves built from the basic earth elements of carbon and iron forged as steel) enabled the explosion of the modern shipping industry, which in turn made it possible to envision and model the planet as a single massive factory. The cargo container is the single unit of value—like a piece of Lego—that can travel thousands of miles before meeting its final destination as a modular part of a greater system of delivery. In 2017, the capacity of container ships in seaborne trade reached nearly 250 million deadweight tons of cargo, dominated by giant shipping companies including Maersk of Denmark, the Mediterranean Shipping Company of Switzerland, and France’s CMA CGM Group, each owning hundreds of container vessels. For these commercial ventures, cargo shipping is a relatively cheap way to navigate the vascular system of the global factory, yet it disguises far larger external costs. Just as they tend to neglect the physical realities and costs of AI infrastructure, popular culture and media rarely cover the shipping industry. The author Rose George calls this condition “sea blindness.”
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - Here, too, the most severe costs of global logistics are borne by the Earth’s atmosphere, the oceanic ecosystem and low-paid workers. The corporate imaginaries of AI fail to depict the lasting costs and long histories of the materials needed to build computational infrastructures or the energy required to power them. The rapid growth of cloud-based computation, portrayed as environmentally friendly, has paradoxically driven an expansion of the frontiers of resource extraction. It is only by factoring in these hidden costs, these wider collections of actors and systems, that we can understand what the shift toward increasing automation will mean. This requires working against the grain of how the technological imaginary usually works, which is completely untethered from earthly matters. Like running an image search of “AI,” which returns dozens of pictures of glowing brains and blue-tinted binary code floating in space, there is a powerful resistance to engaging with the materialities of these technologies. Instead, we begin with the earth, with extraction, and with the histories of industrial power and then consider how these patterns are repeated in systems of labor and data.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - In the late 1960s, the historian and philosopher of technology Lewis Mumford developed the concept of the megamachine to illustrate how all systems, no matter how immense, consist of the work of many individual human actors. For Mumford, the Manhattan Project was the defining modern megamachine whose intricacies were kept not only from the public but even from the thousands of people who worked on it at discrete, secured sites across the United States. A total of 130,000 workers operated in complete secrecy under the direction of the military, developing a weapon that would kill (by conservative estimates) 237,000 people when it hit Hiroshima and Nagasaki in 1945. The atomic bomb depended on a complex, secret chain of supply, logistics, and human labor.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - Artificial intelligence is another kind of megamachine, a set of technological approaches that depend on industrial infrastructures, supply chains, and human labor that stretch around the globe but are kept opaque. We have seen how AI is much more than databases and algorithms, machine learning models and linear algebra. It is metamorphic: relying on manufacturing, transportation, and physical work; data centers and the undersea cables that trace lines between the continents; personal devices and their raw components; transmission signals passing through the air; datasets produced by scraping the internet; and continual computational cycles. These all come at a cost.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - We have looked at the relations between cities and mines, companies and supply chains, and the topographies of extraction that connect them. The fundamentally intertwined nature of production, manufacturing, and logistics reminds us that the mines that drive AI are everywhere: not only sited in discrete locations but diffuse and scattered across the geography of the earth, in what Mazen Labban has called the “planetary mine.” This is not to deny the many specific locations where technologically driven mining is taking place. Rather, Labban observes that the planetary mine expands and reconstitutes extraction into novel arrangements, extending the practices of mines into new spaces and interactions around the world.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - Finding fresh methods for understanding the deep material and human roots of AI systems is vital at this moment in history, when the impacts of anthropogenic climate change are already well under way. But that’s easier said than done. In part, that’s because many industries that make up the AI system chain conceal the ongoing costs of what they do. Furthermore, the scale required to build artificial intelligence systems is too complex, too obscured by intellectual property law, and too mired in logistical and technical complexity for us to see into it all. But the aim here is not to try and make these complex assemblages transparent: rather than trying to see inside them, we will be connecting across multiple systems to understand how they work in relation to each other. Thus, our path will follow the stories about the environmental and labor costs of AI and place them in context with the practices of extraction and classification braided throughout everyday life. It is by thinking about these issues together that we can work toward greater justice.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - I make one more trip to Silver Peak. Before I reach the town, I pull the van over to the side of the road to read a weather-beaten sign. It’s Nevada Historical Marker 174, dedicated to the creation and destruction of a small town called Blair. In 1906, the Pittsburgh Silver Peak Gold Mining Company bought up the mines in the area. Anticipating a boom, land speculators purchased all of the available plots near Silver Peak along with its water rights, driving prices to record artificial highs. So the mining company surveyed a couple of miles north and declared it the site for a new town: Blair. They built a hundred-stamp cyanide mill for leach mining, the biggest in the state, and laid the Silver Peak railroad that ran from Blair Junction to the Tonopah and Goldfield main line. Briefly, the town thrived. Many hundreds of people came from all over for the jobs, despite the harsh working conditions. But with so much mining activity, the cyanide began to poison the ground, and the gold and silver seams began to falter and dry up. By 1918, Blair was all but deserted. It was all over within twelve years. The ruins are marked on a local map—just a forty-five-minute walk away.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - It’s a blazing hot day in the desert. The only sounds are the metallic reverberations of cicadas and the rumble of an occasional passenger jet. I decide to start up the hill. By the time I reach the collection of stone buildings at the top of the long dirt road, I’m exhausted from the heat. I take shelter inside the collapsed remains of what was once a gold miner’s house. Not much is left: some broken crockery, shards of glass bottles, a few rusted tins. Back in Blair’s lively years, multiple saloons thrived nearby and a two-story hotel welcomed visitors. Now it’s a cluster of broken foundations.
[Author: Kate Crawford; From essay:"The Atlas of AI Power Politics and the Planetary Costs of Artificial Intelligence "] - Through the space where a window used to be, the view stretches all the way down the valley. I’m struck by the realization that Silver Peak will also be a ghost town soon. The current draw on the lithium mine is aggressive in response to the high demand, and no one knows how long it will last. The most optimistic estimate is forty years, but the end may come much sooner. Then the lithium pools under the Clayton Valley will be exsanguinated—extracted for batteries that are destined for landfill. And Silver Peak will return to its previous life as an empty and quiet place, on the edge of an ancient salt lake, now drained.
[Author: Javier Cha; From essay:"Big Data Studies The Humanities in Uncharted Waters "] - This article discusses the formidable challenges that the advent of big data brings to the digital humanities broadly and proposes some ways the Korean studies community can prepare to navigate these uncharted waters. Standard digital humanities training in data mining, text analysis, mapping, network science, and machine learning will be developed and refined over the coming years, as will research concerning the ephemeral nature of new media, web archives, and the ethics of artificial intelligence. Yet I contend that established responses to the digital transformation of the humanities, while timely and necessary, will prove inadequate for handling petabyte- and exabyte-scale born-digital sources. In the Zettabyte Era, more data is processed in real time than all of the records produced from early times to the 2010s. To make sense of the current information regime, we need critical reflections and comparisons to the classical internet age of the 1990s, the personal computer revolution of the 1980s, and early modern print cultures. This exercise will allow us to situate the humanities in an age of big data as an extension of traditional humanities research and at the same as something foreign.
[Author: Javier Cha; From essay:"Big Data Studies The Humanities in Uncharted Waters "] - The Humanities in the Zettabyte EraIn 2012, the Zettabyte Era began. To store one zettabyte on Blu-ray discs, each with a storage capacity of 25 gigabytes and a thickness of 1.2 millimeters, 37.2 billion discs must be stacked 45,000 kilometers high. In 2020, global data production surpassed 59 zettabytes.[ 1 ] The hypothetical Blu-ray tower for 59 zettabytes would span 2,637,000 kilometers, or 6.9 times the distance to the Moon.
[Author: Javier Cha; From essay:"Big Data Studies The Humanities in Uncharted Waters "] - The astronomical amounts of data being generated in the twenty-first century spur humanists to reflect on the directions, principles, assumptions, and methodologies of our respective disciplines. Standard digital humanities training in data mining, text analysis, mapping, network science, and machine learning will continue to be relevant, as will research concerning the ephemeral nature of new media, web archives, and the ethics of artificial intelligence. Conversely, the new humanities in the Zettabyte Era will require a comprehensive rethinking of everything from material bibliography to data authentication and analytics, archival preservation, and environmental impact. In 2000, John Unsworth introduced the notion of scholarly primitives in humanities computing.[ 2 ] Twenty-three years later, research practices and pedagogy need to account for the implications of big datas architectural characteristics, as encapsulated in Doug Laneys famous 3Vs: volume, velocity, and variety.[ 3 ]
[Author: Javier Cha; From essay:"Big Data Studies The Humanities in Uncharted Waters "] - Each of the 3Vs of big data presents unique challenges for the humanities that cannot be dismissed as mere historical repetition. Koreanists, for instance, must debate and determine the boundaries of the Korean web and the portions that should be earmarked for long-term preservation. Mapping the geographic, linguistic, and legal boundaries of the internet, which has never been more dynamic, is a complex task. Every day, millions of South Koreans access digital contents provided by Netflix and YouTube, multinational tech giants headquartered in the United States, through content-delivery networks located on or near the Korean peninsula. Namu Wiki, one of the most popular Korean-language knowledge bases, is operated by Paraguay-based limited liability company Umanle using servers located in Slovakia to partially circumvent South Koreas online censorship laws. Moreover, we must pay attention to new data types. By size, eighty percent of big data is said to be unstructured, and the proportion of the more traditional tabular and textual data produced has decreased over time.[ 4 ] How do we archive and navigate the ocean of data that consists primarily of images, audio, video, and 3d point clouds? What about massively multiplayer online role-playing games or blockchain-based metaverses? The third V of big data introduces its own set of difficulties.
[Author: Javier Cha; From essay:"Big Data Studies The Humanities in Uncharted Waters "] - This article discusses the formidable challenges that the advent of big data brings to the digital humanities broadly and proposes some ways the Korean studies community can prepare to navigate these uncharted waters. The commendable range of digital humanities approaches demonstrated in this special section will be developed and refined over the coming years. Yet I contend that established responses to the digital transformation of the humanities, while timely and necessary, will prove inadequate for handling petabyte- and exabytescale born-digital sources. In the Zettabyte Era, more data is processed in real time than all of the records produced from early times to the 2010s. To make sense of the current information regime, we need critical reflections and comparisons to the classical internet age of the 1990s, the personal computer revolution of the 1980s, and early modern print cultures. This exercise will allow us to situate the humanities in an age of big data as an extension of traditional humanities research and at the same as something foreign.
[Author: Javier Cha; From essay:"Big Data Studies The Humanities in Uncharted Waters "] - The Materiality of Big DataInsofar as digital humanists are concerned with static data sets that can be handled in a single personal computer, big data may not appear to be entirely unprecedented. Upon taking note of the facilities and infrastructure that connect millions of servers around the world, however, we realize that big datas characteristics are distinct from those of previous information regimes and media landscapes. In 2014, Facebook processed 4 PB[ 5 ] and Naver 1.2 billion requests for its 18 million portal users daily.[ 6 ] With 50 billion photos posted to its servers as of 2022, Instagram is the largest repository of visual materials ever created.[ 7 ] The total amount of video uploaded to YouTube from 2005 or 2019 is 95,000 years.[ 8 ] In addition to search portals and social media platforms, 6.6 billion smartphones,[ 9 ] 1.5 billion personal computers,10 tens of millions of servers,[11] and countless sensors contribute to the global production of big data. As one among 2.9 billion monthly active users of Facebook and more than 1 billion users of Google Drive, I uploaded 4.7 GB and 1.05 TB to their servers, respectively.
[Author: Javier Cha; From essay:"Big Data Studies The Humanities in Uncharted Waters "] - Smartphones are primarily responsible for this sea change. As of 2022, there are 6.6 billion smartphone users worldwide,[53] including 95% of South Korean adults and 76% of those who live in advanced economies.[54] Smartphones are intelligent devices equipped with an array of advanced sensors, including cameras, lenses, gyroscopes, compasses, location trackers, and depth sensors. Consider what happens when a smartphone user takes a photograph. Depending on the settings, a semiprofessional-grade CMOS sensor captures the subjects light via the main, ultrawideangle, or telephoto lens. Internally on the device, substantial post-processing occurs. Software enhancers may use ToF or LiDAR sensor-collected three-dimensional data to render the photograph into a more visually appealing form. Advanced triangulation techniques that combine data from GPS satellites, cell towers, and WiFi routers yield a precise approximation of the geocoordinates where the user captured the image, which is embedded in the file.
[Author: Javier Cha; From essay:"Big Data Studies The Humanities in Uncharted Waters "] - The Oxford English Dictionary defines big data succinctly as data of a very large size, typically to the extent that its manipulation present significant logistical challenges.[12] The casual use of the buzzword big data to refer to any ostensibly large data set, or as a substitute word for data science, disregards the significance of data materiality.[13] While extremely large databases introduce new analytical problems, query services that operate at a high abstraction layer, such as Google BigQuery and Amazon Athena, eliminate most of these difficulties. Most pertinent to the future direction of the humanities is what the Oxford English Dictionary means by significant logistical challenges. Humanists are trained not to treat documents, books, inscriptions, and databases as self-evident stores of information. Early modern historians pay close attention to printing technologies, paper production, ink, circulation, reception, and reading cultures when handling primary sources. Similarly, source criticism in the context of big data entails engagement with the physical infrastructure designed to enable the handling and circulation of petabytes and exabytes of data: data centers.
[Author: Javier Cha; From essay:"Big Data Studies The Humanities in Uncharted Waters "] - Given that big data exists as a distributed collection of digital storage devices, it is essential to describe the hardware required to store and process massive amounts of data. In 2023, a typical notebook computer has one terabyte of storage in the form of an M.2 NVMe solid-state drive that is 22 mm wide and 80 mm long. One petabyte is equivalent to 1024 one-terabyte M.2 drives, while one exabyte constitutes 1024 PB. A data migration service offered by Amazon Web Services (AWS) helps us visualize the physicality of one exabyte, or 1,048,576 terabytes. The transmission of 1 EB over a 10 Gbps fiber optic connection is estimated to take twenty-six years.[14] AWS Snowmobile loads extremely large amounts of data onto semi-trailer trucks carrying shock-proof hard disks; each truck is capable of transporting 100 PB, allowing the transfer of exabyte-scale data in weeks rather than decades.[15]
[Author: Javier Cha; From essay:"Big Data Studies The Humanities in Uncharted Waters "] - At the exabyte scale, our analytical bibliography of big data turns to supersized data centers at multiple locations around the world built by cloud industry leaders such as Facebook, Amazon, Microsoft, Google, Alibaba, and Naver. Each facility is the size of several football stadiums equipped with hundreds of thousands to millions of hard disks and solid-state drives. Facebook operates a 150,000-m[ 2 ] data center cluster in Clonee, Ireland, which as of 2019 consisted of three facilities and ten more under construction. In 2014, Naver launched its flagship data center Kak (각 or 閣 as a tribute to Changgyŏnggak 藏經閣, which housed the Tripiṭaka Koreana) in Chunchŏn, with 120,000 servers capable of handling more data on its first day than ten-thousand National Libraries of Korea combined.[16]
[Author: Javier Cha; From essay:"Big Data Studies The Humanities in Uncharted Waters "] - Due to the massive physical infrastructure required to store and process modern-day primary sources, energy has emerged as another significant logistical challenge of big data. In the context of a small number of personal computers and mobile devices, the fact that digital media, unlike paper, requires electricity to store and retrieve information is not particularly problematic. Hundreds of server units have already begun to cause issues, however. In 2008, the National Library of Koreas opening of the new digital library placed a tremendous strain on its electrical grid, necessitating the installation of a new substation and a dedicated power source.[17] Data centers have substantially higher energy needs. In 2020, Navers Kak used 156,875 MWh of electricity.[18] Facebooks Clonee site has access to 642 MW, which is enough to power 300,000 average American homes.[19] As of 2020, data centers consume approximately one percent of the worlds electricity, and this figure is expected to increase in the coming decades.20
[Author: Javier Cha; From essay:"Big Data Studies The Humanities in Uncharted Waters "] - Meanwhile, global data production continues to grow at an unprecedented rate. A 2012 prediction anticipated that 40 zettabytes of data would be generated by 2020,[21] but the actual amount turned out to be 59 zettabytes. According to a 2018 report, 175 zettabytes will be created by 2025.[22] Considering that the Covid-19 pandemic caused a 47 percent increase in internet usage in 2020, the total size of global big data in the 2020s is anticipated to surpass all expectations.[23]
[Author: Javier Cha; From essay:"Big Data Studies The Humanities in Uncharted Waters "] - Archiving Big DataThe 1493 Korean reprint of the Chinese encyclopedia Gujin shiwen leiju 古今事文類聚 (Korean Kogŭm samun yuchwi) was one of the most prized books of its time. In 2008, as a graduate student attending a seminar on book history, I discovered that one copy was available in the universitys rare book collection. Despite being more than 500 years old, the volume that my classmates and I examined was in pristine condition, which was perhaps not surprising given the circumstances of its publication. King Sŏngjong 成宗 (r. 1469–1494), who personally commissioned the project that took eight years to complete, ensured that the books were printed on high-grade paper.
[Author: Javier Cha; From essay:"Big Data Studies The Humanities in Uncharted Waters "] - The same cannot be said about digital sources. Most consumer-grade digital storage media are not capable of retaining data for 500 years. Data in
[Author: Javier Cha; From essay:"Big Data Studies The Humanities in Uncharted Waters "] - The 1493 Korean reprint of the Chinese encyclopedia Gujin shiwen leiju. Courtesy of Harvard-Yenching Librarys Korean rare book digitization project https://iiif.lib.harvard.edu/manifests/view/drs:9345235$5i.
[Author: Javier Cha; From essay:"Big Data Studies The Humanities in Uncharted Waters "] - CPU cache and RAM are lost immediately without power. The lifespans of disks depend on whether they experienced prolonged stress, such as constant reads and writes in a server, and whether they were designed for performance or archival purposes. Whereas solid-state drives are relatively more resistant to physical shocks, they are said to retain data typically for about 7 to 10 years on average; while hard disks generally last for up to 30 years, helium-sealed enterprise-grade drives are less likely to experience failure and corruption. Archival-grade optical discs are estimated by Kodak to be able to guarantee data integrity for up to 200 years,[24] but only under specific conditions. The discs must be kept vertically to prevent them from sticking to the case and held in a climate-controlled room set to 25 degrees Celsius and 40% humidity.[25]
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - Word segmentation is another import site of friction: Chinese, Japanese, and Korean do not have clear markers that distinguish words, making analyses that depend on word tokenization difficult to implement. Commercial concerns have driven the creation of segmenters for modern CJK, but we still lack an accurate classical Chinese word segmenter. This is a major problem considering that most premodern East Asian materials are written in some form of classical Chinese.
[Author: Javier Cha; From essay:"Big Data Studies The Humanities in Uncharted Waters "] - The foregoing discussions of data volume, data centers, and energy consumption are significant to the extent that big data can be preserved for posterity. How will future historians study the 2010s and 2020s? What will archaeologists be able to unearth from the ruins of Kak? The ephemeral nature of digital media has reversed our relationship with primary sources. On this ground, archivists and technologists have voiced concerns about the digital dark age since the 1980s, which has largely gone unaddressed and has been exacerbated by the world wide web and big data. The digital historian Roy Rosenzweig initiated the abundance versus scarcity debate in response to the emergence of inexpensive mass storage solutions such as optical discs and Web 1.0 hypermedia.[26] Web 2.0 has further complicated this. Even setting aside the problem of bit durability, the physical presence of big data in the form of data centers and global communications infrastructure means that decades from now, let alone centuries, people will not have the option of excavating what remains of todays digital devices and servers, powering them up, and extracting information from them.
[Author: Javier Cha; From essay:"Big Data Studies The Humanities in Uncharted Waters "] - Our access to the past web relies almost exclusively on the Internet Archive. Between 1996 and 2020, the San Francisco-based nonprofit saved 733 billion web contents totaling 70 PB to its digital archive the Wayback Machine, and it continues to collect more items.[27] Among the many collections saved in the Wayback Machine is the Web 1.0 community GeoCities, which had 38 million homepages from 1994 to 2009.[28] Ian Milligans research on this early internet community[29] was made possible thanks to the Wayback Machine. While the Wayback Machine is an invaluable resource, 70 PB represent a marginal portion of the 59 zettabytes of total global data production and less than the storage capacity of a single AWS Snowmobile truck. Most of the preserved content, moreover, is currently accessible as an uncatalogued data dump, which has yet to be fully indexed and checked for geographic and linguistic distribution.30 Among the Internet Archives catalogued contents, an advanced search for Korean-language materials returned 66,308 results on 9 May 2022; the equivalent search for English returned 36,683,156 hits.
[Author: Javier Cha; From essay:"Big Data Studies The Humanities in Uncharted Waters "] - The archiving of massive amounts of data places a tremendous burden on non-profit organizations and public institutions. In 2010, the U.S. Library of Congress attempted to archive Twitter. This ambitious project was conducted in collaboration with the social media aggregator Gnip and Twitter headquarters, which provided public tweets from 2006 to 2010.[31] Until December 2012, the Library of Congress archived the content and metadata of 150 billion tweets worth 132 terabytes and continued to receive more data.[32] The Twitter archive project received considerable media attention and inquiries from more than 400 researchers around the world.[33] Unfortunately, the library was unable to develop and launch a viable search engine for it. A report published in January 2013 noted that executing a single search of just the fixed 2006–2010 archive on the Librarys systems could take 24 hours.[34] This anecdote serves as a reminder of the amount of human, financial, and technological resources required to return instant results on Twitter, which the archival version of it was unable to provide. In 2017, the Library of Congresses announced the projects premature termination.[35]
[Author: Javier Cha; From essay:"Big Data Studies The Humanities in Uncharted Waters "] - The preservation of big data over the long term requires public–private partnerships with tech giants, but the viability and specifics of such endeavors are largely unknown. What will occur if Amazon, Microsoft, or Facebook cease to operate their data centers? Will the data stored in infrastructure managed by Google, Alibaba, or Naver remain secure and accessible in the future? Navers promotional slogan created during the launch of Kak reads, We protect what you leave behind. We pass on the records of today to tomorrow.[36] Is this really the case? In contrast to the goal that the data created by Naver users must be handed down to future generations in perpetuity,[37] the archiving of big data is a complex problem constrained by technology and privacy laws. Consider the following from Naver Kaks official data preservation policy:
[Author: Javier Cha; From essay:"Big Data Studies The Humanities in Uncharted Waters "] - Just as not all services are duplicated, not all data is backed up, nor are all backup copies retained indefinitely. In practice, the majority of servers in the data center are used for service, while only a smaller portion are used for backup. As required by applicable laws and ordinances, transaction, payment, and personal information logs are only kept for five years, but the remainder of the services are retained based on the underlying philosophy of the service. The goal for personal data stored in services that require a Naver login, such as personal emails, blogs, cafes, and Ndrive, is to keep them forever. This is consistent with our companys guiding philosophy that individual records constitute the records of the present. Except for specialized services such as [Naver] News Library, the logs of regular newspaper articles, advertisements, and Knowledge iN searches that do not require a login are deleted after one week. In situations where backup is ineffective due to long replication times resulting from a large data capacity or number of files, we rely on data redundancy across data centers as a substitute.[38]
[Author: Javier Cha; From essay:"Big Data Studies The Humanities in Uncharted Waters "] - Navers meteoric rise as one of the largest corporations in South Korea owes much to the transition to personalized services during the Web 2.0 era, in which companies generate profit from subscription fees and advertising revenue. In contrast to the digital edition of the Annals of the Chosŏn Dynasty (Chosŏn wangjo sillok 朝鮮王朝實錄) or the GeoCities web archive, Navers online services, and the infrastructure that enables them, are not designed with the public interest in mind. To be fair, Navers mission statement acknowledges the extent to which the companys vast user data and internal logs represent contemporary South Koreas living records. When data no longer contributes to the companys bottom line, a private enterprise does not have the responsibility to pass on the records of today to tomorrow.
[Author: Javier Cha; From essay:"Big Data Studies The Humanities in Uncharted Waters "] - On numerous occasions, online services have deleted or lost data that users believed would last for a long time. Flickr, a photo-sharing platform, deleted the images of its free users en masse in 2019 due to its parent companys financial troubles.[39] Due to a faulty server migration process, Myspace accidentally lost every single piece of content uploaded to its site
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - More than one face was found in a total of 271,993 frames for all movies (μ=6,800, σ=2,446). In Korean films, faces were found at an average of 7,161 frames per movie (σ=2,593), and in Hollywood movies, faces were found at an average of 6,438 frames per movie (σ=2,298). In the films that passed the Bechdel test, faces were found at an average of 6,824 frames per movie (σ=2,116), and in the films that did not, faces were found at an average of 6,776 frames per movie (σ=2,793).
[Author: Javier Cha; From essay:"Big Data Studies The Humanities in Uncharted Waters "] - before 2016.40 The retention of non-profitable data represents a significant burden even—or especially—for large corporations. In 2020, twenty-one years after its founding, the South Korean social media platform Cyworld, which at its peak had 32 million users and more than 100 billion won in annual revenue from online product sales, was on the verge of shutting down. Upon learning that Cyworlds servers would soon go offline, users turned to the programmer O Kilhos CyBackup, which allows users to create personal archives of their activities and photos shared on the platform.[41] Fortunately, an eleventh-hour capital injection allowed the resumption of services, but any user content that had not been saved ran the risk of being lost forever. As evidenced by these preceding examples, data preservation is costly. In 2019, Russias leading telecommunications firms sought government subsidies for the additional equipment purchases required to comply with the data surveillance provisions of the Yarovaya amendments.[42] It was estimated that the legal obligation to store call and message content for six months and metadata for three years would set each operator back $627 million US dollars, or 40 billion Russian rubles, over a five-year period.
[Author: Javier Cha; From essay:"Big Data Studies The Humanities in Uncharted Waters "] - Beyond just volume, the distributed and dynamic nature of big data complicates the traditional definitions of an archive. As Navers Kak demonstrates, big data can be replicated, marked for deletion, retained briefly, or preserved for an extended period. Approximately 90% of global data production consists of redundant copies.[43] When a South Korean user watches Squid Game on Netflix, the video is not transmitted directly across the Pacific Ocean from Los Gatos, California, via submarine fiber optic cables. Instead, content for the South Korean market is stored and maintained locally. This complex procedure involves cached libraries hosted in AWS S3 (Simple Storage Service) and a content-delivery solution known as Open Connect, which installs Netflix-specific servers directly within internet service provider facilities.
[Author: Javier Cha; From essay:"Big Data Studies The Humanities in Uncharted Waters "] - According to traffic demands, Web 2.0 platforms categorize data as hot, warm, and cold, and manage them differently. At Facebook, for example, the most recent news and profile information that must be processed immediately upon login are placed in hot storage, whereas old information that users rarely access is kept in cold storage. From the early years of service, Facebooks challenge has been the management of binary large objects (BLOBs) such as photos and videos. According to information disclosed in 2013, 82% of Facebook traffic was used to transmit just 8% of photo data.[44] To store and handle the remaining 92% of BLOB, Facebook built a special cold storage facility system called Haystack on a 370,000-m[ 2 ] site in Prineville, Oregon, a small city with a population of only 9000.[45] In 2013, Facebooks cold storage consisted of two-petabyte racks consisting of 500 hard disks in RAID-6 arrangement for added stability and capable of reducing the power consumption of units with low data demand.[46]
[Author: Javier Cha; From essay:"Big Data Studies The Humanities in Uncharted Waters "] - Engineers have devised innovative solutions, focusing primarily on bit rot, to address the challenges of storing digital materials for the very long term. In 2009, Millenniata developed the Millennial Disc (M-Disc), an optical disc capable of retaining data for one thousand years. M-Disc is specially designed to resist rust and maintain data integrity under extreme conditions.[47] In 2011, the United States Department of Defense conducted a stress test that expose archival-grade optical discs to a broad spectrum of light for 24 hours in an environment held at 85 degrees Celsius and 85 percent humidity.[48] Only M-Disc did not experience any data loss.[49] Microsoft Researchs Project Silica, another promising archival medium, uses femtosecond laser to inscribe data on a 2-mm fused silica glass.50 Project Silica is officially rated to last ten thousand years; according to an interview my lab conducted in August 2020 with Ant Rowstron, the Deputy Lab Director, the actual lifespan could exceed one million years.
[Author: Javier Cha; From essay:"Big Data Studies The Humanities in Uncharted Waters "] - The Third V: VarietyThe move from old to new media, from Web 1.0 to 2.0, and from personal computing to the cloud is a complex and non-linear transformation. To a certain extent, the situation we face today bears similarities to what historians have demonstrated about the transition from manuscript to print culture in Europe, East Asia, and other regions. In this vein, Robert Darnton compared the early modern anecdotes written by paragraph men to the fragmentary nature of blogging and the social web.[51] Matthew Kirschenbaums innovative application of digital forensics to English literature and media studies has striking parallels to studies of paratext, marginalia, and hidden layers in premodern artifacts.[52]
[Author: Javier Cha; From essay:"Big Data Studies The Humanities in Uncharted Waters "] - However, I would caution against overstating the parallels and continuities with superficially similar past phenomena, and I reckon that Darnton and Kirschenbaum would maintain the same. The resource-intensive infrastructure that houses contemporary primary sources is unlike anything historians and archivists have encountered. By definition, big data does not reside in a single location; unlike static data sets used in conventional digital humanities research, big data cannot be traced to a specific storage device. Extremely large databases are partitioned into thousands of shards and stored on multiple servers, and data streams self-replicate and migrate in response to user activities, dynamically forming new networks and boundaries.
[Author: Javier Cha; From essay:"Big Data Studies The Humanities in Uncharted Waters "] - Variety adds another layer of complexity to the modern information regime driven by big data. Typically, the term data conjures up an image of a matrix containing text and numbers, which can be thick or thin depending on the number of columns and rows. When the tabular arrangement proves to be cumbersome, a relational database is created by separating and organizing redundant entries into multiple tables. Due to the social and participatory nature of Web 2.0, engineers have proposed and developed new kinds of database management systems. Graph databases, for example, provide native support for managing entities and relationships and store records as node and edge properties. However, the data explosion of the past decade was not necessarily comprised of structured text and numeric data or nodes and edges. As previously mentioned in the discussion of Facebooks cold storage mechanism, the proportion of unstructured BLOBs has surged during the Web 2.0 era and new types of binary objects are being introduced to digital ecosystems.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - Rousseau opens The Social Contract with his famous proclamation that Man was born free, and he is everywhere in chains. To be more in keeping with the human/machine realities of the twenty-first century, his sentiment would better read: ‘Humans are born free, and are immediately electronically monitored. If such a slogan seems unduly despairing, one might consider the new electronic ankle bracelet for infants, trademarked HUGS, which is being marketed to hospitals as
[Author: Javier Cha; From essay:"Big Data Studies The Humanities in Uncharted Waters "] - Visual materials are not novel in the humanities, and, privacy concerns aside, automated location tagging is a welcome feature. However, social media, wearable devices, and virtual reality worlds also collect vast quantities of BLOBs that are unfamiliar to humanities researchers. To facilitate what Shoshana Zuboff has termed surveillance capitalism,[55] for example, Web 2.0 services enable highly intrusive trackers to collect information about user behavior and psychology. Facebook offers its advertising partners the Pixel service, in addition to processing user-uploaded contents for targeted advertising. When a few lines of JavaScript code are inserted into a third-party website, information regarding the users online activities outside of Facebook is gathered and sent to Facebooks server. The profiling is based on not solely the content, but also on patterns of interaction with the interface, such as clicks, taps, cursor movements, and pauses. And Facebook is not alone in tracking its users behavior. On content management systems, such as WordPress, a number of plugins offer similar functionalities. The South Korean startup Four Grit provides a user experience analytics service called Beusable that collects and visualizes online customer behavior data.[56]
[Author: Javier Cha; From essay:"Big Data Studies The Humanities in Uncharted Waters "] - Biometric data are another uncharted territory. Both Android and iOS device users increasingly unlock their devices with facial or fingerprint recognition. The latest versions of wearable devices from Fitbit, Apple, Garmin, Huawei, Samsung, and others monitor the users sleep patterns, physical activities, pulse, and blood oxygen saturation. The performance of these biometric sensors is adequate for some medical professionals to use them as a reference. For instance, a research team at the University of California, San Francisco, proposed a method for combating COVID-19 using health data from wearable devices.[57] Should private information such as browsing habits, psychological profiles, and biometric records be made accessible to researchers? If so, what insights can humanists glean from such data? I am not strongly arguing for or against preserving any sensitive data for future generations. However, I do believe it is necessary to have constructive discussions about creating secure, responsible, and sustainable archives of various types of data that may one day prove useful.
[Author: Javier Cha; From essay:"Big Data Studies The Humanities in Uncharted Waters "] - South Koreans have become accustomed to encountering buzzwords such as the Fourth Industrial Revolution, interdisciplinary convergence, artificial intelligence, the metaverse, smart cities, and the internet of things. While the specific keywords and slogans change according to presidency and fad, they share a common thread. Due to technological advances, it has become possible to collect visual, auditory, tactile, olfactory, and gustatory data, even though the detection and datafication of some sensory information are more refined than others. Beyond the five senses, intelligent electronic devices also collect a vast amount of data that is not perceptible to humans. In the years ahead, the line between virtual and physical worlds will continue to blur. In 2020, Naver created an incredibly detailed three-dimensional map of Seoul in which each pixel represented eight centimeters.[58] Using advanced photogrammetry, the model stitched together 25,463 aerial photographs covering 605.2 km[ 2 ] and 600,000 buildings over a thirty-day period. The goal was to assist Naver in preparing for autonomous driving, which will collect an even greater volume of data about Seoul once it is commercially available. In 2019, approximately 5 million cars traversed the streets of Seoul.[59] If these vehicles are replaced with autonomous ones, Seoul could gain millions of data collectors that sense 140 MB of data per second about their surroundings.60 Should the orthomosaic maps or millisecond-level snapshots of Seoul be regarded as virtual representations of the city? Or as Seoul itself? To address these questions indirectly, I invite readers to consider a surprising example: IKEA catalogs. The Swedish furniture company, whose products are adored by a large number of South Koreans, entices prospective customers with displays of appealing Nordic minimalist design. What few people realize is that more than 75% of product images in the IKEA catalog are computer-generated.[61]
[Author: Javier Cha; From essay:"Big Data Studies The Humanities in Uncharted Waters "] - ConclusionIn 2010s and 2020s, the surge of interest in computational and digital methods in Korean studies has been astounding.[62] In December 2018, Korean Historical Review (Yŏksa hakpo 歷史學報) published six articles surveying the current state of digital historical scholarship worldwide. The annual meeting of the Association for Asian Studies in 2022 included a roundtable discussion on digital humanities in relation to Korean studies. In April 2022, KAIST officially launched the School of Digital Humanities and Computational Social Sciences and announced the ambitious goal of hiring digital specialists to fill half of new faculty positions over the next five years.[63] This special section of Korean Studies, devoted to digital Korean studies, contributes to this ongoing trend.
[Author: Javier Cha; From essay:"Big Data Studies The Humanities in Uncharted Waters "] - The question is what digital humanities means in the Korean context. According to a 2017 survey of South Korean scholars in social science and humanities, ninety-four percent believe that digital humanities methodology is necessary for their field.[64] Only four percent of respondents, however, indicated that they used computational methods (referred to in the survey as statistical methods) such as topic modeling and text mining in their research, even though twenty-three percent have received training in big data analysis method.[65] Sixty-four percent associate digital humanities with the web-based access to primary and secondary sources. Sixteen percent use Google Maps or Google Earth to visualize spatial data, and another sixteen percent use digital technology as a platform for social media engagement. Will training the next generation of humanists with Google Earth, Python, network analysis, and other data-driven or data-assisted methods be sufficient?
[Author: Javier Cha; From essay:"Big Data Studies The Humanities in Uncharted Waters "] - Robert Darntons conclusion in his opinion piece on blogs provides some hints: I dont believe that history teaches lessons, at least not in a direct, easily applied manner, but it does raise questions.[66] The big data turn affords area studies specialists an opportunity to evaluate the continued relevance of what has for long a time been an open and interdisciplinary field. The digital approach to the fuzzy notion of Korea includes explorations of the core digital South Korea, North Koreas distinct information technology sectors discussed in Benoit Bertheliers contribution, and the Korean diaspora. As the concept of data sovereignty gains prominence, experts on Koreas national identity and nationhood will want to investigate its fascinating digital layer, which includes data centers, telecommunication networks, electric power grids, and online participants of the Korean web—the definitions of which will be highly contested. Additionally, comparisons will generate new research topics. The European Union enables the uninterrupted flow and sharing of user data and electricity among its member states, whereas East Asia lacks such an arrangement. What are the implications and consequences of this difference? To create archives of the vast amounts of data currently being generated in South Korea, we will need to foster public engagement and citizen participation while respecting and adhering to the countrys data protection and privacy laws.
[Author: Javier Cha; From essay:"Big Data Studies The Humanities in Uncharted Waters "] - Finally, I would like to propose that the digital humanities be kept distinct from its sister disciplines such as computational social sciences and cultural data science. An abundance of topics at the crossroads of big data and the humanities calls for the insights of Korea experts. I hope that my fellow Koreanists refrain from reflexively equating the big data turn in the humanities with quantitative methodologies or data science, although some degree of familiarity with analytics is crucial for nurturing digital literacy. My recommendation is that we, as humanists, study big data in a manner that builds on our strengths in linguistic proficiency, historical understanding, ethnographic inquiry, and critical thinking, rather than following the paths of scientists and engineers. This should be done while cultivating a harmonious relationship with our technically oriented colleagues and potential collaborators.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - Character encoding has been a major issue until quite recently. The tens to hundreds of thousands of unique characters in CJK languages require a character encoding that is larger than a single byte, an expensive proposition in early applications when computer memory was extremely limited. As such, many tools did not adequately handle text beyond 1-byte ASCII characters. The linguistic complexity involved in CJK computing was a critical part of the impetus for developing the Unicode standard in the late 1980s, which is finally beginning to pervade computing. In combination, these factors historically made digital textual analysis difficult.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - This essay works at the empirical level to isolate a series of technical problems, logical fallacies, and conceptual flaws in an increasingly popular subfield in literary studies variously known as cultural analytics, literary data mining, quantitative formalism, literary text mining, computational textual analysis, computational criticism, algorithmic literary studies, so-cial computing for literary studies, and computational literary studies (the phrase I use here). In a nutshell the problem with computational literary analysis as it stands is that what is robust is obvious (in the empirical sense) and what is not obvious is not robust, a situation not easily overcome given the nature of literary data and the nature of statistical inquiry. There is a fundamental mismatch between the statistical tools that are used and the objects to which they are applied.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - Digital humanities (DH), a field of study which can encompass subjects as diverse as histories of media and early computational practices, the digitization of texts for open access, digital inscription and mediation, and computational linguistics and lexicology, and technical papers on data mining, is not the object of my critique. Rather, I am addressing specifically the project of running computer programs on large (or usually not so large) corpora of literary texts to yield quantitative results which are then mapped, graphed, and tested for statistical significance and used to make arguments about literature or literary history or to devise new tools for studying form, style, content, and context. Another suitable definition of computational literary studies (CLS) is the statistical representation of patterns discovered in text mining fitted to currently existing knowledge about literature, literary history, and textual production to close what Andrew Piper, in his manifesto “There Will Be Numbers,” calls “the evidence gap.” CLS claims that literary critics will no longer make unsupported claims about whole periods of literary history using just a few texts or ignore large swaths of literary production—CLS (says Piper) can show us new things and keep us honest by giving us a way to back up claims with empirical evidence, or by using said evidence to challenge various conventional wisdoms about literary history (such as claims about style, genre, periodization, and so on).
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - Literary scholars have few ways to check CLS work, sometimes owing to problems with access. There are also disciplinary circumstances that have made criticisms against CLS hard to mount, such as the mainstreaming of network literary sociology and the semantic reduction of the meaning of form and formalism to trackable units and a study of the patterns made by trackable things. CLS has also adopted an approach to critical contribution characterized by modesty, supplementarity, or incrementality, reframing setbacks as a need to modify methodology and generate more testing. So, while Piper comments “there have by now been so many polemics written for and against the use of data to study literature, culture, media and history that to offer one more rationale seems perilously unnecessary,” he goes on to say, “What is needed, for sure, is more research—more research into why exactly, why right now, the computational study of culture is necessary.” CLS claims to produce exploratory tools that, even if wrong, are intrinsically valuable because exploration is intrinsically valuable. Misclassifications become objects of interest, imprecisions become theory, outliers turn into aesthetic and philosophical explorations, and all merit more funding and more publications. This kind of strategic incrementalism has made some of the most vocal critics temper their argument—after all, who would not want to appear reasonable, forward-looking, open-minded?
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - There are critiques of CLS in place—notably Timothy Brennan’s “The Digital Humanities Bust” and Danielle Allington, Sarah Brouillette, and David Golumbia’s “Neoliberal Tools (and Archives): A Political History of the Digital Humanities.” Political and philosophical critiques of DH have made significant contributions to our understanding of the institutional and ideological underpinnings of the subfield, but they either take CLS at its word that it does what it claims to do or they overlook CLS’s argumentative arbitrariness. It is in fact true that data mining text labs are given institutional resources disproportionate to what they offer and how little computing power (excepting large-scale digitization efforts) their work actually requires. It only took one laptop to recreate almost all of the works here, and a single smart phone could have supplied the computing power, which begs the question of why we need “labs” or the exorbitant funding that CLS has garnered. Nevertheless, because of the way it approaches textual analysis, CLS can use similar data-mining methods to back up very different positions and has already made a case for itself as something that offers new ways to catch inequalities and “read” corpora left out by the canon for reasons of access or judgments of aesthetics and value.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - This essay does not argue that “numbers are neoliberal, unethical, inevitably assert objectivity, aim to eliminate all close reading from literary study, fail to represent time, and lead to loss of ‘cultural authority’” or that “numbers inevitably (flatten time/reduce reading to visualization/exclude subjectivity/fill in the blank).” Nor does it make any claims about “the hegemony of data and data science,” or the instability of the objectivity of data itself. Others have already done this thoughtfully and eloquently. That human and literary phenomena are irreducible to numbers and that good interpretation and style in literary criticism are as objective as the sciences are personal convictions that do not enter into this critique. We can use nonideological reasoning to see that CLS as it currently exists has very little explanatory power that is not negated by its operations.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - To deal with secondary classification problems, CLS often use topological data analysis (TDA) tools, network analysis tools, and topic modeling tools like latent dirichlet allocation (LDA) and latent semantic analysis (LSA). This represents one of the most questionable uses of statistical tools in CLS. Topic modeling, which treats each text as a distribution over topics and each topic as a distribution over words (thus still dealing with texts as an unordered collection of words), is used to discover topics unsupervised in large bodies of texts. It is extremely sensitive to parametricization, prone to overfitting, and is fairly unstable as an “aboutness finder for sophisticated texts because you need only tweak small details to discover completely different topics. There is no real way to measure the accuracy of the topics found since LDA’s recall depends on having true classes of topics arrived at through human decision making. Its utility is most observable in circumstances where recall and precision really do not matter very much, as with content-based recommendation systems (such as Facebook advertising products to its users).
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - I discuss a handful of CLS arguments (chosen for their prominent placement, for their representativeness, and for the willingness of authors to share data and scripts or at least parts of them). Each of the papers I look at suffers from conceptual fallacies from a literary, historical, or culturalcritical perspective, but here I am taking them on their own terms completely—their sampling (too often the only point of contention from outsiders), their testing, their codes, and their truth claims. Drawing on basic statistical principles, I discuss these examples alongside text mining’s known uses and applications and situations in which textual quantitative analysis and simplified reconfigurations of information would be useful. I will explain true applications in simple ways that do not do justice to their myriad complexities (mostly due to my own limitations) but that I believe still capture these applications’ rightful functions and limitations. Critics in digital humanities have provided accompanying explanations for their methods but generally only to initiate more people into the subfield by making the entry bar seem low or so that their audience can follow along. I believe in reintroducing these methodologies in intuitive and efficient ways so that we can begin to understand the logics that drive them and better evaluate CLS’s utility, discerning instances where tools and methods are used suboptimally or for no foreseeable reason. This essay is not an attempt to address all of the errors and oversights that occur in CLS work. Oversights in implementation; lack of robustness, precision, and recall; and less than ideal measurements are endemic to data-mining work. For these reasons, although I go over technical issues, the case against CLS won’t rest on technicality, nor can one person take on that much work hunting down incomplete data work and debugging broken scripts. A clear explanation of the computational work that CLS actually does is enough to constitute a provocation to the rest of us to appreciate the circumstances under which such errors would be permissible and which not. The nature of my critique is very simple: the papers I study divide into no-result papers—those that haven’t statistically shown us anything—and papers that do produce results but that are wrong. I discuss what it is about the nature of the data and the statistical tools that leads to such outcomes.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - CLS papers are more or less all organized the same way, detecting patterns based in word count (or 1-, 2-, n-grams, a 1-gram defined as anything separated by two spaces) to make one of six arguments: (1) the aboutness of something; (2) how much or how little of something there is in a larger body, (3) the amount of influence that something has on something else; (4) the ability of something to be classified; (5) if genre is consistent or mixed; and (6) how something has changed or stayed the same. It will become clear that all six are basically the same argument, with aboutness, influence, relatedness, connectedness, generic coherence, and change over time all represented by the same things, which are basic measurements and statistical representations of overlapping vocabulary—and not even close to all of the vocabulary, as much culling must take place to have any kind of statistical workability at all. Data sets with high dimensionality are decompressed using various forms of scalar reduction (typically through word vectorization) whose results are plotted in charts, graphs, and maps using statistical software. Finally, this model (a newly derived instrument for measuring literary patterns or distinguishing between them) is tested using in- and subsample testing. The argument itself is usually the description of the results of data mining. Quantitative analysis, in the strictest sense of that concept, is usually absent in this work. Hypothesis testing with the use of statistical tools to try to show causation (or at least idiosyncratic correlation) and the explanation of said causation/correlation through fundamental literary theoretical principles are usually absent as well.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - No matter how fancy the statistical transformations, CLS papers make arguments based on the number of times x word or gram appears. CLS’s processing and visualization of data are not interpretations and readings in their own right. To believe that is to mistake basic data work that may or may not lead up to a good interpretation and the interpretive choices that must be made in any data work (or have no data work at all) for literary interpretation itself. In CLS data work there are decisions made about which words or punctuations to count and decisions made about how to represent those counts. That is all. The highest number of consecutive words (1-grams) that CLS work has looked at is three (trigrams). Mark Algee-Hewitt looks at the probabilities of bigrams (the likelihood that one word will be followed by another specific word) to calculate corpus “entropy,” but this is just another way of saying “two words that appear together” (I will return to this paper later). Jean-Baptiste Michel and others’ “Quantitative Analysis of Culture Using Millions of Digitized Books” tracks 5-grams (five 1-grams in a row), but their payoff is for lexicography and for tracking large-scale grammatical shifts, not for literary history or criticism. Roberto Franzosi claims to find “narrative events using trigram tagging. Though it is already outdated in the field, his is the only case I know of natural-language-process tagging that tries to get beyond basic word frequencies. But there, “narrative events” are just 3-gram length subject+verb+object sequences, and accounting for “time” and “space” amounts to little more than known time markers and geographical locations (extremely hard from a coding perspective, reductive from a literary perspective).
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - Despite claims to the contrary, CLS is not able to look at anything like plot beyond three words. And it’s not just a matter of letting a nascent field mature (corpus analysis for literature has been around for half a century) but a matter of their objects being too few and too complex. Suggestions for quantifying literature from experimental early structuralism, such as Claude Lévi-Strauss’s attempt to define the structure of myths using the formula fx(a) : fy(b) = x(b) : f(a – 1)(y), are not operationalizable at all, as such patterns are too difficult and abstract to code and define far too few texts for machine learning to successfully code even one such appearance in a handful of texts. Therefore all the things that appear in CLS—network analysis, digital mapping, linear and nonlinear regressions, topic modeling, topology, entropy—are just fancier ways of talking about word frequency changes. Breaking down CLS mistakes will clarify why, despite the fact that different semantic and syntactical tagging methods have existed since the 1970s, CLS tends to stick to counting words and is, in an even more limited sense, forced to find many of its significances by tweaking stop words.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - The CLS papers I studied sort into two categories. The first are papers that present a statistical no-result finding as a finding; the second are papers that draw conclusions from its findings that are wrong.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - Topological insight and topologically structured visualization tools for word frequency arguments: these are not the same things. Piper describes his topological renderings as “autochthonic” and “protocologic,” a Latourian network of “quasi-objects,” a Deleuzian ““relation of the non-relation,” a Badiouian questioning of ““the aura of the limit,” “a different kind of thinking about the beyond,” a Judd-Morrissey-inspired “radical act of interleaving, something that allows us to “think more about language in agential terms (what it does), a Foucauldian ““field of regularity,” and something that “move[s] past the ontology of discourse but also “allows for a far more nuanced sense of discursive being.” These inspired comparisons are hard to reconcile with what his uses of topology actually amount to, and they get in the way of seeing what topological maps actually do. His project with Mark Algee-Hewitt, “The Werther Effect,” for example, is a series of topological visualizations that capture the influence of Johann Wolfgang von Goethes The Sorrows of Young Werther (1774) on his later works (and other English and German works after Goethe). “Influence means tracking ninety-one representative words in The Sorrows of Young Werther and their frequencies in x other works, a measurement deemed important because Goethe supposedly wrote differently after this renunciation of Werther and because Werther is known to have influenced later works, but we dont know how or to what degree. Piper and AlgeeHewitt take the Euclidean distance of word frequency measurements to measure the similarity of lexicon across works and then, in order to visualize their matrix, try and find the best way to collapse the information in the matrix into a picture because this distance matrix is large and the information is not easily grasped. They chose a Voronoi diagram, a very useful and intuitive form of data visualization that allows you to see geometrically how distant a work is from every other work to scale. Topology functions here as an optimal way to visualize a matrix of word frequency differences; it is not a representation of how we read, visually, no matter how it is plied metaphorically. And generating a Voronoi diagram aside (whose application in these types of data situations is not the authors original contribution), what these distance measurements—which now can be seen all at once—represent is the way that ninety-one words show up (regardless of location, order, context, syntax, speaker, voice, tone, proximity to one another) in the rest of Goethes oeuvre. At the end of the day, the repetitions of those ninety-one words indicate the influence of Werther on other texts. In another forum we as literary critics have to decide how much were invested measuring the precise indices of influence and if the fact that a set of words in A also show up frequently in B means that A influenced B; here, it is enough to see that it is this is the same argument that weve seen in every paper: overlapping most-frequently-used vocabulary denotes influence, and when A isnt exactly B, word for word, B has definitionally influenced A by degrees.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - More than one object was found in a total of 708,217 frames for all movies (μ=17,705, σ=3,057). In Korean films, objects were found at an average of 18,026 frames per movie (σ=3,004), and in Hollywood movies, objects were found at an average of 17,384 frames per movie (σ=3,153). In the films that passed the Bechdel test, objects were found at an average of 17,721 frames per movie (σ=3,124), and in the films that did not, objects were found at an average of 17,690 frames per movie (σ=3,070).
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - I start with a paper that presents a no-result as a finding from using a measuring device too weak to capture a known difference, a paper that will also help us see the problem of measuring so-called homology, repetitiveness, or self-similarity through word frequency. Ted Underwood’s “The Life Cycle of Genres,” which tries to see if genres change over time, models the genre of detective fiction based only on word homogeneity and tests the accuracy of the model by seeing if it can distinguish B (post-1941 detective fiction) from C (a random motley of works) the same way it can distinguish A (pre-1941 detective fiction) from C. Underwood compares A to B and claims that detective fiction is much more coherent over one hundred fifty years than literary scholars have claimed. Underwood wants to argue that genres do not change every generation and that they do not only consolidate in the twentieth century—as others, namely Franco Moretti, have argued—but have in fact been more or less consistent from the 1820s to the present. The problem here is that his model does nothing for his objective. Underwood should train his model on pre-1941 detective fiction (A) as compared to pre-1941 random stew and post-1941 detective fiction (B) as compared to post-1941 random stew, instead of one random stew for both, to rule out the possibility that the difference between A and B is not broadly descriptive of a larger trend (since all literature might be changed after 1941). All that Underwood has shown in using word frequency homogeneity to differentiate detective fiction from random fiction is that the difference between pre- and post-1941 detective fiction is not as significant as its difference from random fiction. This does not mean that the same method can capture the difference between different types of detective fiction. After all, statistics automatically assumes that 95 percent of the time there is no difference and that only 5 percent of the time there is a difference. That is what it means to look for p-value less than 0.05. Think of it this way: if everyone can agree that something is changing—even Underwood concedes that genres evolve—but you have devised one way that concludes that it does not, it does not necessarily mean that you have found something. It just means your instrument of measurement might be too weak—your method might have too little power—to capture this kind of change.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - The use of data mining to present naturally occurring statistical significances as results is a problem that can be seen in Matthew Jockers and Gabi Kirilloff’s paper, “Understanding Gender and Character Agency in the Nineteenth-Century Novel,” which claims that certain verbs are highly correlated with gendered pronouns (he/him, she/her) in the dataset. (Gender is a preferred analytic in CLS, most likely because it’s one of the few things that can offer a clean second-order classification—into male/female.) The authors use a parser to find accurate pronoun-verb pairs in their data and then build a classifier to predict the correct gender for given verb, claiming an 81 percent accuracy rate (30 percent improvement over pure chance). They find fifty verbs most correlated with males and fifty verbs most correlated with females, with ten of these each that “the machine found most useful for differentiating between male and female pronouns”. Putting aside the errors endemic to dependency parsing and OCR recognition, and the lack of accounting for association by negation (when a person doesn’t do something), some of their results are obvious; some are not. As the authors themselves admit, this can make for a backward understanding of gender (it is binary; women weep, men take), but I’ll leave this for others to discuss.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - First, there were always going to be top five, top ten, top fifty, top one hundred statistically significant pronoun-verb pairs. That is a function of finding all pronoun-verb pairs, ranking them by correlation, and cutting off the ranking where one chooses. In good statistical work, the burden to show difference within naturally occurring differences (“diff in diff”) is extremely high. Let us say that you are measuring the overlap of features between two sets of data using a standard 5 percent confidence level; out of n possible shared features, 0.05n will automatically be significant. Datamine something and you will always find significant associations. Their claim that “there is a strong correlation between character gender and verbs in the nineteenth century” is true by fiat, as by their definition of correlation one could claim that about any group of literature in any century (“UG”). The paper does not perform a bootstrap, which means the literary-historical suggestions that follow this genre classification do not stand. But let’s say they did. Just to find the top ten verbs for each gender, a far simpler method—a simple regression of pronoun-verb associations on an almost-identical corpus—regressing male percentage on female percentage of each verb—produces commensurate results. A good-faith replication using a commensurate parser delivers different results. So what is the value added here? Their model has, in-sample, a 22 percent error rate when the true pronoun class was female and 16 percent when the true class was male. The authors account for high error rates by suggesting that gendering of verbs might be less stable for females—but you cannot turn predictive weakness into an argument unless you can prove that your predictive ambiguity is not due to lack of power in your measurement. To extend their contribution by recasting it as a measurement of the gender rigidity of novelistic genres, Jockers and Kiriloff add that their model had 58 percent, 63 percent, and 67 percent accuracy rates for classifying the correct gender for their six bildungsroman novels, four silver-fork novels, and three historical novels; 80 percent for thirty-three gothic novels; and 100 percent accuracy rates for six industrial novels and two Newgate novels. There is no statistical rigor in this, never mind that we’re talking about a very small number of books. Whatever sample size you start with you can always cut it in such a way so that you get something for which there’s a 100 percent accuracy rate. By pure chance, there will be variation in accuracy rates; that does not mean there’s systematic variation or a true pattern between genre and the model’s gender-predicting powers.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - Because of the way the data is treated, CLS can make macrohistorical claims that are statistically uninformative. Consider this graph, “A Network of Three Thousand Novels,” which depicts similarity based on vocabulary and which Matthew Jockers argues reveals things about over three thousand novels over time. According to Jockers, this network map, in which “books are being pulled together (and pushed apart) based on the similarity of their computed stylistic and thematic distances from each other is extraordinary” because it obeys chronology (clustering based on time written) and “chronological alignment reveals that thematic and stylistic change does occur over time. The themes that writers employ and the high-frequency function words they use to build the frameworks of their themes are nearly, but not always, tethered in time.” In other words, Jockers is arguing that because there is a separation between light dots and dark dots, because they’re not all mixed together, and because the network visualization is itself agnostic on date of publication, he has proven that older works are more similar to one another and newer works are more similar to one another: that they reflect their times. Sampling errors notwithstanding, this network graph represents a tiny percentage of the data. What you learn from this 3 percent is tautological. Jockers calculates similarity between books (Euclidean distance) based on 578 features—five hundred are topics extracted from LDA topic modeling (more below), and the rest are frequently used words and punctuation. LDA topics and frequently used words tend to cluster in time, so these features already have time correlation built in. If you take a similar dataset (texts over one hundred years) and regress absolute Euclidean distances (based on similarly determined features) on absolute distances in time, you will see super significant positive correlation. This is neither unique nor insightful; you’ve mechanically guaranteed the capture of a generic time trend— what is discussed over time plus language evolution.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - Computational literary criticism is prone to fallacious overclaims or misinterpretations of statistical results because it often places itself in a position of making claims based purely on word frequencies without regard to position, syntax, context, and semantics. Word frequencies and the measurement of their differences over time or between works are asked to do an enormous amount of work, standing in for vastly different things.
[Author: John Unsworth; From essay:"What is Humanities Computing and What is Not "] - This view of text says that text is an ordered hierarchy of content objects, which means, for example, that content objects nest-paragraphs occur within chapters, chapters in volumes, and so on. It also means that a language that captures ordered hierarchical relationships and allows content to be carried within its expression of those relationships can capture what matters about text. Hence SGML. But, this view of text misses certain textual ontologies-metaphor, for example-because they are not hierarchical, or more accurately, they violate hierarchy. This is not a sign of a flaw in SGML or in the OHCO thesis, but a sign that both are true knowledge representations—they bring certain things into focus and blur others, allowing us to pay particular attention to particular aspects of whats out there.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - Pipers essay “Novel Devotions: Conversional Reading, Computational Modeling, and the Modern Novel” offers a good example of this problem, matching a word-frequency difference to structural difference in an argument that is historically and hermeneutically over-specific. “Novel Devotions makes two claims: first, that the last three chapters of Augustines Confessions are significantly different than the first ten and significantly different from each other. In other words, things start to feel different after the tenth chapter and continue to feel increasingly more different as the book goes on. Piper attributes this to the experience of conversion that happens in book 10—an experience that he argues makes a real difference in terms of vocabulary output. This, he says, is what makes Confessions and books influenced by Confessions act on the reader in measurable ways, what makes them “devotional.” Second, Piper claims that English and German novels share the same structure as Augustines Confessions; the second half of the text is radically different from the first half of the novel and increasingly different within its parts. The amount of change that happens in word frequency (for each word) between first and second half, and between the chunks within the second half, is measured through cross-half and in-half scores, respectively, which are simply Euclidean measurements of the square root of the sum of the square of differences between terms frequencies in text 1 versus text 2 (and up to text n). Piper derives an in-half and a cross-half score to capture this word frequency change and uses multidimensional scaling (MDS) to visualize the result, which essentially reduces a twenty-dimensional set of relationships down to two so it can be visualized.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - There are multiple things wrong with this study. Anyone who has read Confessions knows that the last three chapters differ from the first ten because Augustine turns to discussions of Genesis after spending ten chapters on autobiography, and so of course different words will start to show up. This has nothing intrinsically to do with conversion. His in-half and crosshalf scores do not necessarily indicate this pattern of change and should not be taken as benchmarks for novels having such a “devotional” structure. More technically: Piper did not stem the Latin text (pare words down to verb and noun roots) even though he stemmed the English and German texts. He was counting conjugated verbs and declined nouns as different words in Latin but as the same words in English. Once the Latin text was stemmed and the distance matrices properly scaled for variables, we get scores that are different from his, and his results no longer obtain. I have recreated Pipers plot with a stemmed text, properly scaled. In my rendering, books 1 and 2 are not clustered with the other books in the first half, nor is book 13 as distant from the first half.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - It is easy to see the problem with structuralist arguments that are at bottom tied to word frequency: word frequency differences show up in all kinds of texts and situations that do not match what you want them to represent. Piper cannot stop the second half of texts being quantitatively different from the first half where he does not want them. To define word frequency changes as change itself (and by a conceptual slide, conversion) is both tautological and risky. There is no reason to mystify the process; as more concepts are introduced into a text, more words come with them. An MDS for Exodus, for example, demonstrates this. The Exodus plot shows a spread similar to what Piper finds in Augustines Confessions, with the first half closer together and the second half not only farther than the first but with data points farther from each other. Unless Piper is prepared to argue that the Hebrew Bible also follows Augustines confessional structure (as he defines it), he has to admit that such a pattern is not limited to Confessions. Of course, this may be confusing necessary with sufficient conditions—the fact that Christian conversion narratives exhibit this phenomenon does not mean that nonconversion narratives do not. An effective argument of this sort about religious texts would require further evidence and commentary. In the meantime, a Chinese translation of Augustines Confessions, for example, produces an MDS (using Pipers methods) that does not look at all like his graph for the Latin Confessions. Do conversion experiences not carry through translation?
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - Reducing similarities and differences to word frequency differences forces one to produce findings when there might be an underlying explanation that both obviates your claims and obviates the need for your modeling. A ready example of this problem can be found in Paul Vierthaler’s work on the difference between different types of Chinese writing. The author claims that two genres of Chinese writing, historical apocrypha (野史) and fiction or narratives (小说), are not as similar as literary historians have assumed. He looks at three very small corpora (14, 126, and 524 texts) and compares their word frequencies (1-gram字 frequencies) using a hierarchical clustering algorithm (HCA) that makes dendrograms based on “similarity scores” and PCA. Because he split each book into tenthousand-character chunks and then took the one thousand most-used Chinese characters in that chunk (determined through simple term frequency), each dot on his PCA represents a ten-thousand-character section, not a whole book. And in comparing the one thousand most common 字 in ten thousand 字 segments, the author has already made data points that will look extremely similar and allowed the PCA to look more robust than it is. In other words, the author has already homogenized the data points and needlessly increased their number. Therefore, the number of data points on the PCAs seem to make a strong case, but in fact the data points from each kind of genre are very close to one another simply because of the way the author prepared his data. More pressingly, Vierthaler uses computational methods to prove to us that Chinese historical apocrypha is in fact closer to official history because of similar formal language use. This assertion is based on common tokens that clearly divide between classical language and vernacular, but he describes a bridge between official history and fiction based on common tokens having to do with theme and plot. This relationship is already known to readers of classical Chinese literature. Historical apocrypha and official histories of the Ming and Qing period were predominantly written by a similar set of literati or functionaries. Apocrypha are differentiated by content, not formal language use, while fiction and narratives are primarily written in the vernacular (or an admixture that leans toward vernacular) and contain themes shared with historical apocrypha. As Vierthaler writes, if historical apocrypha and fiction or narrative have been traditionally grouped together, it is because both tend to be collected from hearsay. It is redundant to challenge this classification based on criteria that the classification was never confused about in the first place.
[Author: John Unsworth; From essay:"What is Humanities Computing and What is Not "] - From a purely mechanistic view, reasoning in machines (and somewhat more debatably, in people) is a computational process. Simply put, to use a representation we must compute with it. As a result, questions about computational efficiency are inevitably central to the notion of representation.
[Author: John Unsworth; From essay:"What is Humanities Computing and What is Not "] - Different modes of representation have different efficiencies: Traditional semantic nets facilitate bi-directional propagation by the simple expedient of providing an appropriate set of links, while rule-based systems facilitate plausible inferences by supplying indices from goals to rules whose conclusion matches (for backward chaining) and from facts to rules whose premise matches (forward chaining).
[Author: John Unsworth; From essay:"What is Humanities Computing and What is Not "] - Efficiency stands opposed in some way to the fullness of expression, and that Either end of this spectrum seems problematic: we ignore computational considerations at our peril, but we can also be overly concerned with them, producing representations that are fast but inadequate for real use.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - Hoyt Long and Richard Jean Sos “Literary Pattern Recognition: Modernism between Close Reading and Machine Learning sets out to measure formal influence from East to West by building a Naïve-Bayes classifier to find haikus that are not self-identified as haikus—in part offering a classificatory tool and in part tracking English poetry not expressly designated as haikus. They train their classifier on four hundred haikus (in translation and as adaptations) and one thousand nine hundred nonhaiku short poems and then run it on an unclassified combined set. The Bayes rule is a widely used rule that with every new observation updates the probability distribution; the system is naïve” because the features are supposed to be independent of one another. You do not tell the algorithm the exact criteria by which to make its classificatory decisions; you tell it what to pay attention to and it learns the decision rule based on some basic features, changing the probability distribution every time a new thing comes in and so getting smarter and better at classifying the next thing. Technically, Long and So use Naïve-Bayes (N-B) to refine their classifier, treating each poem in the test sample as a new observation. But instead of letting the N-B figure out the cutoff syllable count, the authors hard code that decision rule into their script (whether a poem is under nineteen syllables if its a translation, or under thirty syllables if its an adaptation). The only other thing that their classifier uses to classify haikus is a simple likelihood of appearance score for individual words (the word sky becomes 5.7 times more likely to show up in a nonhaiku, for example). They end up with a model that is overfitted and for which features are learned very quickly. I ran their N-B classifier on English translations of Chinese couplets that are similarly long and filled with similar imagery as well as two hundred English translations of short Chinese and nonhaiku Japanese poems from Wakan Rōei Shū (Collection of Japanese and Chinese Poems to Sing) from the tenth century (predating the consolidation of the haiku form by almost seven hundred years). Their classifier heavily misclassified the Chinese and prehaiku poems because of the primitiveness of its criteria; in fact, as the reduction threshold increases (removing features that dont occur often enough to prevent overfitting) the accuracy declines even more. It turns out, in other words, if you define haikus as poems under thirty syllables with words that show up a lot in haikus, you will in effect collapse the diversity of many types of East Asian poems into the haiku form.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - The power of a statistical test comes from having meaning and setting up a null/alternative hypothesis thats informative and that explains something with respect to fundamental insights. It is not enough to find a pattern in the data that rejects a poorly chosen null such as “most frequently used words dont change” / “most frequently used words do change. It may be an extremely rigorous test, but it tests the wrong problem. All it accomplishes is the data mining of results. Researchers in the sciences and social sciences are extremely wary of such results. Statistical tools are designed to do certain things and solve specific problems; they have a specific utility and should not be used just for the sake of dressing up word counts. This is not at all to argue that literary analysis must have utility— in fact I believe otherwise—but if we are employing tools whose express purpose is functional rather than metaphorical, then we must use them in accordance with their true functions.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - The reasons for quantifying narrative text, running algorithms based on word frequencies, and topographically visualizing textual data are not very transferable to our discipline. Typical applications of textual data mining involve a trade-off: speed for accuracy, coverage for nuance. Such methods are efficient for industries, sectors, and disciplines that are dealing with so much textual data at such fast speeds that they cannot possibly (nor would want to) read it all or where one wants to extract from a large data set a relatively simple piece of information that is either actionable or that can be quickly labelled and classified along simple features. Whatever ones feelings are towards deterministic algorithmic approaches to worldly phenomena, text mining is ethically neutral. In legal discovery, large volumes of largely identical legal documents (such as contracts) can be machine read for errant phrasing or diction (including misapplied terms of art) amidst standard terminology and formally repetitive syntactic patterns to quickly identify problematic or intentionally misleading clauses. The information that is extracted is not supposed to be semantically complicated. Investors use text mining to determine if a news report or press release by a company has a negative or positive tone so that a trading decision can be made very quickly. Every second, news is released by companies—annual reports, quarterly reports, stock earnings announcements, and so on—that no one wants to read; nor could anyone possibly have the time to read it all. Simple measures of terms that drive certain measurable changes are what one can and would wish to glean from these modes of inquiry; speed is the most important consideration because the corresponding decision often has to happen within seconds if not nanoseconds. We could theoretically verify each report individually—text mining knows that human reading can catch more nuances, exceptions, ambiguities, and qualifications—but why would we? Your email server uses a machine-learning classifier trained on a mix of all the previous emails marked as spam and nonspam by the user to decide if an incoming document is spam. An email might get sorted into the wrong folder or flagged as important for no good reason, but the classifier works instantaneously and is accurate enough that you wouldnt prefer to do it yourself.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - To look for homologies in literature, CLS must eliminate much of highdimensional data and determine the top drivers of statistically significant variation. This always involves a significant loss of information; the question is whether that loss of information matters. One popular way to decompress high-dimensional data is factorization, a way of parsimoniously explaining lots of variance in numerical data. Take for example tools like principal component analysis (PCA) or multidimensional scaling (MDS) that were used in Piper and Vierthaler’s paper and that are used widely in CLS to capture morphology and represent quantitative findings. PCA performs an orthogonal transformation of data, reducing the total number of aspects in multivariate data without knowing exactly what commonalities and differences to look for in the first place. PCA will sort multivariate data into principle components and provide quantitative descriptions of differences between data entities based on their loading on shared vectors. If you have three hundred thousand metric profiles of multivariate data (for example, patients exhibiting one or more illnesses and their chromosome maps) and wish to know what they might have in common—but not everything they have might in common, just three or four things, and again without knowing what those could even be—PCA will help you sort the data by these principal components. It does not tell you what to call these categories or what themes they share, descriptively, but tells you what characteristics (different chromosome maps) might be driving a clustering (patients who all have heart disease). In textual analysis, this means that the greatest difference between one article, piece of literature, or book and another would be their loading on a few of the shared vectors—information that is given quantitatively, not descriptively. You wouldn’t want to go all the way to all of the vectors because that would simply be a reproduction of the entire data set (where you stop is a professional choice); therefore, it is necessarily a significant reduction of information. It is one thing to statistically identify the shared drivers of a medical illness and another to say that the difference between Immanuel Kant’s third critique and G. W. F. Hegel’s Lectures on Aesthetics can be captured in two or two numbers derived from their overlap on two or three vocabulary lists. There are many different ways of extracting factors and loads of new techniques for odd data sets, but these are atheoretical approaches, meaning, strictly, that you can’t use them with the hope that they will work magic for you in producing interpretations that are intentional, that have meaning and insight defined with respect to the given field.
[Author: John Unsworth; From essay:"What is Humanities Computing and What is Not "] - The reason SGML is the way it is, the reason it demands that elements nest properly within a specified hierarchy, is to enable efficient computation. In fact, the SGML in its pure form was too flexible to really allow this, which is why certain features (like overlapping or concurrent hierarchies) were never implemented in software. XML simplifies out of SGML some of the expressive possibilities that made SGML difficult to write software for, and viola, suddenly we have lots more software for XML than we ever had for SGML.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - Deleuze and Guattari outline how ‘rhizomes are plants which grow in surface extensions through interconnected vertical root systems. The rhizome is contrasted with arborescent systems which are those plants with a deep root structure and which grow along branchings from the trunk. The rhizome metaphor accentuates two attributes of the surveillant assemblage: its phenomenal growth through expanding uses, and its leveling effect on hierarchies.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - Consider this plot by the Stanford Literary Lab (and originally produced by Michael Witmore and Jonathan Hope) that argues that perhaps “narrative genres can be reduced to two basic variables” and that perhaps something besides genre drives the differences among William Shakespeare’s comedies, tragedies, histories, and late plays. No one has ever said, though, that consistent word frequency is what distinguishes Shakespeare’s comedies from tragedies, tragedies from histories, and so on—and no one would ever say that because such distinctions cannot be captured with word frequencies. Put another way, the only way that this PCA diagram would be interesting is if word frequencies were recognized as what actually drove the genre differences. That is, if the first and second principal components precisely identified the tragedy and comedy factors. Again, this would be highly unlikely but statistically sound. Hypothetically, researchers could have complied all of the works from each category into one vector so that there are only four data points in the PCA, one for each genre. Then they could go in and look at the vector of word frequencies to see which words are driving the differences. That would actually teach us something, even if it would still be reductive as literary criticism. (In fact, it is good practice to ask users of CLS to show their vectors—it demystifies much of the process and often reveals conceptual weaknesses.) The authors of “Quantitative Formalism” did try to do that, generating multiple PCAs only to find repeatedly that PCA cannot capture generic differences. They then looked at the Docuscope scatterplot to see which component loadings (words) were mostly driving the differences and found mostly stop words; they then presented this phenomenon as a literary-critical argument: “Do you want to write a story where each and every room may be full of surprises? Then locative prepositions, articles and verbs in the past tense are bound to follow”. Whether we find this reasoning sound or not, it is not a revelation but rather an attempt to make something meaningful out of a stop word problem.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - The hitch of using textual pattern mining for forensic stylometry is that even if you apply pattern recognition techniques that reduce noise and nonlinear interactions between data, the stylistic differences that can be captured for literature tend to be driven by stop words—if, but, and, the, of.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - Why is that the case? Mark Algee-Hewitt and Piper tell us that “stop words are usually semantically poor and yet stylistically rich. . . . The best means so far for determining authorship attribution and classifying texts as categorically different.” In reality, stylistic differences boiling down to stop words is not surprising at all. To locate a statistical difference of occurrence means having enough things to compare in the first place. If the word cake only occurs once in one text and four times in another, there’s no way to really compare them, statistically. By the numbers, stop words are the words that texts have most in common with one another, which is why their differentiated patterns of use will yield the readiest statistical differences and why they have to be removed for text mining.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - The stop word dilemma—keep them and they produce the only statistical significance you have; remove them and you have no real results—can be seen in Long and So’s “Turbulent Flow: A Computational Model of World Literature, a paper that tries to come up with a predictive algorithm for the literary phenomenon stream of consciousness (SOC). The paper argues that SOC travels amongst countries and that this “diffusion can be tracked. Long and So compare three hundred twelve-hundredcharacter SOC passages based on what other scholars have said were SOC passages and repeated for sixty realist novels (since realist novels are often seen as not having or using SOC), to build a classifier testing for thirteen linguistic traits unique to SOC (token-type ratio, onomatopoeia, neologisms, noun-ending sentences). They argue that they can predict a piece of SOC literature with 95 percent accuracy (97 percent accuracy for Japanese literature). Of the thirteen features tested for, authors identify tokentype ratio (the number of words divided by number of types of words in a sentence) to be the single most important factor in predicting SOC; this is a concept that critics had already claimed in the 1970s but “never with such precision or on such a scale.” When Long and Sos classifier is less accurate in dealing with SOC in Japanese literature, the authors call this “turbulent flow—when formal influence doesnt carry all the way through.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - However, their strongest predictor for if something is SOC or realism—token-type ratio—is too sensitive to the nonstandard stop words that the authors chose themselves. If you do not remove the stop words, then the statistical significance flips the other way (realist texts have higher tokentype ratios). Removing the stop words flips the equation because the ratio between distinct stop words to total words in passage for SOC is statistically higher. And this is because SOC stop words are similar, while realist stop words are more varied if we are using the stop words that the authors chose themselves (the list of which, even with proper names removed, was three hundred words longer than the standard stop word list). Using this list, realist texts will have significantly more stop words than SOC texts. This explains why the removal of stop words changes the token-type ratio enough to make SOCs token-type ratio statistically higher than realisms. Thus, the only thing the authors needed to do to differentiate SOC texts from realist was to tabulate stop word frequencies—their strongest indicator above any of the four they isolated; their strongest explanatory feature, in other words, is an unnecessary measurement. I reran their code using a standard stop word list, and once we only remove the standard stop words, the difference between token-type ratios for realist texts and SOC texts loses statistical significance.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - In other sectors and applications, texts with stop words removed can further be categorized—into economic terms, political terms, female consumer, for example. Another level of simple and accurate-enough classification has to occur so that categories can be compared rather than an individual word’s frequencies—this is what allows for the statistical analysis of words. When CLS tries to do this for literature, using various methods to reduce large corpora of words to sensible groupings, it realizes that after the necessary dimensionality reduction is performed—uncommon words taken out, stop words removed, groups of words vectorized to become single points in space—it’s left with only a small portion of what it was originally purporting to study, and these are corralled into groupings so general as to preclude meaningful interpretations.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - In certain ways, East Asian digital humanities lags behind digital practice in other parts of the world. Some of this relates to the linguistic nature of the material and the historically Eurocentric focus of software development. For a long time, the majority of scholars developing digital humanities tools studied the West, and so the tools they made were not easy to adapt to Asian language works. Optical character recognition software, for example, was not initially optimized for Chinese-Japanese-Korean (CJK) character sets and could rarely adapt to textual forms and layouts common in Asian xylographic and lithographic printed materials.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - Without meaningful applications, topic modeling will look like a wordcloud generator for literary criticism. Jockers and David Mimno use LDA to extract themes from the Literary Lab Corpus and find that women writers are twice as likely to focus on female fashion (one word cloud for female fashion) and male writers are much more likely to focus on the topic of enemies (another word cloud of war-related terms). In contrast, Underwood argues that topic modeling is only useful for literary studies if it can find “meaningfully ambiguous” clusters instead of “intuitive” ones—“ones that are clearly about war, or seafaring, or trade. But that would mean banking on instances where topic modeling isnt working as it should. The truth is that “meaningfully ambiguous clusters where unexpected words gather either turn out to have rather mundane explanations or to simply replicate the order of appearance of actual words in the works. In that same article Jockers and Mimno try to extend the uses of topic modeling to find writers who hide political information in religious topics—words clustering around “Convents and Abbeys”—only to find that two texts in the anonymous corpus drive most of the content of the topic of abbeys and convents. This is simply because the phenomenon of displacement—when talking about cats is actually talking about one’s mother—is not what topic modeling, which is based on probabilistic models of likely coappearance, is designed for. Underwood discovers topic 22 in womens poetry from 1815–1835, but because it does not intuitively cohere—reading like a poem assembled from the most frequently occurring words in a poetry corpus—interpreting it is nonsensical, which is why interpretation is missing from his presentation of the possibilities of topic modeling.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - Topic modeling has also been used in a new genre of academic surveillance in which academics catch each other out by interrogating the things they have been covering. Ethical considerations aside, there is the question of whether such models can even effectively determine areas of study. Underwood and Andrew Goldstones survey, “The Quiet Transformations of Literary Studies: What Thirteen Thousand Scholars Could Tell Us” looks for what scholars have been “talking about in about thirteen thousand scholarly articles from 1889 to 2016 and finds that many topics are becoming more prevalent. For instance, they find that increase in topic 80— ten words that cluster around the word power”—is a “trend specific to literary study that peaks in the 1980s. If the authors wanted to nonarbitrarily study the change in topics covered in journal articles over time, they could have saved time by just looking at journal abstracts. Treating all the articles published in a year as a single sample (and not separating their data set into training and test sets) and running an LDA without fitting a prior to a posterior means that the algorithm will tend to form topics based on consecutive years in the corpus. They want to argue that some topics are increasing while others are decreasing, but conducting the topic modeling this way will mechanically produce topics that increase and decrease over time.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - As a literature of scholarship grows, there will be more literature. The topics (a cooccurrence of words) that are found are driven by more recent scholarship because theres more of it; therefore, back mining the earlier scholarship for the topic will obviously show the topic to have increased over time. The authors find it counterintuitive that topic 80 has risen over time while the individual words have not (using Google n-gram) but if topic 80 exists over the whole period but is primarily driven by the latter period of scholarship, then definitionally the words in topic 80 do occur but do not co-move in the earlier period. In the presentation of their results the authors eventually perform year-topic scaling for the topics they found, but that doesnt change the fact that they still found those topics in the first place still using the full sample. Ideally, a study either chooses a list of reasonable words to associate with a topic beforehand and looks just for those words in the full sample as a trend, or the study down weighs more recent articles to avoid the clustering effect. If using the full sample to find topics, as Underwood and Goldstone have done, an author cannot make arguments about time-series variation.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - When topic modeling is used in a reasonably correct fashion, one can identify interesting and unexpected topics only when the other topics that are found (say, forty-seven out of fifty topics) pass the smell test. This is not the case for this study; basic robustness tests also fail. To see how article length might influence topics found, I performed two robustness tests. In a partial double test (which randomly doubles the lengths of 30 percent of documents, all other parameters staying the same, and which should not affect the LDA because its based on a bag-of-words model), all the topics changed. When I randomly removed just 1 percent of the original sample, all the topics changed. This paper also does not pass the test of reproducibility; if the method were effective, someone with comparable training should be able to use the same parameters to get basically the same results without swimming through tailored codes and buried filters. I took their dataset and used a Python LDA script (scaled for each documents length) to find one hundred fifty topics of ten words each, exactly as they did. The topics I generated were not at all the same. This does not mean that one of us did not do due diligence, but it does mean that topic modeling is like a kaleidoscope that turns out something entirely different with the slightest tweaking.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - Quantitative visualization is intended to reduce complex data outputs to its essential characteristics. CLS has no ability to capture literatures complexity. Mark Algee-Hewitt wants to go beyond word-frequency counts to measure for literary entropy, or level of redundancy in a piece of work, which would seem to be a complexity measure. His contribution to the multiauthor Stanford Lab article “Canon/Archive. Large-scale Dynamics in the Literary Field is to argue that noncanonical texts are less entropic (more redundant) than canonical texts, using 260 titles from the Chadwyck-Healey corpus as the canon corpus and 949 titles from the same time period as the noncanon corpus. He measures number and probabilities of consecutive pairs of words therein on the reasoning that the more entropic a piece of literature is, the less redundant it is and the more information it contains. Entropy measure sounds sophisticated (and seems analogous to literary complexity), but what it does, actually, is measure the number of distinct pair of words and their distribution in the total number of bigram pairs. It is not a mysterious property but directly tied to variety of words (two thousand, twenty thousand, or two million distinct words make a huge difference) and skewedness of words (whether a couple of words are the ones that are always appearing or if each of the words appears just once). Entropy levels are highest in a situation wherein bigram pairs are diverse but no particular bigrams dominate others, leading to more information in a text which, as Warren Weaver says, “must not be confused with meaning.” Even if we agree that more mathematical entropy somehow means more literary novelty or less literary redundancy, as Algee-Hewitt would have it, his calculations are still wrong. Using an Archive corpus of 356 books (thus closer in size to their Chadwyck-Healy corpus of 260 books), I recalculated entropy for both (scaled entropy scores = 0.796391 and 0.793993, respectively) and found no statistical difference between the two after robustness tests. Algee-Hewitts larger entropy for the ChadwyckHealy corpus is driven by the large size of his archive corpus (263 versus 949), which creates a magnitude of difference between the number of distinct bigrams for Chadwyck and for the archive, which causes the archive entropy score to go down. His finding, the basis for a significant portion of “Canon/Archive,” is just a scaling oversight.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - These days there is no shortage of fancy statistical tools to aid in machine learning. Computation is relatively easy and cheap; tools exist that allow you to run every pathway, make a decision at every step along the way, and provide many ways to tweak the modeling to identify patterns. In the end, statistics is about identifying a higher-order structure in quantifiable data; if the structure is not there (or is ontologically different) statistical tools cannot magically produce one. For instance, while text mining often uses topology, it is meaningless if it does not retain topologys function, which is a meaningful reduction of complexity to make quicker, more intuitive, noncontingent calculations. In a math problem foundational to graph theory, The Seven Bridges of Konigsberg, one has to determine if a path exists that crosses only once each of the bridges in a particular configuration of rivers and land masses. You could do it manually, but this becomes too arduous if we are dealing with larger areas with many more intersections, bridges, and odd-shaped land masses, or indeed a whole municipality. Leonhard Euler proposed a scalar reduction in complexity, reformulating each land mass as a node (the blue dots on the third image), and each crossing to another land mass as an edge, producing a graph that only records nodes and edges. This graph is not a formal rearrangement of the map but a fundamental transformation of the information in the map. It no longer matters how the river runs, how big or what shape the islands are, and how they lay with respect to one another (these are local). You can take any area and count the number of land masses and their exit points. If zero or two of the nodes have odd number of edges, then such a walk is possible. If not, then not (so in the original problem the walk is not possible).
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - A reduction in complexity is necessary in this case because you dont want to have to exhaust all the combinations of routes in order to know the answer for urban planning. Topology, which grew from this problem, relies on a reduction of complexity from actual layout to schematic representation, preserving the relationship between two points under their continuous deformations. Topological maps such as a subway map transform complicated and contingent geospatial information into essential nodes (the map doesnt have to reflect the myriad topographical details in an actual map or even resemble it in any way by being to scale—the only things that matter are the points of interchange). These examples illustrate the criterion by which to judge the usefulness of a topological transformation.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - CLS understands the topological terms global and local in ways that are no longer imbued with graphic theoretical meaning—network diagramming and topology function interchangeably in its practices—and tends to reconfigure information for the purpose of visualizing lower-dimensional homologies (similarities based not on the whole texts but on very finite aspects of it). A corpus is mapped on vectors, each one representing a document by encapsulating it using a measure of relative weight of each term. This vector space model generates a set of data points in a non-Euclidean coordinate system that CLS then presents as topological information. Topological modeling is used, for example, to calculate sociability and social interactions in literary landscapes, using an extremely metaphorical interpretation of the topological edge. What the literary sociologist Alan Liu means by latent social network” or “core circuit is simply a visualization of connections using a functionally reductive definition of “ties.”
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - CLS network analysis can easily become recommender-system literary sociology, in which consumer and discursive associations are visualized without regard to the tone, context, emphasis, rhetoric, and so on—exactly in the way recommender systems function. In these, word frequency overlaps constitute spatial connectivity and a network means a simple visualization of a very small number of these connections. Such diagrams are often rendered with off-the-shelf-social-computing tools and platforms created for other purposes. But these off-the-shelf tools, such as the Facebook Friend Wheel, are useful if you wish to promote socialization or enterprising opportunities by mapping out your networks, which are dynamic and complex not in reference to the nature of the connections in question but in reference to their order of magnitude and the amount of topological information embedded therein. Network maps are used to calculate the centrality of nodes based on directional vectors; so Google, for example, knows how to turn up most relevant searches because it calculates the number of nodes (sites) in its network that link to another site and in so doing calculates the relative centrality of a site. A network map cannot be replaced by other forms of data representation. It becomes complex because of size and connections (which increase at a rate of 2n): seating arrangements for a wedding with five hundred guests—some of whom cannot sit with some others and who all have a descending list of proximity preferences—become that much more complicated if the total number of guests increases to five million. Capturing this kind of complication—or capturing network complexity by studying a network whose degree distribution of nodes to links is neither arbitrary nor regular but obeys some other mathematical law—is not the same as saying that network diagrams of who is talking to whom in Shakespeare can capture the complexity of connections in Shakespeare or character discourse. We are dealing with fundamentally different definitions of complication and complexity.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - Network diagramming for low-volume data is not a meaningless activity if it can help us see what we otherwise cannot see, but this payoff is often missing from such visualization. Ed Finn creates a network map of Junot Diaz’s The Wondrous Life of Oscar Wao’s Amazon page with “book reviews and website recommendations . . . as links and book “titles as nodes, which is intended to visualize consumer and discursive associations. Using scripts that recursively gather recommendations, Finn maps out the first ten Customers Who Bought This Also Bought links and the first ten recommendations for each of these links over several months (from December 2010 to March 2011) in order to produce a network map. In this map, though, where is the network analysis? Where are the centrality scores? What are the assortativity measures? The statistical inference?
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - Properly defining nodes has no operationalizable end here, in contrast to, say, the NSA keeping track of terrorist webs on social media by investigating up to three nodes of connection. For Finn, each mention of another author (regardless of the nature of the mention), whether in Amazons recommender system or in these sundry reviews, is proof that Oscar Wao serves as a “literary gateway from this genre of ethnic literature to a distinct canon of mainstream prize-winners” or proof of “the process of literary reverse colonization, of deliberately contaminating the language of one discourse with the icons of another. These are captivating ideas, but Finn performs no network analysis (he has made Oscar Wao the de facto center of his graphs) because these are simply eleven items and their connections to one another. This is not a network map but only a very, very small piece of a network map—one that could easily be represented in a table. There might be an order of magnitude between the first recommended book and the second one, but Amazon does not reveal that information to the consumer. Finn is weighing the referrals equally because he only has partial access to Amazons item-to-item collective filtering algorithm (full access would mean that Finn will simply duplicate what is already on Amazon).
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - CLS has not kept pace with corpus linguistics in accounting for things like coreferentiality or sentence processing that care about words as embedded in linguistic structures (local discourse). CLS does use natural language processing (NLP) to tag parts of speech and phonemes, seemingly to move beyond summary statistics to grab words in a more semantically meaningful way, but these efforts are halfhearted, and there are reasons for this beyond the fact that NLP has only recently taken off. Speech tagging is extremely inaccurate for literary texts. Lexical, syntactic, and grammatical ambiguities make it difficult for an algorithm to know whether a word is a participle or a gerund, if an adjective is a noun, or if entire phrases are functioning as a single part of speech. NLP is said to have a 93–95 percent accuracy level, but that depends on what youre using it for and the degree of classification you require (thus, formal evaluation is very difficult). Having 95 percent accuracy for building an online chatbot or for basic translations is very different than 95 percent accuracy at picking out all the parts in a piece of literature. NLP software for narrative parts of speech tagging is also not very user friendly because it requires that one manually annotate a training set.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - You quickly run into a data scarcity and data complexity problem with literature. How many distinct sets of literature are out there—that you would be able and willing to manually annotate—that would be large enough to allow you to accurately run NLP on the rest of the set? And what do you do after youve tagged a text? Suppose someday all literary things (including homonymy, figuration, polysemy, irony, transference) can be accurately tagged—a pretty big supposition. The researcher would still be left with a list of tags and their frequencies, which would have to be heavily reduced in dimensionality to have any extractable statistical meaning. In this case semantics or basic plot are still being ignored (unless we are willing to accept their premise that words statistically co-occurring with others can effectively represent semantics, topicality, or plot). For other fields of study, named entity recognition tasks can be used to provide that second layer of classification, sorting tagged words into predefined categories such as names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, and so on. But widening out in this way to get workable categories only make sense when you have truly large data sets and when you desire to quickly extract some usable information. Tagging errors and imprecision in NLP do not sufficiently degrade the extraction of information in many other contexts, but they do for literature.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - Even when applied to the kinds of text most suited for it—NLP lends itself particularly well to reportage of data that are abundant but similar—Franzosi spent thirty years manually training his tagger on newspaper articles (“10–15 minutes per one-page document for an experienced coder working with fairly complex coding schemes”) to confirm simplistic versions of basic historical facts. Martin Paul Eve also tries to get beyond stop word frequencies by turning to NLP to prove that David Mitchells Cloud Atlas is a mishmash of genres. It is an exemplary case because Eve only uses the statistical tools that are needed, explains the relative simplicity of his measurements and gives credit to these measurements as things already available in coding packages instead of presenting them as though he devised them from scratch. Instead of making homology calculations after removing stop words, Eve saw that a much simpler classifier could be had by the frequency measurements of common stop words (the, a, I, to, of, and in) that can accurately classify all the sections of Cloud Atlas save one (with twenty common stop words being able to classify all the sections), and he takes the Manhattan distance of z-scores and the clustered dendrograms of the five thousand most frequently used words (or two words) to predict the likelihood that different sections in Cloud Atlas were written by the same author. Eve then turns to NLP to show that the Luisa Rey section of Cloud Atlas has statistically significant occurrences of the tagged trigram NNP+NNP+VBZ (proper noun singular + proper noun singular + third-person singular present verb). But this turns out to have a completely prosaic explanation. All that Eve has done is to prove that Mitchells sections are as distinct from one another as they are from some other author using stop words. NLP does not offer any additional insight. To actually explain the tropic substrate of distinctive trigram frequencies, he still had to go in and find the instances of adverb+adjective+noun and distinguish a hopelessly uneven gunfight from a “mostly empty wine glass. Because of UK copyright laws, Eve typed out the novel manually. This is a lot of work to learn with certainty that one chapter pairs more full names of characters with actions than another.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - For an even clearer instance, the problem with So and Longs haiku classifier is not its accuracy rate or even its parametricization but its functionality. Of course, the classifier does not have to be 100 percent accurate—one cannot reject it simply by finding examples of misclassification. If for Long and So (1) “translations and adaptation,” (2) things calling themselves haikus, and (3) things other people have classified as haikus are indeed the same kinds of things—haikus (whatever their differences)—then whatever the Naïve-Bayes classifier classifies as an English haiku is, by their very definition, an English haiku, as they dont have a rigorous definition to begin with. But are we talking about enough ambiguous cases (or even total number of very short poems) to justify this error? Are we facing a situation where millions of short poems are published but we cannot possibly find the time to read them? The authors themselves do not have a good way to find and scrape all the short poems that exist in the world without knowing in advance where to look, so they have not saved us any time on that account. Couldnt someone trained in poetry just find, read, and classify them?
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - Supporters of CLS argue that it does not matter that it takes a long time to do something we already know, as the innovation is in a computer being able to do basic reading tasks at all (an argument for artificial intelligence). But it does matter because computation is being used here as an investigative tool that shows you where to look or what to casually opine on, and CLS authors simply pick up influence, change over time, no change over time, generic coherence, or generic difference arguments along the way because theyve defined these to be identical with the only kind of data processing they can do in order to use these particular tools and have statistical inference at all. This is not artificial intelligence but humans working in summary statistics.
[Author: John Unsworth; From essay:"What is Humanities Computing and What is Not "] - Whenever one encounters a new situation (or makes a substantial change in ones viewpoint), he selects from memory a structure called a frame; a remembered framework to be adapted to fit reality by changing details as necessary. A frame ... [represents] a stereotyped situation, like being in a certain kind of living room, or going to a childs birthday party.
[Author: John Unsworth; From essay:"What is Humanities Computing and What is Not "] - Reasoning and representation are intertwined-how we think by way of representations.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - CLS has also excused its methodological and argumentative flaws by appealing to a trade-off: Who can possibly read all the literary texts that are out there? Machine reading is not perfect, but its better than nothing, and it can show us latent patterns that no one reader can see. Literary critics, especially those studying contemporary literature, tend to look to DH to help them account for literary objects that they feel are exponentially increasing. They naturally assume that computational methods can help them tackle this scale in a faster, more comprehensive, and nonarbitrary manner. As all the above examples prove, that is a misperception. Looking for, obtaining copyrights to, scraping, and drastically paring down “the great unread into statistically manageable bundles, and testing the power of the model with alternative scenarios, takes nearly as much, if not far more, time and arbitrariness (and with much higher chance of meaninglessness and error) than actually reading them. CLSs methodology and premises are similar to those used in professional sectors (if more primitive), but they are missing economic or mathematical justification for their drastic reduction of literary, literary-historical, and linguistic complexity. In these other sectors where we are truly dealing with large data sets, the purposeful reduction of features like nuance, lexical variance, and grammatical complexity is desirable (for that industrys standards and goals). In literary studies, there is no rationale for such reductionism; in fact, the discipline is about reducing reductionism. Even macroanalytical results cannot themselves be the products of reductionist thinking.
[Author: Nan Z Da; From essay:"The Computational Case against Computational Literary Studies "] - With regard to the overabundance argument, it is important to remember that many of the key examples come from corpora or texts that have already been read. CLS is really not dealing with nearly as much data or complexity (of the order that justifies the use of the tools they use) as authors like to think. Basic math also helps here: one million words roughly equals ten novels; one and a half billion represents about fifteen thousand novels, which at one novel a month will only take one thousand people one year to read. At the end of the day, the overabundance claim is not a legitimate argument in and of itself. In the sciences and social sciences there is also an inestimable volume of texts, data sets, and scenarios that havent been covered. There are a lot of things about which we dont know and lots of questions we havent answered. That does not mean that any pattern that can be found in that unknown data, any answer to any previously unasked question, or any question, is automatically worthy of attention. The basic criteria should always be to not confuse what happens mechanically with insight, to not needlessly use statistical tools for far simpler operations, to present inferences that are both statistically sound and argumentatively meaningful, and to make sure that functional operations would not be far faster and more accurate if someone just read the texts. It may be the case that computational textual analysis has a threshold of optimal utility, and literature—in particular, reading literature well—is that cut-off point.
[Author: John Unsworth; From essay:"What is Humanities Computing and What is Not "] - Ill give the short answer to the question what is humanities computing up front: it is foreshadowed by my two epigraphs. Humanities computing is a practice of representation, a form of modeling or, as Wallace Stevens has it, mimicry. It is also a way of reasoning and a set of ontological commitments, and its representational practice is shaped by the need for efficient computation on the one hand, and for human communication on the other. Well come back to these ideas, but before we do, lets stop for a moment to consider why one would ask a question such as What is humanities computing?
[Author: John Unsworth; From essay:"What is Humanities Computing and What is Not "] - First, I think the question arises because it is important to distinguish a tool from the various uses that can be made of it, if for no other reason than to evaluate the effectiveness of the tool for different purposes: a hammer is very good nail-driver, not such a good screw-driver, a fairly effective weapon, and a lousy musical instrument. Because the computer is—much more than the hammer—a general-purpose machine (in fact, a general-purpose modeling machine) it tends to blur distinctions among the different activities it enables. Are we word-processing or doing email? Are we doing research or shopping? Are we entertaining ourselves or working? Its all data: isnt it all just data processing? Sure it is, and no it isnt. The goals, rhetoric, consequences, benefits, of the various things we do with computers are not the same, in spite of the hegemony of Windows and the Web. All our activities may all look the same, and they may all take place in the same interface, the same “discourse universe of icons, menus, and behaviors, but theyre not all equally valuable, they dont all work on the same assumptions-theyre not, in fact, interchangeable. To put a more narrowly academic focus on all this, I would hazard a guess that everyone in this audience uses a word-processor and email as basic tools of the profession (anyone care to admit that a secretary still types your manuscripts?). But even though many of you are in the humanities, you do not all do humanities computing-nor should you, for heavens sake-any more than you should all be medievalists, or modernists, or linguists.
[Author: John Unsworth; From essay:"What is Humanities Computing and What is Not "] - So, one of the many things you can do with computers is something that I would call humanities computing, in which the computer is used as tool for modeling humanities data and our understanding of it, and that activity is entirely distinct from using the computer when it models the typewriter, or the telephone, or the phonograph, or any of the many other things it can be.
[Author: John Unsworth; From essay:"What is Humanities Computing and What is Not "] - A Phone Book is a very simple example, but it works. Its fundamental conception of intelligent inference assumes that you know someones name and will want to know their phone number, but more deeply, it assumes that name, address, and phone number are (for the purposes of the phone-book world-view) a complete person-record. It would support an analysis of name frequency, or knowing someones number and finding out their address, or other things, because all those relationships are in there, but it makes the name-to-number inference much easier than the others.
[Author: John Unsworth; From essay:"What is Humanities Computing and What is Not "] - A Relational Database. Think about how a relational database establishes the grounds of rational inference by establishing fields in records in tables, and think about how it sanctions any sort of question having to do with any combination of the elements in its tables, but actually recommends certain kinds of queries by establishing relationships between elements of different tables.
[Author: John Unsworth; From essay:"What is Humanities Computing and What is Not "] - The second reason one might ask the question “what is humanities computing” is in order to distinguish between exemplars of that humanities computing activity and charlatans or pretenders to it. Charlatans—a strong word, I know—are people who present as humanities computing some body of work that is not: it may be computer-based (for example, it may be published on the Web), and it may present very engaging content, but if it doesnt have a way to be wrong, if one cant say whether it does or doesnt work, whether it is or isnt internally consistent and logically coherent, then its something other than humanities computing. The problem with charlatanism is that it undersells the market by providing a quick-and-dirty simulacrum of something that, done right, is expensive, time-consuming, and difficult. Put another way, charlatans trade intellectual self-consistency and internal logical coherence (in what probably ought to be a massive and complicated act of representation) for surface effects, immediate production, and canned conclusions. When one does this, one is competing unfairly—in the market of deans, provosts, funding agencies, and private donors-with the project that is more thorough in its approach to the problem of representation and more thoughtful about what one might call economies of temporal scale—the long-term costs and benefits of painful planning vs. rush to production. Now, the bad news is that all humanities computing projects today are involved in some degree of charlatanism, even the best of them. But degree matters, and one way in which that degree can be measured is by the interactivity offered to users who wish to frame their own research questions. If there is none offered, and no interactivity, then the project is pure charlatanism. If it offers some (say, keyword searching), then it is a bit more real. If it offers structured searching, a bit more real. If it offers combinatorial queries, more real. If it allows you to change parameters and values in order to produce new models, a bit more real. If it lets you introduce new algorithms for calculating the outcomes of changed parameters and values, a bit more real, and so on. This evaluative scale is not, as it seems to be, based on functional characteristics: it uses those functional characteristics as an index to the infrastructure—intellectual and technical—that is required to support certain kinds of functionality. On this scale of relative charlatanism, no perfectly exemplary project exists, as far as I know. But you see the principle implied by this scale—the more room a resource offers for the exercise of independent imagination and curiosity, the more substantially well thought-out, well-designed, and well-produced a resource it must be. This is true regardless of the content as true for E-commerce as it is for whatever we will call E-ducation, rEsearch, and the like.
[Author: John Unsworth; From essay:"What is Humanities Computing and What is Not "] - Finally, and most candidly, one asks the question “what is humanities computing” in order to justify, on the basis of the distinctions I have just drawn, new and continuing investments of personal, professional, institutional, and cultural resources. This investment could take the form of a new grant-funded project, or a new undergraduate or graduate degree, or a new Center or Institute. At this level, the activity that is humanities computing competes with other intellectual pursuits-history, literary study, religious study, etc. for the hearts, minds, and purses of the university, even though, in practice, the particulars of humanities computing may well—and will likely-call upon and fall into traditional disciplinary areas of expertise. So, as Willard McCarty has often noted, we have a problem distinguishing between computing in the service of a research agenda framed by the traditional parameters of the humanities, or, on the other hand, the much rarer, more peculiar case where the humanities research agenda itself is framed and formed by what we can do with computers.
[Author: John Unsworth; From essay:"What is Humanities Computing and What is Not "] - So, given that humanities computing isnt general-purpose academic computing-isnt word-processing, email, web-browsing-what is it, and how do you know when youre doing it, or when you might need to learn how to do it?
[Author: John Unsworth; From essay:"What is Humanities Computing and What is Not "] - At the opening of this discussion, I said that “Humanities computing is a practice of representation, a form of modeling or .. mimicry. It is ... a way of reasoning and a set of ontological commitments, and its representational practice is shaped by the need for efficient computation on the one hand, and for human communication on the other. As I unpack these terms, one at a time, I will expand on each point, and then look at some examples from the realm of humanities computing.
[Author: John Unsworth; From essay:"What is Humanities Computing and What is Not "] - The first question about any surrogate is its intended identity: what is it a surrogate for? There must be some form of correspondence specified between the surrogate and its intended referent in the world; the correspondence is the semantics for the representation. The second question is fidelity: how close is the surrogate to the real thing? What attributes of the original does it capture and make explicit, and which does it omit? Perfect fidelity is in general impossible, both in practice and in principle. It is impossible in principle because any thing other than the thing itself is necessarily different from the thing itself (in location if nothing else). Put the other way around, the only completely accurate representation of an object is the object itself. All other representations are inaccurate; they inevitably contain simplifying assumptions and possibly artifacts.
[Author: John Unsworth; From essay:"What is Humanities Computing and What is Not "] - A catalogue record is obviously not the thing it refers to: it is, nonetheless, a certain kind of surrogate, and it captures and makes explicit certain attributes of the original object-title, author, publication date, number of pages, topical reference. It obviously omits others—the full text of the book, for example. Now, other types of surrogates would capture those features (a full-text transcription, for example) but would leave out still other aspects (illustrations, cover art, binding). You can go on pushing that as far as you like, or until you come up with a surrogate that is only distinguished from the original by not occupying the same space, but the point is all of these surrogates along the way are “inaccurate; they inevitably contain simplifying assumptions and possibly artifacts”—meaning new features introduced by the process of creating the representation. Humanities computing, as a practice of knowledge representation, grapples with this realization that its representations are surrogates in a very self-conscious way, more self-conscious, I would say, than we generally are in the humanities when we represent the objects of our attention in essays, books, and lectures.
[Author: John Unsworth; From essay:"What is Humanities Computing and What is Not "] - Any knowledge representation is a “fragmentary theory of intelligent reasoning,” and any knowledge representation begins with some insight indicating how people reason intelligently, or ... some belief about what it means to reason intelligently at all. ... A representations theory of intelligent reasoning is often implicit, but can be made more evident by examining its three components: (i) the representations fundamental conception of intelligent inference; (ii) the set of inferences the representation sanctions; and (iii) the set of inferences it recommends. Where the sanctioned inferences indicate what can be inferred at all, the recommended inferences are concerned with what should be inferred. (Guidance is needed because the set of sanctioned inferences is typically far too large to be used indiscriminantly.) Where the ontology we examined earlier tells us how to see, the recommended inferences suggest how to reason. These components can also be seen as the representations answers to three corresponding fundamental questions: (i) What does it mean to reason intelligently? (ii) What can we infer from what we know? and (iii) What ought we to infer from what we know? Answers to these questions are at the heart of a representations spirit and mindset; knowing its position on these issues tells us a great deal about it.
[Author: John Unsworth; From essay:"What is Humanities Computing and What is Not "] - Knowledge representations are also the means by which we express things about the world, the medium of expression and communication in which we tell the machine (and perhaps one another) about the world. . . . a medium of expression and communication for use by us. That in turn presents two important sets of questions. One set is familiar: How well does the representation function as a medium of expression? How general is it? How precise? Does it provide expressive adequacy? etc. An important question less often discussed is, How well does it function as a medium of communication? That is, how easy is it for us to talk or think in that language? What kinds of things are easily said in the language and what kinds of things are so difficult as to be pragmatically impossible? Note that the questions here are of the form how easy is it? rather than can we? This is a language we must use, so things that are possible in principle are useful but insufficient; the real question is one of pragmatic utility. If the representation makes things possible but not easy, then as real users we may never know whether we have misunderstood the representation and just do not know how to use it, or it truly cannot express some things we would like to say. A representation is the language in which we communicate, hence we must be able to speak it without heroic effort.
[Author: John Unsworth; From essay:"What is Humanities Computing and What is Not "] - Well, SGML in general has raised in some the fear that humanists would never be able to speak it without heroic effort. To be fair, good software removes some of the complexity-for example, by offering you only the elements that can legally be used in a particular point in the hierarchy. But still, you have to be able to grasp the purpose and intent of the DTD in order to use it sensibly.
[Author: John Unsworth; From essay:"What is Humanities Computing and What is Not "] - Another feature of knowledge representations is that they are expressed in formal languages-languages “composed of primitive symbols acted on by certain rules of formation (statements concerning the symbols, functions, and sentences allowable in the system) and developed by inference from a set of axioms. The system thus consists of any number of formulas built up through finite combinations of the primitive symbols--combinations that are formed from the axioms in accordance with the stated rules. For our purposes, what is important about this fact is that it puts humanities computing, or rather the computing humanist, in the position of having to do two things that mostly we dont do—provide unambiguous expressions of our ideas, and provide them according to stated rules. In short, once we begin to express our understanding of, say, a literary text in a language such as SGML (standard generalized markup language), a formal grammar that requires us to state the rules according to which we will deploy that grammar in a text or texts, then we find that our ideas are subject to verification—for internal consistency, and especially for consistency with the rules we have stated. This kind of error-checking (parsing) is new for humanities scholars (with the exception of philosophers of certain kinds).
[Author: John Unsworth; From essay:"What is Humanities Computing and What is Not "] - What does this do to us, what does it mean?
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - There have been significant developments in the application of the digital humanities (DH) to East Asian studies in recent years. The field is rapidly growing, attracting interest from funding bodies, cultural heritage institutions, and scholars from undergraduate students to full professors. Digital East Asian studies has gone through a few distinct phases, marked by ever expanding areas of inquiry and increased technical sophistication. The East Asian studies community has been developing digital infrastructure, datasets, and tools since long before the term digital humanities was coined. As computers have become more widespread, new resources and platforms are making information increasingly available to anyone with a computer and an internet connection. At the same time, new communities and organizations are emerging as scholars begin to capitalize on the impressive infrastructure that has built up over the last 30 years.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - The meaning of the term digital humanities has historically generated a lot of debate, and scholars have spent significant time establishing, and tearing down, its boundaries. In East Asian studies, digital humanities is most often used as a catch-all term to refer to the creation and use of diverse digital datasets, archives, and computational tools. This is in line with broader views of DH, as Matthew Kirschenbaum notes of Wikipedias entry on the topic: [It] is a field of study, research, teaching, and invention concerned with the intersection of computing and the disciplines of the humanities. DH encompasses a wide variety of concerns, which Lauren Klein and Matthew Gold laid out in clear terms:
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - Along with the digital archives, quantitative analyses, and tool-building projects that once characterized the field, DH now encompasses a wide range of methods and practices: visualizations of large image sets, 3D modeling of historical artifacts, born digital dissertations, hashtag activism and the analysis thereof, alternate reality games, mobile makerspaces, and more.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - Digital humanities is not simply concerned with computational analysis: Scholars also study the influence technology has on politics, culture, and society in general. In short, digital humanities has been increasingly acknowledged as a two-way street on which scholars use the digital to study core topics within the humanities, while also centering the digital world itself as a subject of humanistic inquiry.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - The older parts of this digital humanities framework remain centered in digital East Asian studies: digital archives, tools, and quantitative analysis. At the same time, East Asian studies is generally following the arc of digital studies elsewhere: Qualitative studies with significant digital components are becoming more important, and the politics of the digital is never far from center stage. Technology shapes both scholarship and society in East Asia. The complex nature of the digital is perhaps most clearly illustrated by censorship, which the digital both enables and hampers. This is a persistent issue and has significant influence on the kinds of research that are possible, particularly in Chinese studies. This is most immediately a concern for those who use born-digital materials to study contemporary hot button topics ranging from pro-democracy protests in Hong Kong, internment camps in Xinjiang, to the status of Taiwans political independence from the PRC. Yet these issues exist even when working on historical, and seemingly apolitical, topics. Politics shapes everything from access to archival materials to the maps deemed acceptable in mainland publications. The digital also makes studying the very act of censorship possible, as scholars attempt to understand how nation states and corporations leverage technology to reshape society.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - In this article, I will focus mainly on the archives, tools, and research that currently (as of mid-2020) form the vanguard of Chinese, Japanese, and Korean digital historical and literary research. I will cover some of this history of this development and highlight some important resources for those hoping to familiarize themselves with major trends and players in East Asian DH. I will begin with an overview of several digital resources and tools, continue with a short discussion of trends in current research, and finish by covering emerging communities and training centered on digital Asian studies. This will not be a comprehensive account of the field, as there are too many excellent projects to mention, and although I cover worldwide East Asian digital humanities, my emphasis skews a bit toward North American and European work. This article is designed to provide a starting point for those looking for a broad overview of current activities.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - It is not easy to distill the work East Asian studies scholars are doing with digital methods into a short article, as the work occurs across diverse sets of fields with their own conventions and concerns. Some scholars and institutions, often focusing on increasing access to reference materials, build and refine large-scale infrastructural projects. Others push to use available digital materials to feed geographic, network, textual, and image analyses, sometimes in ways the developers did not intend. East Asian DH exists in a sort of symbiosis: data creation and curation, infrastructural development, and research all mutually feed each other. This is all aided by advancements in optical character recognition, data mining, and natural language processing that are breaking through the technological gates that previously hampered digitization and analysis in the CJK space.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - East Asian digital humanities can draw its roots back to the 1970s with the creation of digital datasets that have grown to include textual, bibliographic, biographical, geographic, and visual information. Projects have created many resources ranging from collections of unstructured information, usually in the form of textual corpora, to highly structured databases.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - Most early efforts focused on texts. The Scripta Sinica, founded at the Academia Sinicas Institute of History and Philology in 1984, aims to digitize all documents essential to research in traditional Sinology. As of 2020, it now holds nearly 1300 titles containing over 718 million characters, and it remains an example par excellence of conscientious corpus building.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - Some commercial organizations have also focused on digitization. An important example is the digital Wenyuange Complete Library of the Four Treasuries Wenyuange Siku Quanshu 文淵閣四庫全書, which the Digital Heritage publishing company began digitizing in 1996. This Qing dynasty text collection contains over 3000 works meant to represent the most important pieces of Chinese writing and was published on CD-ROM in 1998.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - Although access to much material is limited to paying subscribers, East Asian scholars and institutions have long been at the forefront of the open-access movement. As a result, many projects were produced under this ethos. Two prominent examples are the Chinese Buddhist Electronic Text Association (CBETA) and the SAT Daizōkyō Text Database. The former was established in 1997 at National Taiwan University and the latter in 1998 at the University of Tokyo. Both are open-access databases that cover the entire Taisho Buddhist canon (in addition to other materials).
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - The Aozora Bunko 青空文庫 is an important open-access collection of Japanese texts, hosting nearly sixteen thousand works. The Centre for Open Data in the Humanities also hosts the Pre-modern Japanese Text dataset, which contains over 3000 high-quality scans of premodern Japanese books that are sometimes accompanied by high-quality transcriptions.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - There was a similar flowering of textual materials in Korea in the late 1990s. This was largely driven by the Korean government, which decided to foster digital corpora by making digitization part of its economic stimulus plan in reaction to the 1997 economic crisis. In 1998, an economic stimulus program called the Informatization Labor Project (Chongbohwa kŭllo saŏp 정보화근로사업) allocated approximately USD $200m over two years to create 48,000 white-collar jobs involving the digitization of cultural heritage”. Since then, various funding mechanisms have made multiple millions of dollars available to open databases and corpora. A good portion of this material, including the Compendium of Korean Collected Works Hanguk munjip chonggan 韓國文集叢刊 is available via the Database of Korean Classics. Other materials, such as the Annals of the Joseon Dynasty Choson Wangjo Sillok 朝鮮王朝實,are also available online.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - Individual scholars have also produced open-access corpora. Christian Wittern (n.d.) established the Kanseki Repository at Kyoto University using materials he began digitizing in 1989. This project initially focused on Buddhist texts and shares significant materials with CBETA. It now contains extensive materials from other sources like the Siku quanshu. The vast majority of these texts are available for bulk download via GitHub.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - As of May 2020, the largest open-access project for Chinese texts is Donald Sturgeons (n.d.-a) Chinese Text Project. This project began with early Chinese canonical works and now covers texts written through the Republican period. The Chinese Text Project now hosts over five billion characters worth of transcribed Chinese texts, many of which have been carefully edited by members of the Chinese Text Project community. In 2016, Chinese Text Project began hosting over five million pages of scanned images of text from the Chinese rare books held in the Harvard-Yenching Library. Sturgeon has also developed an OCR pipeline to automatically create searchable texts out of these images, producing results that usually suffice for searching, if not for text mining. There is also an institutional subscription schema that offers programmatic access to the Chinese Text Project.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - Another project with similar goals to Sturgeons Chinese Text Project is The Bookcase Shuge 書格, which was founded in 2013 by Wei Ceng. Shuge provides access to high-quality scans of a large selection of public domain rare Chinese books held in libraries around the world. The image-based nature of Shuges materials makes large-scale text mining impractical, but bulk download is possible with peer-to-peer software. Shuge also provides extensive bibliographic apparati and details on its materials origins.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - There are a number of smaller scale projects that specialize in texts of specific types. Researchers interested in local history should note the Max Plank Institute for the History of Sciences Local Gazetteer database project. This database and analysis platform provides access to local gazetteers containing extensive information on local politics, ecology, language, and more. Run by Dagmar Schäfer and Shi-pei Chen, the project is interested in exploring how the change of scales-by turning local records from individual gazetteers into a single global database-can reshape the study of historical China.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - Gender historians should likewise know about the Ming-Qing Womens Writings project. This project, run by Grace Fong at McGill University, began in 2003. It now contains scanned copies of 342 different collections of writing done by women in the Ming and Qing dynasties. This represents works written by over 5000 different women. The project also includes a database with extensive metadata on these writings, from names of the authors to the rhyme schemes of individual poems.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - Textual collections produced by scholars, academic institutions, and governments form the backbone of a significant portion of East Asian digital textual studies. Still, there are many places textual materials float around online whose origins are significantly less clear. WikiSource holds thousands of Chinese, Japanese, and Korean materials. The Daizhige 殆知阁 and Open Literature 開放文學 websites also host many texts of unclear provenance. Texts are also shared across social media sites, though access to these materials tends to be unstable.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - Database construction forms the other pillar of East Asian digital infrastructure and biographical databases are abundant. The China Biographical Database (CBDB), a project that emerged out of Robert Hartwells work, is possibly the most famous and widely used of these. It is a relational database containing prosopographical information on over 420,000 historical Chinese people as of May 25, 2020. It is valuable as both reference material and data for quantitative analysis. The CBDB is both an online service and can be downloaded as either an SQL or Access database. The modern iteration, designed by Michael Fuller and extended by Song Chen, can export structured data, making analysis simple. It is maintained by a group of scholars at Harvard University, the Institute of History and Philology at the Academia Sinica, and Peking University.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - The CBDB has inspired a similar databases of Japanese historical figures, hosted at Sophia University, but it remains significantly smaller in scope. The Japanese Biographical Database (JBDB) is a relational database first created in 2012 that in May 2020 contains the biographies of just over 9,400 people.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - The Academy of Korean Studies hosts Whos Who in Korean History ?국역대인물종?정보시스템, a biographical database containing information on over 25,000 people from very early Korea through to the Republic of Korea. This database also contains extensive supplementary information on official titles and the like.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - There are also a variety of more specialized biographical databases and more emerging every year. Yuxue Ren, Cameron Campbell, and James Lee have been working on the Chinese Government Employee Database- Qing (CGED-Q) since 2013, collecting information on people who worked for the Qing government. The Dharma Drum Buddhist College runs the Gaoseng zhuan 高僧傳 project, which provides biographies of Buddhist monks in Text Encoding Initiative (TEI) encoding. The project links the biographies with various name authorities and tools to facilitate biographical research.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - There are also many resources for geographic analysis. Historical maps, files for map making, and geographic data services all proliferate. The China Historical Geographical Information System (CHGIS) project, for example, contains detailed shapefiles (a data format used in Geographical Information Systems software to make maps) of historical jurisdictions in both China and Japan. It also provides information on place names, allowing users to associate latitudes and longitudes with historical places. It is hosted by Harvard and Fudan University and maintained under the direction of Jianxiong Ge, Peter Bol, and Lex Berman. The University of Tsukuba provides statistical maps of historical Japan, and the Centre for Open Data in the Humanities provides numerous shape files. There are similar resources available for Korea via the National Archives of Korea. Many individual libraries have put digitized copies of their map collections online, and the David Rumsey map collection contains a significant number of East Asian maps.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - Bibliographic data are also widely available and are very useful for book historians. A variety of premodern descriptive bibliographies are available online, but most of these exist only as full-text documents within online corpora. More structured databases on still-extant materials also abound. We owe many of these resources to librarians, who have been critical to the construction of our current digital ecosystem. Sadly, an important early project that made significant contributions to the field has been defunct for some years now (at least at western universities). Soren Edgren of Princeton University headed the now finished Chinese Rare Book Project, which created a union catalog for rare Chinese books. Fortunately, most of the projects data has been incorporated into the WorldCat online catalog, and the project itself moved to The National Library of China in Beijing in 2011. In the process of creating this union catalog, Edgren, Sarah Ellman, Chi-wah Chan, and other scholars affiliated with the team developed and published guidelines for systematically cataloging these books. The resulting machine-readable format reflects traditional bibliographic practice and contains a wealth of valuable information. The development of this material is critical for increasing access to rare books and lays the foundation for quantitative bibliographic analysis. There are similar resources for Japanese bibliographic information. One of the most prominent is the Union Catalog of Early Japanese Books Nihon Kotenseki Sogo Mokuroku日本古典籍総合目録. Reitaku University in Japan also maintains a union catalog of Korean books now held in Japan. Other projects, like the Chinese Religious Text Authority, are developing domain-specific bibliographies.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - There are a plethora of resources for scholars who work on images or art history, and much of this has been facilitated by the development of the International Image Interoperability Framework (IIIF). This API, developed by a consortium of universities, museums, libraries, and other cultural heritage institutions, is designed to make it easier to share access to their collections. This protocol enables projects like IIIF Discovery in Japan, which exposes the holdings of many libraries and museums. Individual museums with strong East Asian holdings, like the Cleveland Museum of Art and the British Museum, are also taking advantage of the format to provide access to their collections.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - Other organizations provide open image repositories but have mixed adoption of IIIF. The National Palace museum in Taipei has an open data portal with tens of thousands of downloadable images. The Library of Congress also has large holdings of images that are freely available online. The Asian reading rooms website links to multiple digital collections including high-quality scans of the Librarys rare Chinese books, censored prints from Japan, and pieces produced by minority populations in China and Japan. The library also has an image portal with tens of thousands of photographs and prints from China, Japan, and Korea.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - Tool development in East Asian digital humanities is flourishing and new projects frequently emerge. Some of these emerge in the process of creating the resources above, helping scholars explore and analyze the contents. The downloadable Microsoft Access version of the CBDB includes search and output tools that facilitate network analysis. The Chinese Text Project includes multiple tools like dictionaries, concordance tools, and parallel passage identification utilities that help users to closely read and do basic text-mining tasks.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - Tool development has also occurred independent of content creation. A variety of platforms have been created over the last 10 or so years to aid scholars. The MARKUS platform, created by Hilde De Weerdt and Brent Ho at Leiden University, for example, allows users to upload premodern Chinese and Korean texts and automatically tag people, places, user-defined concepts, and so forth. MARKUS also includes features that allow users to visualize when multiple texts share contents (via the COMPARATIVUS module) and to export marked-up results for analysis in other platforms.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - The recent flourishing of tool development depends on an important new trend in the digital humanities, closely related to (and partially dependent on) the open-access ethos in the field: the widespread creation of Application Programming Interfaces (APIs) by the major developers of East Asian databases and tools. An API allows software developers to retrieve information directly from someone elses servers or to incorporate the functionality of a particular tool developed by someone else. The Chinese Text Project, the CBDB, and other projects all provide this type of access, which is helping to create a densely interlinked ecosystem of projects that can build off of each other. MARKUS exemplifies this; a user can directly import a text using the Chinese Text Project plugin and use information from sources such as the CBDB, CHGIS, online dictionaries, and other resources to automatically mark it up. The Max Plank Institute for the History of Sciences Research Infrastructure for the Study of Eurasia (RISE) and SHINE projects also aim to streamline research by creating software and APIs that provide easier access to licensed and open-access materials for text and data mining.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - The DokuSky project, led by Jieh Hsiang at National Taiwan University, is another important effort that benefits from inter-compatible APIs. This project is designed to provide scholars with a personalized database and suite of tools designed to facilitate textual and geographic analysis.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - Some efforts aim to leverage the communal nature of the internet to build platforms that use technology to streamline annotation and translation. A prime example of this is the Ten Thousand Rooms Project, a Mellon Foundation-funded project at Yale University run by Tina Lu and Mick Hunter. The project bills itself as a collaborative workspace for pre-modern textual studies that is open to the public. Based on Stanfords Mirador IIIF viewer, users can upload images and build research projects around them. The crowd-sourced and collaborative nature of this platform has both research and pedagogical implications. Because multiple people can work on the same text, groups of scholars living anywhere in the world can easily develop annotated editions of premodern works and university class can transcribe, translate, and comment on the pieces they study. Scholars can also upload and annotate images of artwork. The materials hosted on Ten Thousand Rooms use a Creative Commons attribution non-commercial license, making the results of these collaborative projects public.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - Other tool development is centered on helping with specific research tasks by creating user interfaces, as many realms of analysis have long been reserved to those who know how to code. The Intertext project is one of a number of such projects, providing software to identify instances of text reuse among different documents. The National Endowment for the Humanities funded Topic Modeling the Handian Ancient Classics, likewise produced a tool to help visualize and explore topic models. The European Research Council funded OpenPhilology project is designing a platform that will automatically align Buddhist scriptures written in Chinese and Tibetan. The team of scholars working on the project is producing critical editions of the texts within the Mahāratnakūta Collection (大寶積經), but the platform the team will develop will allow scholars to more efficiently explore the philological relationships among multiple recensions of a given text.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - Some general technological developments in the last few years have had significant implications for East Asian digital humanities. Textual digitization pipelines involving OCR and handwriting recognition are getting better, and linguistic analysis tools are quickly advancing. Scholars at the Center for Open Data and the Humanities and the Université de Montréal have developed KuroNet, a deep learning-based character recognition framework designed to recognize Kuzushiji (an old style of cursive Japanese). This project facilitates both computational analysis and allows more people to read the texts, as the project members note that few can still read Kuzushiji. Important work is also being done on natural language processing pipelines for Chinese, Japanese, and Korean, although work on classical Chinese lags.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - One important development is the Stanza project at Stanford University, a multilingual natural language processing (NLP) package for Python. Stanza includes models for segmenting simplified and traditional Chinese, classical Chinese, Japanese, and Korean (and multiple other languages). There are also multiple projects focused on individual languages. Jieba is a popular modern Chinese segmenter for Python, CBETA has a tool for segmenting Buddhist Texts, and MeCab is an important open-source dependency parser. The Academia Sinicas CKIP lab is also developing promising new Python NLP tools for Chinese.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - It is important to note that East Asian digital humanities is not limited to Chinese, Japanese, and Korean materials (even if these make up the vast majority of ongoing projects). A number of projects in minority languages are happening around the world, although in most cases these projects are subsumed within the broader fields of Chinese or Buddhist Studies. One important example of such a project is the Manc.hu (n.d.) reading platform, which integrates Manchu texts and provides reference materials that serve as reading aids. Manc.hu is developed by Fresco Sam-Sin and Léon Rodenburg at Leiden University. There are also extensive Tibetan materials and tools available, primarily in the Buddhist studies sphere. There are also more efforts being made to digitize Monogolian texts. And importantly, Tangut and Khitan have recently been added to the Unicode standard, which will open up further possibilities for new studies in the lesser explored corners of East Asia.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - As in other fields, East Asian studies digitization, infrastructural, and tool-based projects exist in a symbiotic relationship with research and research aided by these tools has exploded in popularity in recent years. In East Asian studies, like in other historical studies around the world, historians were quick to employ geographic and network analyses. Song Chen, who worked on the CBDB, leveraged its prosopographical data to study the geographic distribution and social relationships of officials in Song dynasty China. Marcus Bingenheimer, Jen-Jou Hung, and Simon Wiles have done important work taking advantage of TEI encoded text to study the social networks of Buddhist Monks. Michelle Damian has used GIS to study the movement of commercial goods in premodern Japan.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - Text mining and textual analysis of varying degrees of sophistication has more recently become important in East Asian studies, although this work tends to be relatively constrained by copyright law, so less is done in the post-1920 period. Issues of authenticity and authorship have long been important to the field, and scholars are using text mining to address these issues. In the 1980s, Bing Chan reopened the issue of the authorship of Dream of the Red Chamber, and this continues to be relitigated. I myself am interested in using machine learning to explore the authorship of the Plum in the Golden Vase. Richard Jean So and Hoyt Long have been at the forefront of cross-linguistic/cultural digital humanities and have coauthored multiple important pieces that touch on Japanese, Chinese, and English literature and history. Long also coauthored a piece with Anatoly Detwyler and Yuancheng Zhu on Self-repetition and East Asian Literary Modernity”.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - Other projects use textual materials to extract information that drives other analysis. Hilde De Weerdt, Xiong Huei-Lan, and Liu Jiaolong have been working on the history of materiality using MARKUS to extract information from gazetteers, which drives significant geographic analysis. Michael Stanley-Baker and William Chong have been using a combination of tools to map drug locations in a Chinese medicinal manual.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - One interesting development in digital humanities is a push to new modes of publishing. This is illustrated quite well by the Chinese Deathscape, edited by Tom Mullaney, a born-digital publication that takes full advantage of its web platform to convey three detailed chapters on the history of grave relocation in China. Other projects exist in a similar mode, as digital exhibitions, like East Asian Textiles.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - A parallel development is that traditional scholarly venues are beginning to publish scholarship based on digital methods, so work done by these scholars is not only appearing in journals focused on the digital humanities (Cultural Analytics, Digital Scholarship in the Humanities, and the Journal of Historical Network Research have published research focused on East Asian topics) but also in long-standing East Asian studies journals like Early Medieval China, the Harvard Journal of Asiatic Studies, and Shandong Social Science 山東社會科學. The Journal of Chinese Literature and Culture and the Journal of Chinese History have both devoted special issues to the digital humanities recently. At the same time, there are new journals devoted specifically to digital humanities research: Tsinghua University is publishing a new Journal of Digital Humanities 《数字人文》 with the Zhonghua shuju, and Renmin University is launching Digital Humanities Research 《数字人文研究》. Still, East Asian DH is marginalized in western publications, making a very small percent of total articles.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - As research has flowered, so have the groups devoted to the digital humanities. In the last few years there have been a number of important organizational developments that help to foster digital scholarship. A major player in this is the Digital Sinology Facebook group founded by Elena Valussi, Mikael Ikivesi, and Christian Wittern in May 2015. As of May 2020, the group has over 1,500 members and is an active site of discussion in Chinese DH where scholars share news of conferences, articles, job listings, and calls for papers. It is worth noting, however, that Facebook is closed to mainland Chinese scholars. As such, mainland digital humanities discussions often occur on platforms like Weixin and Weibo.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - Scholars in Japanese Studies are making similar efforts at community building via efforts like Digital Humanities Japan, an organization that aim[s] to foster collaboration between those with similar interests by promoting scholarly dialogue, holding workshops to develop technical skills and project ideas, and creating a central platform for the sharing of resources related to digital methods. Digital Humanities Japan also hosts a wiki that collects resources to aid Japanese DH.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - The DHAsia program, organized by Tom Mullaney at Stanford University, is a premier example of community building. While it is focused on Asian Studies generally, many East Asian studies scholars have been involved. The program began in 2016 as a series of speakers who did week-long residencies at Stanford in which they gave a talk, advised students, and conducted a workshop. In 2018, DHAsia hosted a conference that featured papers by nearly 40 scholars from around the world.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - There has been a worldwide flowering of conferences focused on digital methods and concerns. In 2016, Peking University and Tsinghua University hosted the New Approaches in Chinese Digital Humanity conference. Tobie Meyer-Fong at Johns Hopkins University organized a conference in October 2017 assessing how scholars are dealing with the super-abundance of Qing materials called Anxieties of Abundance: Source and Methods for Qing Studies in the Digital Age. Tom Mazanec at UC Santa Barbara also organized Patterns and Networks in Classical Chinese Literature: Notes From the Digital Frontier in February 2018. In October 2018, Seoul National University Hosted a DH Conference and Hackathon.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - Professional organizations, unsurprisingly, are the drivers for a number of important East Asian digital humanities conferences. The Japanese Association for the Digital Humanities plans to hold its 10th annual conference in November of 2020. The Taiwanese Association for the Digital Humanities likewise routinely holds conferences that draw scholars from around the world. In 2019, Peter Bol and others organized a Digital Humanities Expo to occur alongside the annual Association for Asian Studies conference. This has since been more formally integrated into AASs conference offerings.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - Other efforts are focused explicitly on issues of infrastructure and sustainability. In March of 2018, Harvard, Peking University, the CBDB, and the Chinese Text Project hosted the International Conference on Cyberinfrastructure for Historical China Studies”. The goal was to bring together scholars, librarians, and commercial vendors to work through issues of scale and interoperability.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - There are also more research groups focused specifically on the digital humanities. The Centre for Open Data in the Humanities at the National Institute for Informatics in Japan is fostering research, tool development, and resource creation. Another innovative project is the Big Data Studies lab at Seoul National University in Korea, which aims to rethink how we approach humanistic inquiry in light of the exponential growth of human-produced data that are distributed across millions of servers around the world.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - A major barrier to the rapid development of East Asian DH is a lack of formal training mechanisms, particularly in the West. There are training programs for the digital humanities generally (e.g., Kings College in London offers a Masters degree in the digital humanities), though these are far from widespread. Students rarely have a chance to study digital humanities before graduate school, which often leaves students unprepared for complex digital projects. Some universities, like Leiden University, have created BA minor programs to try to ameliorate this issue. These programs offer students a broad introduction to digital tools, criticism, and materials. At Leiden, for example, students are introduced to text mining, social network analysis, social media criticism, digital history, mapping, all culminating in a capstone project.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - A number of universities in East Asia now offer digital humanities courses/training: Tsinghua University, Nanjing University, Seoul National University, Tokyo University, and Kyoto University are but a few of the places increasing their focus on digital humanities. Yet outside of Asia, there are relatively few courses of study in East Asian-specific digital humanities. A notable exception is the Digital China Lab at Harvards Fairbank Center for Chinese Studies, which offers training in digital methods to students across multiple departments. Many East Asian studies students are still self-taught or trained by scholars without domain expertise. They have to learn for themselves how to adapt tools and methods designed for western studies and to identify/use the increasing variety of tools and methods developed by scholars in East Asia.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - To correct this, workshops are becoming more common. In 2016, Leiden University held a summer school for digital humanities that trained Chinese studies students from around the world in database design, GIS, network analysis, and text mining (and there is hope that at some point this will be expanded to East Asian studies more generally). The same year Hoyt Long, Mark Ravina, and Molly Des Jardin organized the Impact of the Digital in Japanese Studies workshop, which likewise discussed areas of potential research in Japanese digital humanities.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - Online educational materials are also increasingly available. The Digital Humanities Japan Wiki includes a section on tutorials. Paula Curtis (2020) maintains a list of East Asian projects, some of which are training materials. Molly Des Jardin (2020) maintains a continually updating syllabus of East Asian Digital Humanities. I create video tutorials on both general DH and on computational issues related to Chinese.
[Author: Paul Vierthaler; From essay:"Digital humanities and East Asian studies in 2020 "] - The key to understanding where East Asian digital studies currently stands lies in the increasing openness of the scholars and institutions creating digital tools. Digital methods are becoming more easily accessible, and they are providing scholars are ever-more encouraging results. As Universities begin to expand training and more materials are digitized, this trend will accelerate. For the moment, scholars working in digital studies are somewhat sequestered in their own world, but gradually, these methods will become standard components in scholars toolkits. Not all scholars need to use quantitative analysis or text mining in their work, but they will need to be conversant in the methods and able to judge them just as they would more broadly accepted methods. It is an exciting time to be in East Asian studies, and new methods and models will continue to teach us interesting things about East Asias past.
[Author: Mario Wimmer; From essay:"Josephine Miles Doing Digital Humanism with and without Machines "] - Style and Proportion was both published and discounted in 1967. Ten years of hard intellectual work by Josephine “Jo” Miles, a poet and professor of English literature at the University of California at Berkeley, was dismissed by the very press that had carefully produced a strikingly original and lavish 200-page book. Miless passion for poetry was augmented by a curious interest in reading at a “middle-distance,” a term rather ironically coined by Franco Moretti in 2000. Given the current interest in digital humanism, Miles can be furthermore considered one of the pioneers of digital humanities—given that she concluded what today would be called a digital humanities project seventeen year prior to the publication of the first volume of Roberto Busas Index Thomasticus, which is considered the beginning of digital humanities. It is not entirely clear why the historiography of digital humanism has mostly overlooked Miles contribution to the field. Thus far, two articles have accounted for her work as early digital humanist.
[Author: Mario Wimmer; From essay:"Josephine Miles Doing Digital Humanism with and without Machines "] - The practice of distant reading across text, looking at semantic structures rather than doing hermeneutics of authors intentions, has a long history. The example of Miless scholarship shows that the field of digital humanities grew out of a rather orthodox research practice that can be connected to the big humanities projects of the nineteenth century. Back then, the hope was to establish a canon of texts to shape different national traditions. Accordingly, humanists in the academies of Berlin, Cambridge, or Paris were busy collecting, collating, and editing texts. The industrious efforts and good practice of nineteenth-century textual criticism from Jacques-Paul Mignes edition of the church fathers to the publications of the Monumenta Germaniae Historica or the Cambridge edition of Shakespeare-shaped those corpora that, a century later, would be digitized to be read with the aid of algorithms and not critical attitude.
[Author: Mario Wimmer; From essay:"Josephine Miles Doing Digital Humanism with and without Machines "] - What makes Josephine Miles a classic in humanities scholarship is not so much the application of computer-processing power for literary research but the method of distant reading she first developed during her graduate work in the 1930s. In this short introduction to her work, I will not speak to her many books of poetry or the three volumes of literary scholarship that grew out of her dissertation and others to follow. Instead, I shall focus on two of her projects: the concordance of John Dryden (1631– 1799) and her 1967 book Style and Proportion, which, for reasons to be explained, exists only in few copies around the globe. During her graduate work at the University of California at Berkeley, she developed a highly productive and original approach of literary analysis with linguistic tools. Parallel to her studies, she wrote poetry herself. It was her true passion. This became the reason for a curious choice of subject for her dissertation topic—the poetry of William Wordsworth (1770–1850). His poetry was at the time considered out of fashion and therefore did not have a chance to be critically appraised. This was exactly why Miles wanted to work on his language of emotion. She did not like it. “I am really more interested in finding out what I dont like and trying to understand that.” Her dislike of Wordsworths language allowed for analytical distance, which, in turn, was supposed “to support and invite intuition.”
[Author: Mario Wimmer; From essay:"Josephine Miles Doing Digital Humanism with and without Machines "] - When Miless senior colleague Guy Montgomery (1886–1951) passed away, he left an unfinished project: the concordance to the poetical works of John Dryden. Since Miles had experience with counting words and a reputation as a meticulous philologist, the department chair approached her to take it on. When Miles agreed to see the concordance to its conclusion, it was in the the form of 240,000 alphabetized cards, based on the Cambridge Edition. After a year of hard work, Miles realized that the project was in bad shape. Montgomery and his assistant, it seems, had not found a proper method of collating to account for the accuracy of the alphabetization. After talking to the linguist Charles Douglas Chretien (1904–69) about the project, he suggested using a computer as they already did in making dictionaries for exotic languages. Miles was intrigued and “went up to Cory Lab and said, Youve got the computers up here. Can I do anything about making these concordances?” Her colleagues in the department of electrical engineering and computer science were excited to help even though they had never done this before and did not quite know how. However they found a way of dealing with the “difficulty of accurate checking by assistants unfamiliar with the material. Together they decided to use an IBM 704a “large-scale, high-speed electronic calculator,” basically as an “aid in checking.”
[Author: Mario Wimmer; From essay:"Josephine Miles Doing Digital Humanism with and without Machines "] - At this point, however, quotations could not yet be processed by machines. Montgomerys cards were translated into machine readable punch cards for each word, poem symbol, and line number. “The resulting listing was corrected, and the cards were then sorted by machine into poem and line order, so that proof could be read in text-order. After correction and return to alphabetical order, the final listing was made, the columns pasted three to a page on specially ruled sheets, and the pages then delivered to the Press for photolithography.” In the preface to the published concordance she gives a list of what today would be called stop-words, that is, terms excluded from the concordance because they are insignificant, too frequent, or exceptional. As a result the book lists 208,000 word occurrences, after excluding around 32,000–33,000 according to the stop-list. Those listed are quite frequent and occur in a range in use between 400 and 1,100 times apiece. According to this analysis, fifty-seven words are particularly characteristic for Drydens language. In short, Miles directed, in the early 1950s, what might be one of the first digital humanities projects in history. In retrospect, however, the result seemed disappointing to Miles: “Nobody uses it unless he absolutely has to find out something about Dryden.” The concordance provides the precise location of a word using only the coordinates, say, poem X, line Y. It thus lacks the beauty of concordances once described by Miles but operates on the level of the word, not the sentence.
[Author: Mario Wimmer; From essay:"Josephine Miles Doing Digital Humanism with and without Machines "] - Starting in the later 1950s Miles wanted to take her approach to a new level. For about a decade she was doing meticulous work on the poetry and prose of England and America from the sixteenth century to the 1940s. No less than 120 works were sampled and scrutinized according to their relative number of adjectives, nouns, verbs, and connectives. The outcome was “more comprehensive and ambitious,” as one of the reviewers put it, than any of her previous studies in which she had pioneered a novel quantitative approach of literary analysis beginning in the 1930s. When she had finished the manuscript and overcome the problems of producing the tables for print, the Boston-based Little & Brown acquired the work for publication. However, there was a change of staff at the press. Her editor left his position, and the new one did not like what he saw. He told her, “Miss Miles, this is what I would call a non-book.” When the publishing house was taken over by a conglomerate, they circulated only a small number of copies and shredded the remaining as a basis for tax loss.
[Author: Mario Wimmer; From essay:"Josephine Miles Doing Digital Humanism with and without Machines "] - Indeed, what Style and Proportion offered was a methodologically intriguing analysis of the language of English poetry and prose from the seventeenth to the twentieth centuries. Among Miless archive one can find word-counting notes tracking the lexical habits of no fewer than 400 authors. The language of the 200 cases in point in Style and Proportion were categorized into three types predicative,” “connective-subordinative,” and “adjectival”—based on the number of adjectives, nouns, verbs, and connectives. Each of these groups characterize a particular type of crafting sentences, since “the art of sentence-making and figure-making in poetry and prose,” as she put it, allows to account for particular styles, maybe even styles of thinking. The proportions of language she accounts for are the relations between those words that Miles had counted earlier.
[Author: Mario Wimmer; From essay:"Josephine Miles Doing Digital Humanism with and without Machines "] - Furthermore, Miless analysis produced not just text but also visualizations of the proportions of language. The majority are tables; however, she also found a beautiful and intriguing way of representing the language of poetry, that recalls lart concret, in particular concrete poetry and its elements of montage, linguistic awareness, and the use of language as acoustic and visual material. She arranged the semantics of major words counted in each poem according to the structure of the original. Miles was ahead of her time not only in this respect—her avant-garde approach to literary analysis drew on various fields of research that, at the time, had not yet delivered the results that would have been necessary to bring her project to full success.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - This book began with a question and an opportunity. These arose, respectively, from conditions of the nineteenth century and of the twenty-first. In Australia in the nineteenth century, in contrast to the much more diversified literary markets of Britain and America, newspapers were the main local publishers as well as the major sources of fiction: local and imported. Literary historians knew much-though as this book shows, much less than we thought—about the Australian fiction published in this context. But very little was known about the fiction from elsewhere that appeared in these newspapers, even as it was estimated to comprise around 80 percent of all titles. In asking what fiction was published in nineteenth-century Australian newspapers I wanted to know where it came from, who wrote it, when it was published, and how it got there. By association, I sought to understand the transnational conditions in which local authors wrote and were read and by which an Australian literary culture developed.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - In the twenty-first century, the National Library of Australias (NLA) Trove database represents the largest mass-digitized collection of historical newspapers internationally. This was my opportunity: Trove made it possible, for the first time, to explore nineteenth-century Australian newspaper fiction in a systematic and extensive way. I devised a “paratextual method,” outlined in chapter 3, that uses formal features of these digitized newspapers to automatically identify and harvest fiction. On this basis, I discovered over 16,500 works, a massively expanded record of nineteenth-century Australian literary culture and its connections with the international circulation of fiction in this period.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - The titles I uncovered came from across the globe-from Britain and America as well as Austria, Canada, France, Germany, Holland, Hungary, Italy, Japan, New Zealand, Russia, South Africa, and Sweden. I found established international authors in Australian newspapers much earlier than had previously been realized, including multiple titles by Charles Dickens published prior to the mid-1850s, along with fiction by Honoré de Balzac, Alexandre Dumas, Eugène Sue, William Makepeace Thackeray, Gustave Toudouze, and Ivan Turgenev. However, it was after this time that fiction in Australian newspapers really expanded. Among the thousands of titles discovered were works by other canonical American, British, and European authors, including Benjamin Disraeli, George Eliot, Thomas Hardy, Victor Hugo, Henry James, Harriet Beecher Stowe, Anthony Trollope, Mark Twain, Oscar Wilde, Émile Zola, and Heinrich Zschokke. I also found numerous stories by prolific American dime novelists such as Sylvanus Cobb, Prentiss Ingraham, Laura Jean Libbey, and Ann S. Stephens, as well as an extensive array of works by popular British authors, including Mary Elizabeth Braddon (the most published international author in colonial Australia), Wilkie Collins, Arthur Conan Doyle, Robert Louis Stevenson, Ellen Wood, and many others. Alongside these international writers were a host of new Australian works and authors, with notable findings, including previously unlisted fiction by Catherine Martin and Jessie Mabel Waterhouse; a new Australian author, John Silvester Nottage, responsible for multiple full-length novels; and new titles by Captain Lacie” and “Ivan Dexter,” in addition to the discovery that both were well-developed pseudonyms for James Joseph Wright, who thereby emerges as one of, if not the most, prolific of Australias early authors.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - This greatly expanded record of nineteenth-century Australian newspaper fiction was created as a basis for what I call data-rich literary history, and what many scholars, especially in the United States, refer to in the terminology of the fields most prominent practitioner, Franco Moretti, as “distant reading. This approach to literary history applies computational methods of analysis to large bibliographical and/or textual datasets-derived increasingly though not exclusively from mass-digitized collections—to explore how literary works existed, interrelated, and generated meaning in the past. By investigating the cultural and material contexts in which literature was produced, circulated, and read, data-rich literary history seeks to challenge and move beyond the literary canons that organize perceptions of past literature in the present. This, then, was my intention: to move from question, to opportunity, to answers and, in so doing, to advance a noncanonical, data-rich, and transnational history of the literary, publishing, and reading cultures of nineteenth-century Australia. But working with Trove interrupted that neat sequence. Instead of simply answering questions, that engagement produced its own, pressing questions about the nature and implications of literary history conducted with mass-digitized collections and the literary data derived from them.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - While I had approached constructing a dataset of nineteenth-century Australian newspaper fiction purely as a preparatory task necessary to enable the synoptic form of literary history I sought to write, I came to realize that this supposedly precritical activity formed an extended historical argument in and of itself, in the context of a specific mass-digitized collection. And while current discussion of mass-digitization foregrounds the scale of such collections and the extensiveness of the digital access they provide, I became increasingly conscious of the gaps in Trove. and this is a mass-digitized collection that is among the largest, and most complete, in the world. When I looked to other data-rich literary history projects to see how they were meeting this challenge I found that the complex relationships between documentary record, digitization, data curation, and historical analysis were not fully articulated. In the highest profile work in this field-Morettis distant reading and the “macroanalysis of his longtime collaborator and the cofounder of the Stanford Literary Lab, Matthew L. Jockers—these relationships and their effects were essentially denied in preference for a view of large-scale literary data and mass-digitized collections as transparent windows onto the past.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - The result of the questions and opportunities that led me to this project-and of the questions they, in turn, generated is a book in two parts. The first explores limitations in existing approaches to data-rich literary history and offers an alternative in the form of a new scholarly object of analysis. My central contention in this first part is that, whatever computational methods allow us to do with ever-growing collections of literary data, the results cannot advance knowledge if the literary data analyzed do not effectively represent the historical context we seek to understand. I draw on the theoretical and practical foundations of textual scholarship to constitute what I call a scholarly edition of a literary system: that is, a model of literary works that were published, circulated, and read—and thereby accrued meaning—in a specific historical context, constructed with reference to the history of transmission by which documentary evidence of those works is constituted. In doing so, I seek to provide an appropriate foundation not just for data-rich research but for the broader discipline of literary history as it increasingly operates in a dynamic and expansive digital environment.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - The second part of A World of Fiction demonstrates how analysis of a scholarly edition of a literary system can revolutionize knowledge of literary history as well as the frameworks and concepts through which we perceive past literature in the present. While the “transnational turn in Australian literary studies has represented nineteenth-century Australian readers as oriented almost entirely toward British fiction, I offer a more complex picture: one where this orientation exists, but where colonial authors, publishers, and readers also forged a distinctive Australian literary culture. In the process, I describe an entirely new organization for and structures within literary culture in the colonies and demonstrate the capacity of data-rich literary history to advance understanding of major concepts and phenomena in the broader discipline, ranging from literary anonymity and pseudonymity to reception, fiction reprinting and syndication, and the nature of literary traditions.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - Although terminology in the field is evolving, literary histories that employ data and mass-digitized collections still typically begin by citing Morettis influential concept of distant reading and/or Jockerss notion of macroanalysis. There is good reason for this. These scholars have dominated academic and general discussion of data-rich literary history. Not only have they written some of the only book-length contributions to the field, but remarkably for literary scholarship, Morettis and Jockerss work is reported on in major public forums: the Financial Times, the Los Angeles Review of Books, the New York Times, the New Yorker, the Paris Review, and more. Distant reading is a term routinely employed to describe the methods of computational textual analysis in general.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - While the use of extensive data to investigate historical literary systems has significant antecedents, not least in book history, Moretti has been highly effective in demonstrating the potential critical sophistication of this approach to the wider field of literary studies, and in thereby generating interest in digital methods.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - Although influenced by both Morettis and Jockerss work, this book begins rather differently: with a critique of their approach. Chapter 1, “Abstraction, Singularity, Textuality: The Equivalence of Close and Distant Reading,” argues that distant reading and macroanalysis offer an inadequate foundation for data-rich literary history because they neglect the activities and insights of textual scholarship: the bibliographical and editorial practices that literary scholars have long relied on to interpret and represent the historical record. Textual scholars understand literary works as events—unfolding and accruing meaning across time and space—and the documentary record as partially and provisionally expressing that process. By contrast, distant reading and macroanalysis conceive data and mass-digitized collections as providing direct and unmediated access to the historical literary record and reduce literary works to texts, perceiving them as singular and stable entities, related to each other in history via basic categories of production. The models of literary systems that Moretti and Jockers construct on this basis are limited, abstract, and often ahistorical.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - Such inattention to the historical and material nature of the documentary record is inherited from, not in opposition to, the New Criticism and its core method of close reading. Close readings are generally protected from the worst consequences of such underlying assumptions by the disciplinary infrastructure that textual scholars have produced, and by the documentary context in which such readings are enacted. The same cannot be said of distant reading and macroanalysis, which take the problematic assumptions of close reading to an extreme conclusion. The problem with these prominent approaches is not that they introduce new quantitative methods, inimical to nuanced literary-historical understanding, as has been argued. Rather, distant reading and macroanalysis construct and seek to extract meaning from models of literary systems that are essentially deficient: inadequate for representing the ways in which literary works existed and generated meaning in the past. The solution to this problem is not rejecting data-rich literary history, nor proposing new, more elaborate forms of computational analysis; it is not even integrating computational and non-computational methods, as is often proposed. These solutions define the issue in terms of the method of analysis used, when the foundational problem is the lack of an adequate object to analyze: one that is capable of managing the documentary records complexity, especially as it is manifested in emerging digital knowledge infrastructure, and of representing literary works in the historical contexts in which they were produced and received.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - Turning to provincial newspapers, I demonstrate that fiction reprinting, as well as publishing, was more common in that context than in the metropolitan one. Such reprinting was performed, in part, by editor- and author-led enterprises, of varying degrees of formality. But provincial newspapers sourced most of their fiction from an extensive array of hitherto unrecognized syndicates, operating both locally and internationally. While past histories of Australian publishing are primarily book-based, this investigation shows that a local newspaper syndication agency, Cameron, Laing, and Co., published the most Australian fiction for the nineteenth century and probably well into the twentieth. It also reveals that provincial syndicates provided more fiction to the colonies than any of the book publishers, local or global, or the metropolitan newspapers that have been the focus of previous literary and book histories.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - Chapter 2, Back to the Future: A New Object for (Data-Rich) Literary History, responds to this challenge by articulating the case for and describing a scholarly edition of a literary system. I begin by surveying existing projects in data-rich literary history that model literary systems differently than do distant reading and macroanalysis. Instead of treating literary works as singular and stable objects, these projects represent and explore the means by which those works connected to each other and accrued meaning in the past. I argue that explicit engagement with modeling, as the method has been theorized in digital humanities, has significant potential to enhance data-rich literary history by offering a mechanism through which to interrogate and refine conceptions of literary works and systems. But modeling alone is insufficient as a basis for the field. A data-rich model of a literary system is inevitably an argument shaped by not only the scholars perception of cultural artifacts and phenomena but the complex history by which those artifacts and phenomena are transmitted to and by us in the present. Modeling does not provide a mechanism through which to recognize and represent the inevitably transactional nature of the documentary record: the fact that, whether analog or digital, that record never exists in a stable form but is produced in time and in our interactions with collections. Nor does modeling offer a framework for assessing and managing the inevitable, sometimes radical, disjunctions that exist between knowledge infrastructure, the historical context it is intended to represent, and the data employed in that representation.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - Despite the “heavy... associations” the scholarly edition carries “from print culture”, it provides an effective framework through which to understand and accommodate such contingency and partiality. A scholarly edition of a literary work is sometimes perceived simply as a version with extra references, or with added historical context. In fact, it is an argument—a historical and critical but also a technical one—that offers a foundation for other literary-critical and historical arguments to build upon. It presents that argument through a curated text, one for which the content is, by definition, contested, and a critical apparatus that demonstrates and manages those contested features by describing and justifying the editors engagement with the documentary record relating to a literary work, including with the inevitable gaps, remediations, and uncertainties that engagement exposes and creates. Another way to describe this interrelationship is to say that a critical apparatus explains the history of transmission that the editors understanding is based upon and contributes to, while the curated text embodies the outcome of that history of transmission, including the current moment of interpretation, in the form of a stable, historicized, and publicly accessible object for analysis. While the curated text of a conventional scholarly edition embodies or models the editors argument about a literary work, a scholarly edition of a literary system uses a curated dataset to model the editors argument about the nature of and relationships between literary works in the past.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - For this book I did not simply theorize a scholarly edition; I built one. Its curated dataset is a subset of the 16,500 titles I discovered in the nineteenth-century Australian newspapers digitized by Trove. This subset encompasses a little over 9,200 works of extended fiction. Most of these (98 percent) are extended due to serial publication over two or more (often many more) newspaper issues. A small fraction of these titles (2 percent) were completed in a single issue but amount to ten thousand or more words (sometimes considerably more, with stories identified in this project of over sixty thousand words in a single-usually a Christmas-newspaper issue).
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - The curated dataset is made available in two forms: as downloadable bibliographical and textual data from the University of Michigan Press website and through an interface for searching, browsing, partial editing, and selective or wholesale exporting through the Australian National Universitys Centre for Digital Humanities Research.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - This database also makes available the other (approximately seventy-three hundred) works of short fiction identified in nineteenth-century Australian newspapers (along with a further five thousand short and extended titles from the early twentieth-century). But I chose to focus on nineteenth-century extended fiction—to subject those titles to extensive historical and bibliographical research and representation—because those works constitute a literary system in the form of production and reception they imply. While stories completed in a single newspaper issue suggest incidental publishing and reading-with such content often selected simply to fill column inches and likely read in a casual manner—extended fiction required deliberate sourcing and publishing by editors and implies more intensive or committed engagement from readers.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - The critical apparatus for this curated dataset is composed of certain fields in the dataset that explain and justify decisions about derivation and attribution of these stories, as well as a historical introduction to the models parameters and principles, presented in chapter 3. From World to Trove to Data: Tracing a History of Transmission” describes the history of transmission by which these nineteenth-century Australian newspapers were published, and subsequently collected and remediated, ultimately as digitized documents in Trove, and by which I identified the fiction they contain and represented it as bibliographical and textual data. In seeking to describe the translations and transformations, as well as the gaps and uncertainties, involved in this sequence, I foreground the fact that all collections—analog and digital, those we find and those we construct—have histories. These histories fundamentally determine access to the documentary record, in conjunction with the assumptions and arguments we bring to that inquiry.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - As well as a basis for the arguments presented in A World of Fiction, this scholarly edition of a literary system is, fundamentally, for others to use. It makes the outcomes of a rigorous engagement with a mass-digitized collection available for the benefit of all literary historians, whether they are computationally inclined or not. Accordingly, while the argument it embodies about the existence of, and the interrelationships between, literary works in the past is an outcome of analysis, it is also a basis for future exploration. As I elaborate with examples in the conclusion, like a scholarly edition of a literary work, a scholarly edition of a literary system is designed to enable and advance—rather than to decide or conclude—investigation.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - In using this scholarly edition to investigate extended fiction in nineteenth-century Australian newspapers, each of the three chapters in the books second part adapts and applies a core digital humanities method: bibliometrics in chapter 4, network analysis in chapter 5, and topic modeling in chapter 6. Each chapter begins with a critical analysis of the respective method: although it is a premise of the book that data-rich literary history should not focus on the methods of analysis used to the detriment of the object analyzed, in a field that attempts to understand literature and culture by applying techniques devised for other purposes a critical approach to methodology is essential. My focus, however, is not on trialing the newest or most innovative digital methods per se but on crafting computational approaches best suited to exploring this publishing and reading context, and to responding to the requirements of humanities inquiry. To these ends I present a form of bibliometrics that accommodates the prevalence of anonymous and pseudonymous works in the curated dataset and enables insights into reception as well as publication; I offer an application of network analysis that manages extensive gaps in the newspapers digitized and titles captured; and I devise an approach to topic modeling that creates an intelligible relationship between thematic and documentary features of individual literary works, and of the literary system in which they generated meaning.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - In presenting the results of this analysis, these three chapters intervene in a prominent critical trajectory in literary studies, for Australia and internationally: the so-called transnational turn. As has been the case for many national literary fields, in recent decades Australian literary scholars have recognized that Australian literary history is not coterminous with the history of literature by Australians. Yet in Australia, this turn has arguably gone further than elsewhere, particularly for the nineteenth century, where colonial literature has been recast as marginal to the history of literature in Australia. Using various empirical sources-publishers’archives, lending library records, and reading group minutes—scholars have described an Australian literary tradition as chronically belated and have discussed colonial readers in terms of their derivative . . . reading habits, disregard for local fiction, and marked preference for writing from elsewhere, especially Britain.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - While such research offers an important counterweight to the fields earlier literary nationalism, the view that Australian readers were entirely focused on overseas, particularly British, literature moves too far in the opposite direction. In investigating the main form of local publishing and source of fiction in nineteenth-century Australia—newspapers-this book explores the profound importance of British fiction to literary production and reception in the colonies. But it denies that the relationship to British culture was one of subservience and imitation. To the contrary, I demonstrate at scale what emerging analyses of individual works and reading practices increasingly recognize: how literary forms and practices were translated and transformed by their colonial enactment.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - More contentiously, I emphasize the distinctiveness of literary culture in the colonies, including in the forms of writing and authorship that readers valued, in how fiction was sourced, and in the themes explored in Australian stories.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - Chapter 4, “Into the Unknown: Literary Anonymity and the Inscription of Reception,” examines conceptions of literary and cultural value operating in colonial Australia by investigating the origins, known and inscribed, of newspaper fiction. It takes as its starting point an acknowledged characteristic of transnational literary culture in this period—anonymous, pseudonymous, and indeterminate authorship-that is nevertheless occluded by the way we study literature in the past: by extracting (predominantly canonical) works from the anonymous conditions under which they were originally published and read. Not only is this strategy impossible when dealing with thousands of works where authorship is unknown, but it treats anonymity and pseudonymity as an absence, rather than a constitutive presence, in literary culture. This approach also ignores the extensive information about authorship contained in the titles, subtitles, and other paratextual and textual components of publication events, whether or not these details align with those of the historical individuals who wrote the works. To avoid these pitfalls, I consider bibliographical designations of authorship as well as how authorship was represented—or inscribed—in newspapers.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - Exploring these two models of authorship shows that cultural value in the colonies was strongly associated with mens writing, that Australian fiction was both more present and accorded more importance than has been recognized, and that newspapers privileged British, while marginalizing American, writing. With respect to colonial fiction, I demonstrate a significant shift in its promotion and publication in the late 1870s and 1880s: from metropolitan to provincial newspapers. Based on the existing understanding of provincial newspaper fiction as rare, and pirated when present, this finding would seem to have little import. In showing that these provincial newspapers published substantially more fiction than their metropolitan counterparts, this chapter reassesses the development of early Australian literature and literary culture, as well as the gendered ethos organizing this process.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - Chapter 5, Fictional Systems: Network Analysis and Syndication Networks, investigates fiction reprinting to explore the ways in which colonial literary culture both intersected with and was distinct from the global circulation of fiction in the nineteenth century. For the metropolitan context, the results challenge the existing emphasis in studies of colonial literary culture on the first and best-known British syndication agency, Tillotsons Fiction Bureau, along with the view that fiction provided by this company—and syndicated British writing generally-overwhelmed local publishing and writing. I show that Tillotsons was only one entity among many operating in Australia, that it became systematically involved with colonial newspapers a decade earlier, and by a different motivation, than has been argued, and that the involvement of colonial newspapers in publishing local writing, and in sourcing and distributing fiction, continued despite the presence of multiple international companies in the market.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - Chapter 6, Man people woman life / ‘Creek sheep cattle horses: Influence, Distinction, and Literary Traditions,” turns from the source and reception of fiction in nineteenth-century Australian newspapers to its content, and to the question of whether colonial writing demonstrated any distinct features. Given the transnational market in which colonial literature developed, it has been argued that a distinctive Australian literary tradition was impossible. I show, to the contrary, thematic tendencies in colonial fiction that are clearly different from those in American and British writing. Focusing on rural colonial spaces, characters, and activities, such writing resonates in certain ways, including in its masculine orientation, with the primary framework through which nineteenth-century Australian fiction was perceived prior to the transnational turn: the bush tradition. Yet this fiction also departs from past perceptions of that tradition, including in its presence before the supposedly foundational decade of the 1890s, and most strikingly in its consistent and prominent depiction of Aboriginal characters. Such depictions refute the prevailing view that Aboriginal people were excluded from colonial fiction and offer new perspectives on literary engagements with colonization.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - The British and American fiction in nineteenth-century Australian newspapers tends to be characterized not by representations of identifiably British or American people and places but by contrasting attitudes toward history and time. While British fiction frequently emphasizes the interconnectedness of past, present, and future and is ambivalent about the capacity of individual actions to effect historical change, much American fiction shows the opposite: a focus on the present and optimism about the future and the individuals role in shaping it. Both of these attitudes toward time are manifested in nineteenth-century Australian fiction, with writing by women and in metropolitan newspapers more likely to share the prevailing British perspective and fiction by men and in provincial newspapers more likely to demonstrate features in common with American writing. These correlations between fictional contents and material and authorial trends suggest a fundamental cultural divide in the orientation of colonial fiction to “new” and old worlds.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - A World of Fiction works across, while contributing to, two fields-digital humanities and literary history—that are in many ways closely integrated, while in others, still academic worlds apart. It may well be that scholars who primarily align themselves with digital humanities will gravitate to the discussion of digital knowledge infrastructure, digitization, data, remediation, and modeling in part 1 and to the critiques of digital methods that begin the other chapters. Alternatively, literary, book, and media historians might find more of specific interest in part 2s exploration of literary production and reception and of the transnational circulation of nineteenth-century fiction in periodical form.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - While such variation in the interests of readers is perhaps inevitable, the two parts of this book fundamentally require and inform each other. Although it is often understood as such, literary history is not solely an analytical and critical enterprise; it has always been bound up in-enabled and produced by—the knowledge infrastructure that it creates and employs. Equally, although digital humanities is frequently presented as a methodological and infrastructural endeavor, it is just as much a historical and analytical one. The approaches and infrastructure developed and employed in that field have histories, just as the conceptual entities examined—including literary data and computational models—are critical and interpretive constructs. Confronting the challenges and possibilities that new digital technologies and resources bring to literary history and to the humanities broadly requires a mutually informative relationship of traditional and digital scholarship. Only such a relationship can enable the emergence and consolidation of the new forms of evidence, analysis, and argumentation required by the contemporary conditions of cultural research.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - In a blog post entitled “A Dataset for Distant Reading Literature in English, 1700-1922, Ted Underwood describes as malarkey” the version of distant reading currently circulating in the public imagination”— namely, that it analyzes “a massive database that includes everything that has been thought and said.” He continues,
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - In the early days of distant reading, Franco Moretti did frame the project as a challenge to literary historians claims about synchronic coverage. (We only discuss a tiny number of books from any given period-what about all the rest?) But even in those early publications, Moretti acknowledged that we would only be able to represent “all the rest” through some kind of sample.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - Underwood is correct in a narrow sense: Moretti engages in, and occasionally acknowledges his use of, data sampling. But it does not follow that the public imagination, or the mainstream media outlets feeding it, confected the view of distant reading as enabling direct and objective access to a comprehensive literary-historical record. Morettis work provides more than ample grounds for this public perception, as does Jockerss closely related paradigm of macroanalysis. While claiming direct and objective access to “everything,” these high-profile authors represent and explore only a very limited portion of the literary system, and do so in an abstract and often ahistorical way.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - Moretti has been criticized in similar terms previously: for adopting a reductive approach to literature and associating data with comprehensive and authoritative knowledge. Those who reject any role for data in literary history maintain both of these criticisms, on the basis that data are inimical to literature, and only close reading can explore its nuance and complexity. Katie Trumpener, for instance, argues that data-based methods “violate” the “individuality of literary works, while Stephen Marche insists “literature is not data. Literature is the opposite of data”. James English attributes such responses to the disciplines foundationally “negative relationship” to “counting,” noting its intensification in the face of “ever more stringent quantification regimes of value and assessment,” as well as Morettis role in exacerbating that oppositional perspective.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - Those who advocate the use of data in literary studies typically deny that Morettis approach is needlessly reductive. Echoing Morettis account in Distant Reading of the method as a little pact with the devil”, they acknowledge that abstracting and simplifying complex phenomena is an inevitable consequence of quantitative approaches, but one that is justified by the new forms of knowledge it enables. Regarding Morettis tendency to “overestimat[e] the scientific objectivity of his analyses opinion is more divided. Some who support a data-rich approach to literary history perceive Morettis claim to authoritative knowledge as an unfortunate side effect of his polemical intent to challenge literary historys reliance on close reading. As Tim Burke writes, “There is no requirement to purchase the entire methodological inventory he makes available, or to throw overboard close reading or aesthetic appreciation”. Others ascribe a more foundational essentialism to Morettis work. John Frow argues that Moretti conceives of literary history... as an objective account of patterns and trends” by ignor[ing] the crucial point that these morphological categories he takes as his base units are not pre-given but are constituted in an interpretive encounter by means of an interpretive decision”.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - In my view, these critiques describe the symptoms-not the essence of a problem, which also characterizes Jockerss macroanalysis, as well as the New Criticisms core method of close reading. Contrary to prevailing opinion, distant reading and close reading are not opposites. These approaches are united by common neglect of textual scholarship: the bibliographical and editorial approaches that literary scholars have long depended on to negotiate the documentary record. Because of this neglect, like the New Critics before them, Moretti and Jockers cannot benefit from the critical and historical insights presented by editorial and bibliographical productions. As a consequence, both authors conceive and model literary systems in reductive ways and offer ahistorical arguments about the existence and interconnections of literary works in the past.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - Underappreciated in commentary on distant reading and macroanalysis is the shifting meanings of both terms. When Moretti originally proposed distant reading in “Conjectures on World Literature in 2000, it was a “new critical method for world literary studies, not for literary history. Distant reading aimed to overcome the focus on national canons by collating the work of multiple scholars to identify and explore “units that are much smaller or much larger than the text: devices, themes, tropes—or genres and systems”. With his 2005 book, Graphs, Maps, Trees: Abstract Models for a Literary History, the framework of world literary studies was superseded by literary history (indeed, a national formation of that endeavor, focused on eighteenth- and nineteenth-century British literature). While units smaller or larger than the text were theoretical notions in his “Conjectures” essay, in this book they are translated into data points. A systemic approach to literary history also became central, with the abstract modeling devices of the title-graphs, maps, trees-employed to explore, and to visualize, the operations of a literary history that “cannot be understood by stitching together separate bits of knowledge about individual cases, because it isnt a sum of individual cases: its a collective system, that should be grasped as such, as a whole”.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - Computational methods and digital resources were in turn central to Morettis 2013 book, Distant Reading, but there literary history was ceding ground to “the theory of literature as the focus in “the encounter of computation and criticism”. Although literary data remains central, the primary object of distant reading is now less often literary systems-designated social, material, and political contexts for literary development and change- than the concepts of literary study. And while Moretti previously identified the importance of literary systems in their inclusion of the great unread”, these concepts of literary study (including characterization, plot, and dramatic form) are approached predominantly through formal and computational analyses of canonical literary works. Jockerss focus has remained more consistent over time. But his recent work demonstrates this same shift from literary history-his explicit concern in his 2013 book, Macroanalysis: Digital Methods and Literary History—to categories of literary analysis: in his case, plot and characterization. Yet even as Moretti and Jockers have moved from a historical to a conceptual emphasis in their own work, distant reading and macroanalysis dominate—and limit—public, and much academic, perception of what data-rich literary history entails.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - Pace Underwoods defense, in their literary-historical work both Moretti and Jockers present literary data and digital collections as precritical, stable, and self-evident. In conceiving data and computation as providing direct and comprehensive access to the literary-historical record, they deny the critical and interpretive activities that construct that data and digital record and make them available for analysis. In Morettis work on literary history, literary data are consistently presented as facts rather than interpretations. Thus the first chapter of Graphs repeatedly references “the large mass of [literary] facts as ideally independent of interpretations”, as “data, not interpretation”, and as useful because they are independent of interpretation”. On this basis, Moretti accords his arguments an unrealistic exactitude. For instance, his claim that bibliographical data “can tell us when Britain produced one new novel per month or week or day, or hour for that matter” denies the inevitable gaps between the publishing context and the bibliographies he proposes to explore them with. Similarly, Moretti presents data visualization as a transparent window onto history, with the idea that graphs, maps, and trees place the literary field literally in front of our eyes—and show us how little we still know about it. The same understanding of literary data appears in Distant Reading, where Moretti celebrates data visualization as providing a set of two dimensional signs . . . that can be grasped at a single glance.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - Such descriptions, which substitute seeing what is there for the interpretive acts involved in constructing literary data, organizing it, and ascribing a historical explanation to the results, underpin Morettis contention to explore “the literary field as a whole”.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - While this view of literary data as factual and transparent has been noted and critiqued—such assessments miss its underlying cause: Morettis lack of interest in the scholarly infrastructure that enables his analyses. For results derived from analog bibliographies—as in the first chapter of Graphs and his stylistic “Reflections on 7,000 Titles in Distant Reading—parentheses and footnotes occasionally admit that comprehensive access to the facts of literary history is not achieved. For example, figure 7 in the latter study, showing the number of British novels, stops in 1836, while the other graphs extend to 1850. In a footnote, Moretti explains the discrepancy with the comment “it seems very likely that Andrew Blocks bibliography significantly overstates the number of novels published after that date”. Yet acknowledging that his dataset arises from a (“significantly) flawed interpretive encounter affects neither Morettis rhetoric nor his subsequent analysis. The chapter still claims to “read the entire volume of the literary past”, and while the data are absent from figure 7, Blocks bibliography is the only source for titles published from 1836 to 1850. Moretti proceeds, in other words, by analyzing titles he knows never existed.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - While literary data derived from analog bibliographies are only ideally independent of interpretations,” Moretti regards mass-digitized collections as achieving this independence. With such collections becoming the rhetorical, if not the primary analytical, focus of Distant Reading, Moretti looks forward just a few years, to when “well be able to search just about all novels that have ever been published and look for patterns among billions of sentences”. He notes that, while literary studies has previously experienced “the rise of quantitative evidence . . . without producing lasting effects, this time is probably going to be different, because this time we have digital databases and automatic data retrieval”. While digital technologies are celebrated and foregrounded, beneath these claims lies the same disregard for the specifics of the disciplinary infrastructure that characterized Morettis approach to bibliographies. This attitude is apparent in an interview in which Moretti aligns digital humanities with three elements:
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - new, much larger archives; new, much faster research tools; and a (possible) new explanatory framework. The archives and the tools are there to stay; they are important but not intellectually exciting. What appeals to me is the prospect of a new explanatory model—a new theory and history of literature.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - In presenting digital “archives” or collections as “there to stay, Moretti disregards their status—like bibliographies—as interpretative constructs. And unlike print-based bibliographies, most digital collections are constantly changing: a dynamism with significant practical and conceptual challenges for literary history.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - Asserting comprehensive access to the historical literary record is even more essential to Jockerss notion of macroanalysis. Although he sometimes presents his method as complementing rather than replacing idiographic approaches to literature, foundational to macroanalysis is Jockerss view of interpretation as methodologically defective: “Interpretation is fueled by observation, and as a method of evidence gathering, observation—both in the sciences and in the humanities—is flawed”. While interpretation and observation are “anecdotal and speculative,” “big data is supposedly constituted without human involvement and thus offers “comprehensive and definitive historical facts. According to Jockers, literary scholars “have the equivalent of ... big data in the form of big [digital] libraries [or] massive digital-text collections,” and these enable “investigations at a scale that reaches or approaches a point of being comprehensive. The once inaccessible population has become accessible and is fast replacing the random and representative sample”. As Jockers says of one of Morettis analyses, such unprecedented and supposedly uninterrupted access to the documentary basis of literary history leaves little room for debate”: a perspective that overlooks the fact that all collections are selections, made according to (implicit or explicit) arguments about value, and with varying degrees of expertise and funding.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - Jockers employs a number of scientific metaphors to buttress this association of scale and comprehensive access, the most explicit being open-pit mining or hydraulicking.” While “microanalysis” (including reading and digital searching) discovers “nuggets, macroanalysis accesses the deeper veins [that] lie buried beneath the mass of gravel layered above”. In working with the “gravel” of literary history, employing “the trommel of computation to process, condense, deform, and analyze the deep strata from which these nuggets were born, Jockers supposedly unearth [s], for the first time, what these corpora really contain. This metaphor not only renders literary history as concrete an entity as a mountain: all of it can be accessed and analyzed. It also conflates analysis with the achievement of complete access. The network visualizations with which Jockers presents the cumulative results of macroanalysis reinforce this view of literary data as factual and comprehensive. As chapter 5 explores in depth, because the form and meaning of most network graphs change when new nodes and edges are added, to claim that they display the structures and relationships that organized literature in the past implies that all data are available for analysis.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - In modeling literary systems, Moretti and Jockers define literary works as single entities in time and space, typically located as such by the date of first book publication and the authors nationality. Literary works are constituted as literary systems when they share these basic features as in “nineteenth-century” or “British” novels—with other characteristics added within that framework (such as the authors gender or the works genre). This basic understanding of literary systems is evident, for instance, in Morettis analysis of 7,000 titles (British novels, 1740 to 1830)” in Distant Reading, discussed above, or of eighteenth- and nineteenth-century British novels defined in terms of the number of titles published, authors genders, or fictional subgenres. It likewise underpins Jockerss exploration in Macroanalysis of 758 works of Irish-American prose literature spanning 250 years or 3,346 nineteenth-century British and American novels.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - While Moretti occasionally acknowledges limitations in his data (before proceeding with analyses regardless), Jockers maintains that any leap from the specific to the general” is flawed because based on interpretation. Only in the books final chapter does he admit the obvious gap between his datasets and the “population” of nineteenth-century novels, describing his largest “corpus of 3,346 texts” as “incomplete, interrupted, haphazard,” and noting, “The comprehensive work is still to be done”. This concession generates an awkward comparison of macroanalysis with Charles Darwins theory of evolution, which reinforces Jockerss equation of knowledge with scale and comprehensive access. While both are “idea[s],” because “there are further dimensions to explore”, literary scholars are advantaged over evolutionary biologists “in terms of the availability of our source material”. In a context where bigger is better—as Jockers says elsewhere in the book, “eight is better than one, [but] eight is not eight thousand, and, thus, the study is comparatively anecdotal in nature-his 3,346 observations and 2,032,248 data points are seemingly indicative of knowledge in and of themselves. Jockers concludes by admitting one impediment to macroanalysis, but it is only legal: though almost “everything has been digitized, post-1923 publications remain (at the time he was writing) protected by copyright, leaving literary scholars dependent on legal reforms before they might realize “what can be done with a large corpus of texts”.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - A recent collaboration of the Stanford Literary Lab, which Jockers is no longer part of, departs in one important way from the approach to literary history, data, mass digitization, and computation I have described. Literary Lab Pamphlet 1 1 closely attends to the gaps between the published (all literary works made public in history), “the archive” (the portion of what was published that has been preserved and is now increasingly digitized), and “the corpus (the segment of the archive selected for a research question). Although incorrectly imagining that the “convergence of these three layers into one may soon be reality,” in acknowledging that this state has not been achieved, the authors admit the constructed—and selective-nature of literary data. Yet Pamphlet 11 follows Morettis and Jockerss precedent in misconstruing the nature of our disciplinary infrastructure, in that the authors presume to overcome the selections and biases of mass-digitized collections by using analog bibliographies to generate a random sample of what was published. This strategy misses the vital point that both digitized collections and analog bibliographies are derived from “the archive,” predominantly the collections of major (usually American or British) university libraries. Pamphlet 11 also replicates Morettis and Jockerss approach in not publishing its datasets.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - Moretti often references his sources of data—chapter 1 of Graphs, for instance, begins by listing the bibliographies it draws upon-and he advocates data sharing: “Because data are ideally independent from any individual researcher, [they] can thus be shared by others, and combined in more ways than one”. Moretti, however, does not share his data. Jockers occasionally publishes the results of data analysis, such as the five hundred themes developed from topic modeling, presented as word clouds on his website. But he does not provide the textual data analyzed, even at the level of word frequencies, and is significantly less open than Moretti about the composition of his datasets. Although in more recent work Jockers adjusts this approach somewhat, for research pertaining to Macroanalysis I have discovered only one instance in which he indicates the titles and authors investigated, and then, only for 106 of the total 3,346 works. These are identified almost incidentally, in reporting confusion matrices.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - In Morettis case, one might suppose it possible to reconstruct his datasets from cited sources. But his account (in an appendix to Graphs) of creating the dataset for “British novelistic genres, 1740–1900,” highlights why this is not feasible. There he describes his periodization as not always explicit” in the bibliographies, thus evincing the role of his own-unpublished and therefore unspecified-interpretive decisions in data construction. And even if Jockers listed all the titles and authors he analyzed, it would be impossible to reconstruct the basis of his arguments without access to the textual data he uses, which are not just texts of literary works but highly prepared—or preinterpreted—selections from those texts. Neither critics nor supporters of Morettis and Jockerss methods typically comment on this lack of published data. But far from an incidental oversight, this practice maintains the fiction that literary data are prior to interpretation: it removes the need either to describe the procedures for collecting, cleaning, and curating datasets or to expose the inevitably selective and limited collections resulting from that construction.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - The meaning derived from a literary-historical dataset-like the interpretation of a literary work—is shaped, profoundly, by the methodological and critical frameworks through which it is approached, and by the selections and amplifications those frameworks produce. Two scholars can read the same dataset—like the same literary work—and derive different meanings. While an independent observer may be more or less convinced by the different arguments, deciding between them depends upon access to the object on which they are based. In the absence of data publication, distant reading and macroanalysis are analogous to a scholar finding a set of documents in an archive or archives, transcribing them, analyzing those transcriptions, publishing the findings, and asserting that they demonstrate a definitive new perspective on the literary field, without enabling anyone to read the transcriptions (or in Jockerss case, without revealing the titles of most of the original documents).
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - As noted in the introduction, Moretti and Jockers have been highly influential in foregrounding data-rich models of literary systems as primary units of historical analysis. Yet in not recognizing the critical and constructed nature of the bibliographies and mass-digitized collections they use to create these models, neither author can benefit from the historical insights that underpin and are translated through this scholarly infrastructure. The resulting models can be used to address certain questions. But they are ultimately reductive: these models do not represent the historical existence of literary works, including the ways in which they connect to produce literary systems.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - Depending on the reliability of the source and the type of questions asked, datasets constructed on this basis can support insights into trends in new literary production. Jockerss study of Irish-American prose pursues an approach manifested in other digital projects—some of my own work included —of using publication data to test existing perspectives on literary history. Employing a dataset with the date of first publication, as well as “the geographic settings of the works, author gender, birthplace, age, and place of residence”, Jockers challenges the notion of a “lost generation of Irish-American authors from 1900 to 1930 and proposes a likely explanation for this misperception: a predominance of eastern male authors in the canon—and hence in critical assessments—of Irish-American literature. Morettis work on new literary production extends beyond testing and revising particular arguments in literary history and is highly innovative in this respect. His study of titles, for instance, investigates a category of literary data that had not, as far as I know, been subjected to synoptic, stylistic analysis previously. More broadly, Moretti combines multiple bibliographies to challenge claims about the distinctiveness of new literary production in different historical periods, as in his discussion in Graphs of gender trends in British novel publication.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - But literary works are not defined by a single time and place, and collecting them together in those abstract terms does not represent the interconnections that constitute literary systems. William St Clairs The Reading Nation in the Romantic Period aptly diagnoses the limitations of this approach. Like Moretti and Jockers, St Clair rejects what he calls the parade of authors” convention in literary history, where canonical authors file past the commentators box in chronological order, taken as representative of the historical period in which they wrote. But he equally dismisses the “parliament of texts” approach, where literary works first published at a particular time, and usually by authors of a particular nationality, are understood as “debating and negotiating with one another in a kind of open parliament with all the members participating and listening”. As St Clair notes, literary systems frequently include “texts written or compiled long ago and far away”, and some literary works are inevitably more widely published, circulated, read, and referenced than others.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - New domestic literary production, the basis of Morettis and Jockerss datasets, is only a subsection of the literature available at any time and place. By considering only that component—without accounting for its relative importance or acknowledging that literary systems encompass other types of works—Moretti and Jockers occlude major aspects of how literature existed in the past. The date of first book publication overlooks the differing availability of literary works in the years after they are published and that first book editions are not necessarily and for many periods are rarely the first time works are available. Some titles are never published as books, and many literary works—whether in book or other formats—are republished, sometimes on multiple occasions. Likewise, an authors nationality is a poor marker for the geographical existence of a literary work in a marketplace that has been globalized since at least the eighteenth century. More broadly, the construction of literary systems from the categories supplied by enumerative bibliographies—the title, the author, the date of publication, the publisher, and by association, the text-ignores the different titles, author names, dates and places of publication, and texts that occur as literary works are issued and reissued, and the implications of these differences for understanding production and reception.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - Even for studies that focus on new book publications, this approach to modeling literary systems ignores most differences between literary works, and hence, most dynamics of those systems. Reflecting on Morettis work, David A. Brewer notes that flattening the literary field nicely undoes the monumentalizing that so often accompanies the literary canon,” but at the expense of ignoring the varied profiles and presences of works in history. Brewer focuses on commercial success, arguing that the popularity of different literary works at the time they are published and in subsequent generations accords them a massively different footprint in history, altering their influence, and hence their meaning, for readers. But commercial success is not the only relevant factor. As textual scholars show by exploring the material and social dimensions of literary works, multiple issues shape their meaning, extending from the documentary forms they take to the relative positions and prestige of the individuals and institutions involved in producing them (authors, publishers, editors, illustrators, booksellers, advertisers) and the interconnected systems (economic, religious, educational, legal, geopolitical) in which they circulate.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - While textual scholars such as Johanna Drucker, Paul Eggert, and Jerome McGann thereby conceptualize literary works as events—unfolding over time and space and gaining different meanings in the relationships thereby formed-Moretti and Jockers construct literary systems as composed of singular and stable entities and imagine that this captures the complexity of such systems. In fact, because their datasets miss most historical connections between literary works, their analyses rely on basic features of new literary production to constitute both the literary phenomenon requiring explanation and the explanation for it. Macroanalysis purports to investigate the context in which [literary] change occurs, chiefly by analyzing words in nineteenth-century novels. What Jockers actually shows is the capacity of his computational method (a combination of stylistic analysis, topic modeling, and network analysis) to predict whether a work (or “bag of words” from that work) was by a man or woman, and its date of publication, genre, and national origin, from a corpus defined according to those parameters. Notwithstanding the variable accuracy of this approach for different categories, the methodological demonstration is impressive for extending stylistic analysis beyond small groups of documents.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - CENS sensor systems have been developed and deployed within a larger project that seeks to collect data in order to respond more effectively to environmental challenges. Higher-resolution data promise to create more effective models for predicting and managing environmental events. This “new mechanistic understanding of the environment” involves a near-future commitment to developing a critical infrastructure resource for society” in the form of detailed environmental monitoring. The promise to respond to crises more effectively develops not just through larger data sets but also through more extensive data gathering that is better tuned to detecting anomalies and extreme events, since most ecological data have largely consisted of documenting ecological conditions within a logic of averages and generalities.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - But the methodological achievement does not translate into historical insight because the study considers only an abstract amalgam of literary works. In reducing context to a few predetermined categories, Jockers is confined to stating their presence. He cannot offer any alternative influences, nor can he comment on the extent to which gender, nationality, and chronology shape literary history, except perhaps implicitly, in the proportions of titles misidentified by his models. The approach yields very general, and I would argue, self-evident statements. To give examples drawn from the conclusions to Jockerss various chapters: “the linguistic choices an author makes are, in some notable ways, dependent upon, or entailed by, their genre choices”; there are both national tendencies and extranational trends in the usage of . . . word clusters”; “a writers creativity is tempered and influenced by the past and the present, by literary ‘parents, and by a larger literary ecosystem; and “thematic and stylistic change does occur over time”. The generality of these conclusions is predetermined by the dematerialized and depopulated conception of influence underpinning the analysis. The model constitutes literary works as a system based on the date of (presumably first book) publication, and any book within the dataset is understood to exert influence in a chronologically discrete manner, regardless of the actual conduits of literary influence, which require availability to readers who buy, borrow, and sometimes write literary works. Because he is modeling a diffused and generalized system, the “influence” of gender, genre, temporality, and nationality is in turn diffuse and generalized.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - The inadequacy of this conception of literary systems is foregrounded when Moretti considers readers, who, as Anne DeWitt notes, “are both central to his argument and absent from his evidence”. Moretti takes literary data on publication and/or formal features of literary works as both expressive of and explicable by the actions of readers and the market. We can see this strategy in Morettis discussion of the first graph in Graphs: the rise of the novel” across a number of national contexts (Britain, Japan, Italy, Spain, and Nigeria) at different times. Leaving aside the question of whether his graph depicts the numbers he attributes to it, Moretti ascribes the leap “from five-ten new titles per year to one new novel per week to the horizon of novel-reading,” the shift in the market that occurs when the novel is transformed from “an unreliable commodity to that great modern oxymoron of the regular novelty: the unexpected that is produced with such efficiency and punctuality that readers become unable to do without it”. The argument makes intuitive sense, but it presumes that only-and all-new titles by authors of particular nations were available to, and read by, only—and all—readers of those nations. The explanation, in other words, claims that publication data are both indicative of national reading habits and explicable in terms of that activity.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - A similarly circuitous mode of argumentation characterizes “The Slaughterhouse of Literature chapter in Distant Reading. Moretti proposes a framework for canon formation, wherein readers are the butchers of literary history
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - who read novel A (but not B, C, D, E, F, G, H . . .) and so keep A “alive” into the next generation, when other readers may keep it alive into the following one, and so on until eventually A becomes canonized. Readers, not professors, make canons.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - Nominating formal choices as the reason readers select certain titles over others, Moretti employs the example of detective fiction and decodable clues to demonstrate this process. He identifies the presence of such clues in Arthur Conan Doyles fiction as the reason that author was progressively selected by generations of readers to attain his now canonical status. Again, this is an interesting but circular argument. Moretti acknowledges one of the ways in which his claims are tautological”: “if we search the archive for one device only, and no matter how significant it may be, all we will find are inferior versions of the device, because thats really all we are looking for”.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - Yet the same problem of assuming the shape of the past from that of the present—occurs at a larger scale in that Moretti assumes that authors who have a canonical status in the present were selected from the time of first publication. This argument is intrinsic to his evolutionary model, and while Moretti supports it by citing an empirical study, others show its falsity. St Clair, for instance, demonstrates the minute early nineteenth-century readerships of five of the big six Romantic male poets (excepting Byron) who form the contemporary canon: however that Romantic canon was formed, it was not based on the poetry contemporaneous readers preferred. While in the earlier study Moretti aligns publication with reading (a title was published; ergo it was read), in this instance his argument requires titles to be published but not read. What determines if titles were read is whether they had decodable clues; thus, once again, a feature of the data (the presence or absence of decodable clues) is used both to indicate and explain the activities of readers.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - Moretti has said, in defense of his method, that reducing literary works to one or two features is part of the “specific form of knowledge that distant reading provides: “fewer elements, hence a sharper sense of their overall interconnection. Shapes, relations, structures. Forms. Models”. My argument is not against reduction and abstraction per se. While especially obvious in data-rich studies (which rely on identifying attributes that can be represented in uniform fields), reduction and abstraction characterize all analysis. Close readings do not interpret literary works as a whole but specific, extracted instances of particular, abstracted features of those works. What I am arguing is lost in Morettis and Jockerss approach-especially in their definition of literary systems as analogous to first book publication by authors of a designated nationality—is precisely a historical sense of “interconnection. Failing to acknowledge that the disciplinary infrastructure they use is made not given, and thus overlooking the historical information embedded in it, Moretti and Jockers model literary systems in terms of potentially, and certainly relatively, abstract categories of production. In the process they ignore the socially, spatially, and temporally specific and complex ways in which literary works exist and relate to one another in particular, historical contexts.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - Although distant reading initially faced considerable resistance from literary scholars, now a common response to that paradigm is the call to integrate nondata- and data-based approaches. While Moretti originally suggested that distant reading should replace close reading, this integrated position is the one he subsequently adopted. Describing the contrast between close attention to the canon and distant exploration of the archive in terms of “too much polyphony on the one hand and too much monotony on the other, Moretti asserts, Its the Scylla and Charybdis of digital humanities. The day we establish an intelligible relationship between the two, a new literary landscape will come into being”. Other examples of this stance abound, including from Jockers and scholars such as Frederick Gibbs and Daniel Cohen, who argue for the profitability of “mov[ing] seamlessly between traditional and computational approaches” or between “our beloved, traditional close reading and untested, computer-enhanced distant reading”.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - This apparent moderation in the terms of debate belies the continuing perception of close and distant reading (or micro- and macroanalysis, or nondata- and data-based approaches) as opposites. Whether close reading is presented as less “rational” than distant reading, or more authentic and authoritative, or if together, the two perspectives are understood to supplement the others limitations, close and distant reading are conceived as antithetical in their assumptions and approaches. However, the main problems I have sought to diagnose in distant reading and macroanalysis a disregard for textual scholarship and an assumption that literary works are stable and singular entities—are ones they share with the New Criticism and its foundational method of close reading. Distant reading and macroanalysis take the core object and premise of the New Criticism-the decontextualized text as the source of all meaning-to a conclusion rendered more abstract and extreme by the number of texts under consideration.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - As is well known, the New Criticism was an early- to mid-twentieth-century movement that subordinated the historical and contextual (biographical, material, sociological) concerns of literary scholarship to the text itself. The critique of this movement is also well established, with the contextual focus in many subsequent forms of literary history-feminism, postcolonialism, New Historicism-explicitly rejecting the New Critical view of the text as a self-contained and self-referential aesthetic object. Despite the apparent demise of the New Criticism, the continuing centrality of close reading in literary studies, including literary history, and the rhetorical focus of such research on the text, perpetuates the earlier movements dismissal of textual scholarship. As Eggert, McGann, and others have observed, assuming that literary works are texts, and that texts are single, stable, and self-evident entities, dismisses the documentary records multiplicity, and with it the critical contributions of those endeavors-bibliography and scholarly editing-dedicated to investigating and understanding that multiplicity.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - Even as contemporary enactments of close reading often foreground context, the centrality and assumed singularity of the text, and the disassociation from the literary works complex historical existence this produces, can negatively impact the capacity of such analyses to investigate literary history. Mary Elizabeth Leighton and Lisa Surridge highlight this effect in critical discussion of Wilkie Collinss Moonstone. Describing the varied interpretations that contemporary critics offer of its meaning for nineteenth-century readers—from tale of “imperialist panic to critique of colonial domination-they note that all critics assume, first, that they are “reading the same text” as readers in the past and, second, that all past readers encountered the same text as each other. In fact, as Leighton and Surridge show, Moonstone “took on strikingly different forms—and hence different meanings in different markets, specifically in British and American serializations. In projecting textual singularity onto a historical period characterized by documentary multiplicity, the close readings these critics produce obscure the historical production and reception of this literary work even as they propose to emphasize that context.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - Notwithstanding such instances, close readings are generally protected from the abstraction inherent in the notion of the text by the knowledge infrastructure in which they are embedded, and by a focus on particular documents. Scholarly editions provide critics with carefully historicized texts for consideration; when one is not available, the standard practice of bibliographical referencing ties discussion of the supposedly singular text to a version of the work. Moreover, because a close reading inevitably analyzes a version, any discussion of the text is contextualized by the information about the works history contained in the material form that the critic assesses. Distant reading and macroanalysis do not benefit from such provisions or protections; to the contrary, as this chapter has argued, these approaches negate the interpretive nature of the disciplinary infrastructure they use, as well as their own role in constructing the meaning of the data they derive from it.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - While explicitly opposing close reading, the form of Morettis and Jockerss arguments mirrors the New Criticisms perception of the text as the source of all meaning, even as the text under consideration has expanded from a single version of a literary work to a version of bibliographical and textual data derived from multiple versions of literary works. Morettis investigations of readers based only on data relating to first book publications enacts this view of the text as inevitably containing all that is relevant to interpreting it. In treating the literary system as a dispersed linguistic field, Jockers takes the rhetoric of text to its ultimate conclusion, proposing a literary-historical world in which there are no structures beyond the textual. Signals” of gender, genre, or nationality, comprised entirely of word frequencies, are substituted for gender, genre, or nationality as historical and cultural constructs. Far from the opposite of close reading, the dematerialized and depopulated understanding of literature in Jockerss work enacts the New Criticisms neglect of context.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - In this way, processes of filtering, aggregating, and selecting have already been put in place to turn sense data into relevant information. At the same time, these filters may not always capture intended phenomena. A researcher walking through the James Reserve forest might create noise that is picked up on sonic booms, which through algorithmic parsing activates cameras to record activity. In this field of environmental sensing, researchers might fall within the data event-space of motion detection, but inaudible birds traveling in a different column of air might not be detected.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - Whether literary histories are conducted in traditional or data-rich forms, the outcomes of analysis are inevitably tied to the object analyzed. When a gap exists between the contemporary object assessed and the historical object it supposedly represents—and when the critic is unaware or dismissive of that gap-no degree of nuance or care in the reading can supply that historical meaning. Herein lies the fundamental problem with proposing to integrate close and distant reading as the obvious way forward for research in literary history. Understood in terms of the different perspectives the two approaches offer, this strategy seems eminently sensible: data-rich analysis has the potential to explore large-scale patterns and connections in ways that nondata-rich research cannot; likewise, conventional textual analysis can provide insights into the meaning of literary works that quantitative studies cannot.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - Yet in couching debate about the role of data purely in terms of method, this response maintains the focus on the mode of analysis employed and conceals the lack of an adequately historicized object to analyze. What data-rich literary history needs is an object capable of representing literary systems—as manifestations of literary works that existed and generated meaning in relation to each other in the past—while managing the documentary records complexity, especially as it is manifested in new digital knowledge infrastructure. The lack of such an object, not the fundamental opposition of data and literature, is the real reason it has proven so difficult, in practice if not in theory, to integrate “traditional and computational methods for the purposes of historical investigation.
[Author: Katherine Bode; From essay:"A World of Fiction Digital Collections and the Future of Literary History "] - An appropriately historicized representation of relationships between literary works in the past would avoid the problems I have identified in Morettis and Jockerss approach, which claims to represent everything—directly, comprehensively, and objectively-while exploring only a limited part of any given literary system. The difficulties with their approach are not the result of using data to investigate past literature. They occur because distant reading and macroanalysis adopt and perpetuate the disregard for textual scholarship foundational to the New Criticism, without benefiting from the institutional and infrastructural protections afforded to close reading. Given this source of the problem, the next chapter proposes a solution from textual scholarship. I argue that the fields foundational technology of the scholarly edition supplies both the supports and constraints necessary for data-rich literary history, while providing a framework capable of extending the insights gained from engagements with emerging digital infrastructure to the broader discipline.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - SURROUNDED BY THE San Bernardino National Forest and situated within the San Jacinto Mountain Range in California, there is one particular patch of woods that is distinct in its ecological processes. This forest is equipped with embedded network sensing that digitally detects and processes environmental phenomena, from microclimates to light patterns, moisture levels and CO₂ respiration in soils, as well as the phenology, or seasonal timings, of bluebirds and auditory signatures of woodpeckers. These multiple modes of experimental forest observation are part of a test site for studying sensors in situ. A “remote sensing lab, the University of California James Reserve is an ecological study area that has hosted field experiments since 1966. The use of this ecological study area to test electronic sensors developed through the Center for Embedded Networked Sensing (CENS) research project is at once a continuation of experimental ecological practices in this area, as well as a shift in the technologies and practices for studying environmental processes. The question that arises here is: When the ecological experiment changes, how do experiences also change?
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - This notion of experimenting and experiencing as springing from a shared modality is put forward by Stengers in her discussion of Whitehead, where she uses “a (French-inspired) neologism” that does not draw a “clear distinction between the terms ‘experience and experiment as there is in English.” This merging of terms is also a critical way for describing the speculative approach of Whitehead, which might be characterized by a crossing-over of experience and experiment, where experimenter and experiment are part of a unified and concrete occasion. This point of entry is important for this discussion, as it immediately points toward a consideration of sensors not as instruments sensing something “out there” but rather as devices for making present and interpretable distinct types of ecological processes. These processes are articulated computationally, and they draw together a wide range of experiencing entities that begin to in-form new arrangements of environmental sensing. The becoming environmental of computation extends to the experiencing entities that sense and express ecological processes.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - The use of wireless sensor networks to study environmental phenomena is an increasingly prevalent practice. Sensing projects encompass studies of seismic activity, the health of forests, maps of contaminant flow, and the tracking of organisms from dragonflies and turtles to seals and elephants. These projects generate sensor data that are meant to provide greater insights into environmental processes. At a time when ubiquitous computing is extending to multiple aspects of everyday life, where the Internet of Things promises to have your refrigerator communicating with supermarkets, and smart city designs propose to harvest your location data to ensure your roast-chicken dinner is prepared on time, sensing environments for ecological study is just one set of practices within a larger project of programming environments through distributed modes of computation. Sensor networks arranged over static and mobile platforms and widely distributed throughout environments are the common thread throughout these projects, but the deployment of sensors within ecological study sites has been one of the key and ongoing areas for early sensor research and development.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - Processes of producing data are also processes of making sense: the experiment is generative of modes of experience. These processes include how sensors are developed in the lab, tested in the field by technologists and scientists, merged with historic ecological study practices, and read across new data sets, while also producing distinct insights into ecological relationships by connecting up multiple experiencing subjects. The architectures and algorithmic processes for relating sense data are a critical part of how sensor systems operate. They articulate how sense data will come together into arrangements indicative of environmental and planetary processes.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - As discussed in the introduction to this study, although a range of research has been conducted on ubiquitous computing and the Internet of Things, less has been written in the context of digital media theory or science and technology studies about the ways in which understandings and practices of environmental science have shifted through sensor systems and how these shifts have also had ongoing effects on more “participatory” sensor projects. While sensors and sensor systems were initially developed for use in military contexts, wireless and embedded sensor systems have further developed through ecological study, which has in turn provided an additional basis for deploying sensor systems within social media and citizen-sensing contexts. This chapter focuses on the use of sensors for study in environmental science in order to consider how these science-based sensing practices might influence practices in expanded areas such as citizen sensing.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - Situated within the context of these ubiquitous computing developments, this chapter specifically focuses on the distinct forms of sensing that concresce in relation to the monitoring of environmental phenomena. One key advantage that sensor systems are meant to provide is the ability to understand the complex interactions and relations within ecosystems in greater detail. Ecological relations are meant to appear in higher resolution because sensors monitor and make available aspects of environmental processes as they unfold over time rather than as more discrete moments—and because more data are available for generating models of complex interactions. This study asks how the ecologies that materialize through more continual sensor observation are not simply the result of increased data output and processing, but might also be understood as generative sensory relations articulated across humans, more-than-humans, environments, and devices. In what ways do distributed sensor technologies contribute to new sensory processes by shifting the relations, entities, occasions, and interpretive registers of sensing? How do the interpretative practices that are individuated in experimental environmental sensing test sites in-form attention to environmental problems? And what are the implications of these experimental environmental sensing arrangements as they migrate into policy and influence participatory sensing processes?
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - In order to consider these questions, I first give an overview of the increasing use of sensors for monitoring environments and studying environmental change. The generation of more and higher-quality data is seen as critical to developing more advanced insights into how environments are transforming, and so the sense data produced through these projects are often gathered for the purposes of advising science and policy, in addition to testing prototype computational technologies in the field. Environmental monitoring can bring with it a sense of increased responsibility; and the commonly used phrase, “all eyes on earth,” is a way of articulating the watchful concern that sensors embody and operationalize through the continual observation of environmental processes.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - But sensors connect up more than just a network of human- or sensor-based eyes. This chapter draws on more-than-human theory to move beyond human-centric interpretations of computational sensing technology and engages with Whiteheads approach to experience as something that concresces across human and more-than-human subjects. As Whitehead suggests, perceiving subjects are neither exclusively human nor pregiven, but combine as feeling entities through actual occasions. In this way, sensors might also be understood not as detecting substantialist external phenomena but as contributing to inventive processes for making interpretive acts of sensation possible—and for articulating environmental change and matters of concern. This is a way of saying that interpretation matters, and that experience to be interpreted concresces across multiple registers and entities. In addition, interpretation is integral to processes whereby things come to take hold as objects of relevance.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - Based on a consideration of the distinct articulations of sense across more-than-human and environmental processes, this chapter moves to focus specifically on the use of embedded networked sensors at the James Reserve ecological study site. Drawing on fieldwork carried out at this site where sensors were tested in situ, as well as a review of scientific papers and online records of sensor data, I discuss new formations of distributed sense that concresce through these experimental forms of environmental sensing. Part of the way in which sensors might be understood as operative within distinct registers of experience is as distributed computational technologies. Sensors are distributed in at least two ways: in terms of their spatial distribution, by monitoring environments in a widespread and localized way; and in terms of the distributions of experience that generate sense data and interpretations. If we take seriously Whiteheads suggestion that sensing entities concresce through experiences (or prehensions) and that they are inseparable from occasions of experience, then how do experimental environmental sensor arrangements mobilize distinct sensing practices that are creative of new environmental abstractions and entities?
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - As Whitehead suggests, abstractions are not separate from concrete things, but rather influence “the process of concrescence” and provide a “lure for feeling. The concrescences that come together here might be understood not just as scientists-devices-flora-and-fauna but also as relations that individuate and are individuated through data sets and algorithmic processes, across sedimented environmental effects, and through responsive modes of environmental action. The coming together of an experiment presents the possibility for distinct experiences and subjects to concresce. Sensing an experimental forest is not about detecting information “out there” but about “tuning” the subjects and conditions of experience to new registers of becoming. Tuning is a way to describe the co-creation and individuation of agencies within experiments and the complex process of developing facts or matters of concern within such experiments. This chapter sets out to provide an understanding of the dynamic, distributed, and multiple modes of computational sensing environments that might also provide insights for more “cosmopolitical” participation, where sensing is a process of multidirectional tuning and experiencing. The becoming environmental of sensor-based media is then distributed to include multiple subjects, organisms and technologies, as they process their environments.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - The use of instrumentation within ecological study, from bird ringing to anemometers, has a longer history than the more recent use of networked sensor systems. However, the miniaturization and faster processing speeds of sensors have contributed to their increasing use as instruments within ecological study. Sensor systems—composed of relatively small-scale in situ sensors and actuators that are able to collect and transmit data through networked connections, as well as undergo remote reprogramming—have been described as nothing less than another “revolution comparable to the rise of the Internet. These imagined and actual transformations involve extending computational capacities to environments through sensors, where objects and phenomena are transformed into sensor data and made manageable through those same computational architectures.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - In related literature, sensor networks have also been described as a revolution in scientific instrumentation, similar to the telescope and microscope, where a new order of insights might be realized. But instead of probing outer or inner space, sensor networks operate as “macroscopes,” which enable a new way “to perceive complex interactions” through the high density and resolution of temporal and spatial monitoring data. While issues related to providing a reliable power source, ensuring the robustness of hardware, and maintaining the validity and manageability of large data sets remain, sensor systems present the possibility for understanding environmental processes and relations more thoroughly by providing real-time data that are more detailed than existing modes of data collection, including remotely sensed and manually gathered data that may exist at a much larger scale or more discrete moments in time. The hope is that a background of new and undiscovered relations may be connected up and made evident through these sensing devices.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - A wide range and number of projects now employ sensors for environmental monitoring, from bird migration and nesting to the social life of badgers, to water quality monitoring, phenological observations, the acoustic sampling of volcanic eruptions, and the monitoring of microclimates in redwood forests. One of the key projects within sensor-systems development—a 2003 study of Leachs Storm Petrels at Great Duck Island, a wildlife preserve in Maine—established that “habitat and environmental monitoring is a driving application for wireless sensor networks.” This sensor project employed static sensor nodes and patches, with burrow motes” and “weather motes”—or sensor nodes—to study the underground nesting patterns of migrating birds.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - As with many similar and subsequent sensor deployments, this project produced more detailed data on previously unobserved ecological phenomena and relationships while also providing a test-bed for experimenting with the system architecture of sensor networks. The ecological relationships observed—or sensed—are in many ways coupled with the capacities of sensor networks, which similarly are adapted to and “learn” from the processes under study. The “tuning” of sensor networks may take place not just between scientists and devices but also across devices, code, and ecological processes. In this way, sensors become environmental by tuning in and developing along with the phenomena and organisms under study.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - At the same time that sensor observations are intended to provide more detailed accounts of environmental phenomena on the ground, they also contribute to the building up of multiscalar and widely distributed approaches to environmental sensing, including remote sensing by satellites and airborne observations. These data are often generated across scales and derived from diverse modes of sensor input for wider and more detailed views on environmental processes and to study the effects and possible impacts of environmental change. Multiple “observatories,” together with long-term ecological research sites (LTERs), and the U.S. National Ecological Observatory Network (NEON), attempt to collect and synthesize sensor data across the United States. While a site-specific sensor project may study the detailed relationship between birds nesting behavior in relation to microclimate and multiple other environmental factors, this same study may benefit from climate data resources or may contribute to climate monitoring programs. In other words, the sense data gathered may have the potential to elucidate environmental relations within a particular area of study, as well as across expanded and yet-to-be-gathered data sets as long as the data to be compared are of compatible formats.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - Just as sensing systems are proliferating, numerous attempts are underway to amalgamate and make sense of the many forms of data—a key “cyberinfrastructure” task—since the multiple formats and provenances of data may mean that they are rendered meaningless for ongoing use and study if not consistently handled. Sensor-gathered data sets, which are typically “heterogeneous,” are increasingly brought together not only in larger data networks but also in mapping platforms where fine-grain sensor data provide a real-time “ground-truth” to coarser remote-sensing and field-gathered data. From Microsofts SenseWeb to the former DIY-sensing platform Cosm, such platforms intend to consolidate environmental sensor inputs. The range of possible sensor inputs is illustrated by one Microsoft diagram, “Instrumenting the Earth,” which outlines twenty different modes of sensor input, from snow hydrology and avalanche probes to citizen-supplied observations and weather stations. Innumerable potential points and processes in the environment become the basis for sensor input, and it is from these delineated sites of input that newly observed relations might be studied, articulated, or managed.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - Sensing in the James Reserve is distributed not only across this experimental site (and at distinct locations for the study of ecological processes) but also across larger sensor networks. Many of the CENS James Reserve sensors measure phenomena over time, which is meant to enable researchers to study sequences of data that are fine-grained and relatively continuous in comparison to more discrete data sets, with data captures taking place in localized settings as frequently as every fifteen minutes or more. Still other sensor test beds are in place that connect up to larger networks, including national observatories such as NEON. Observations are successively gathered and joined up in far-reaching networks, so that sense data becomes an amalgamated and comparative networked infrastructure of ecological observatories for studying environments and environmental change.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - While these sensing projects and networks have been under development within universities and public institutions, technology companies working individually or often in collaboration with universities are also developing multiple sensor network systems for environmental observation. These projects range from Nokias “Sensor Planet” to IBMs “A Smarter Planet,” HP Labs’“Central Nervous System for the Earth” (CeNSE), and Cisco’s “Planetary Skin” (in collaboration with NASA, the University of Minnesota, Imperial College, and others). Governments and their militaries are also investing in the development of sensor networks, with white papers and research issuing from the EU, China, and the United States DARPA, among others. Many of these sensing projects raise ethical issues related to surveillance, while still other projects are enabling new forms of resource exploitation. The project of monitoring and managing environmental relationships continues to be a way in which the governmentality—and even environmentality—of sensor systems unfolds, where sensor capacities may point toward particular relations to manage or sustain in distinct ways.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - All together, these environmental sensing systems variously undertake a project of instrumenting or programming the earth. Within a sensor-ecology imaginary, the planet might be understood as an entity to be sensed and transformed into data. Improved sensing capabilities are critical to advancing understandings of environmental change while also indicating ways of acting (whether through automated systems or environmental policy) in response to that data. With small-scale, distributed, and pervasive computation embedded in environments, new relationships emerge not just to studying but also to managing environments, since sensor systems computationally describe and capture environmental processes while also providing the promise to design and control these complex systems.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - In many ways, the notion here is that increased amounts of environmental data allow for the improved management of environments. Data are descriptive indicators capturing environmental processes. But from a Whitehead-influenced perspective, it could be argued that sense data are less descriptive simply of pre-existing conditions and more productive of new environments, entities, and occasions of sense that come to stabilize as environmental conditions of concern. The ways in which phenomena are tuned into as sense data are one part of this operation of the becoming environmental of computational sensors; but the ways in which sensory monitoring gives rise to new formations of sense within and through data, computational networks, humans, more-than-humans, and environments also in-form distinct sensing practices. Since sensor networks offer distinct insights into the complex interactions and processes within environments, then the ways in which these relationships are joined up, articulated, and transformed into new observational capacities also matters.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - Turning now to a more detailed discussion of one embedded sensor network project, the CENS sensor installations at the James Reserve forest, I consider how the rise of distributed sensing might be looked at more closely in the context of this experimental project and test site. The CENS initiative is one of many sensor developments as discussed previously, and it is a well-known and frequently cited project for sensor research. Established in 2002 as a National Science Foundation Science and Technology Center, the CENS project was a collaboration between several California-based universities. The project, which finished in 2012, focused on four key areas of research: Terrestrial Ecology Observing Systems (TEOS), Contaminant Transport and Management, Aquatic Microbial Observing Systems, and Seismology. A fifth area of research, Participatory Sensing, grew out of the project research into ecology and focused on how sensor applications may be used for citizen engagement in environmental and social issues. This discussion focuses on the TEOS sensing deployments, which were primarily situated at the James Reserve (while the other study areas were located in a diverse range of sites). Participatory Sensing is a further project research area that I briefly address in the conclusion to this chapter.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - The James Reserve ecological study site is in many ways an environment for developing experimental practices as well as for transporting laboratory techniques into the “wild.” The fieldwork that I conducted at the James Reserve also moved from the laboratory to the field, as I first visited the CENS laboratory at UCLA where most of the sensor prototypes were developed, and then observed the sensors at work in situ at James Reserve. I held informal interviews with researchers involved in the CENS project, mapped the different locations and functions of sensors in the field at James Reserve, and compared the online records of sense data with the sites where sensors were installed. However, this is not a project of following the scientists, which is by now a well-established area within science and technology studies. Instead, through a discussion of fieldwork conducted at the site, I attempt to understand processes and sites of sensing as they intersect with ecological practice and cultures of computation. Rather than focus exclusively on how ecologists use sensors to obtain scientific meaning or generate data or facts, I concentrate on James Reserve as a particular ecological research site that concresces through a distribution of sensing processes across organisms, ecological processes, and sensing technologies in the form of computational hardware and software, online interfaces, conservation infrastructures, resident scientists, environmental change, citizen scientists, publics, and visiting researchers. In other words, I attend to the becoming environmental of sensor-based media as a concrescence of these experiencing entities.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - The nearly 12-hectare and 1,640-meter-high site is characterized by a complex intersection of ecosystems, “including montane mixed conifer and oak forest, montane chaparral, wet and dry meadows, montane riparian forest, a perennial stream, and an artificial lake.” Since James Reserve is located in a relatively remote wilderness setting, it is effectively “off the grid,” and is a study area that generates its own solar power and has its own well for water. In this sensing lab or experimental forest, infrastructures are realigned, not as obvious allocations of roads, electricity, and water, but rather as new arrangements of energy, sensation, and observation.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - However, data expressive of average conditions do not capture the effects that major if singular disruptive events have on environments and rapidly shifting ecological relations and processes. CENS and related projects such as NEON are oriented toward the objectives of monitoring changing environmental processes, where an increasing number of disturbance events are contributing to the perceived need to develop different practices and technologies for sensing environments. The expression and agitation of environments (which, as Whitehead suggests, “seep” into all things) also turn up in and transform the sensing practices and technologies that monitor them. Instruments for capturing sense data are here specifically honed toward disturbance, since environmental change becomes more of a matter of concern within ecological study. At the same time, disturbance detection rather than observation of norms begins to influence what counts as relevant sense data.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - The sensors at work in the James Reserve within the TEOS group of research projects consisted of everything from soil sensors that detected moisture levels, a Rhizotron installation of tubes that allowed robotic cameras to capture images of root growth and CO2 sensors at three different soil depths to estimate soil flux, a bird-audio system involving sonic booms triggered by camera activity to capture woodpecker auditory data, weather stations for gauging microclimatic conditions, tree-sap flow sensor systems, nest boxes with cameras and audio installed within bird boxes, pan-tilt-zoom tower cameras on thirty-foot-tall poles, and a Moss Cam web camera. At the time of this fieldwork, there were over 550 connected and untethered sensor nodes, as well as reconfigurable robotic mobile sensors working above and below ground, within waterways and across tree canopies, capturing data on plants, animals, birds, soil, microclimate, and more. Sensor observations provided the ability to observe fungal growth patterns, soil CO2 production, the times at which plants shut down their CO₂ fixing, and all manner of activity that typically takes place outside the scope of direct human observation.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - The initial proposal for this project made a bid to develop “distributed sensor/actuator networks [that] will enable continual spatially-dense observation (and ultimately, manipulation) of biological, environmental, and artificial systems.” Midway through the project, many of the initial proposals for comprehensively distributing a large number of small sensors within an area of study shifted to a practice of strategically deploying sensors in precise locations to study specific ecological activities and to develop a hierarchy of sensing platforms that could span from small-scale motes to larger sensors such as imaging robots on cables.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - The sensor practices and arrangements developed in the James Reserve context were specific responses to site conditions and processes, so that phenomena to be observed in-formed which sensors would be used and how. At the same time, the difficulty of creating a pervasive sensor network led to a focus on specific sites of study as a more feasible test of the technology. This points to a key aspect of the sensor systems: they were almost always physically proximate to that which they monitored. Sensors were distributed in the environment, and networks were developed and paired with those environments. Sensors in the field at James Reserve were wrapped around tree trunks in a loop of foil and cables; they were interspersed in the ground as arrays at regular intervals; and they were clustered at bird boxes to cross-correlate microclimate in relation to nesting at distinct locations.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - The ways in which sensors were paired with environments was not a simple mirroring, however. Sensors proximate to roots and soil, for instance, did not stream all possible data all the time. Instead, sensor motes within a network talked to each other to coordinate data detected, processed, and sent according to distinct algorithms. Part of this configuration had to do with energy efficiency, where motes were triggered to record events only at select times and were turned off during times of inactivity to save energy. Indeed, a key aspect of imagining the possibilities of sensors as environmental systems involved thinking through how it may be possible to realize “pervasive sensing” without “pervasive infrastructure, which primarily meant not requiring a central electrical grid for power. The sensors at James Reserve were in part powered by a solar array that was the primary source of energy to power this elaborate sensing lab, which was supplemented by batteries, including motorcycle batteries, for distinct devices to transmit their sensory data via wireless and networked connections.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - Part of the algorithmic processing of sensor data involved setting sensors to pick up, filter, and amalgamate data within established ranges. The processing that sensors undertook was ad hoc and in situ, rather than a continual capturing and streaming of environmental activity. Each mote within a network was already set to detect some things and not others, to make correlations among certain data criteria, and to discard anomalies and redundancies according to predetermined phenomenal ranges. Sensor motes detected events within a specific range, and processed and communicated this data across short distances or hops to other sensors within the network for collection at sensor nodes. Data were typically fused and processed at each individual mote in order to make real-time streaming more efficient and effective.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - While sensors were physically proximate to what they sensed, that which was sensed and communicated traveled through channels of algorithmic detection and processing. While sensor applications are intended to record extreme events and anomalies, the algorithms that capture data have a tendency to smooth and fuse data at the source in order to conserve energy and generate manageable quantities of data, which even with these filtering mechanisms can easily run to several million records per year per sensor patch. These syntheses are intended to turn data into “high-level information,” where the multitude of records and raw data transform into something like observations or experience. This transformation required “data reduction” in the form of “in-network processing” that aggregated similar data and filtered redundant data. As CENS researchers Jeremy Elson and Deborah Estrin write,
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - For example, emerging designs allow users to task the network with a high-level query such as notify me when a large region experiences a temperature over 100 degrees” or “report the location where the following bird call is heard.”
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - Inevitably, the focus on gathering massive amounts of sense data raises issues related to data ontologies. Sensor networks provide the basis for monitoring and acting upon environments, and yet the data and connections made across sensors are selectively captured and joined up, and are also subject to failure and incompatibility of data. Different data standards, classification techniques, and dispersed practices in-form the content and processing of dataspaces. Databases and dataspaces are more than collections of objectively observable facts—they are embedded within and performed through infrastructures of science, governance, and public outreach.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - On the one hand, there are issues related to how an entity becomes data, as Wolff-Michael Roth and G. Michael Bowen have discussed in relation to the digitization of lizards. On the other hand, there are questions about what constitutes data (a lizard may seem to be a clear artifact of digitization; but when its habits and habitat become part of the sensed data, where does the organism begin and the environment leave off?). Data ontologies in-form which data are collected, but they also in-form possibilities of sense by giving rise to new actual entities and occasions for articulating and experiencing relevant sense data.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - In order to create a more effective parsing of environmental phenomena, sensors are not just used as individual devices that simply generate discrete sense data at the James Reserve. Instead, multiple sensors and sense criteria within a sensor network are often also brought together to form a composite picture of a distinct environment under study. Chemical analysis of pollution may provide readings on contaminant concentration levels, but additional sensors may also work out the direction and speed of contaminant travel, as well as the size of an affected area, by cross-correlating multiple sensor data. In this process of data fusion, the “system is the sensor.” Sensors working together within a network establish a computational pattern of correspondences, where the physical sighting, sensor type, coding, and correlating of data coalesce into an environment of sensor data that in-form observations. When the system is the sensor” and the network operates as a sort of distributed instrument, it might be possible to create models and forecasts of ecological processes and, through these sensor systems, act upon environments.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - Sensor systems may also act as proxies for the environments they sense. Sensors as proxies are not standing in for a more-real version of environments, but rather are sensory operations that mobilize environments in distinct ways. Sensor networks perform—and so transform—environmental systems. Data may be correlated across sensor types, or sensors may trigger other sensors to capture phenomena, or trigger actuators to collect samples for later study. Inferences can be made about phenomena through sensors and actuators, and sensors can be arranged through flexible, multiscalar platforms that investigate particular sensing relationships.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - As a CENS “Distributed Sensing Systems” white paper notes, “Embedded sensing can involve a mix of observations with inherently different characteristics. For instance, it is common for systems to include multiple sensors, each with a different form of sensory perception or modality. This is the case in James Reserve, where seemingly traditional image and audio technologies provide a new way to “sense” phenomena in the absence of direct biological sensors. While the majority of sensors now available are capable of detecting physical and chemical attributes, devices such as cameras become newly deployed as biological sensors in the absence of direct biological sensing capabilities, where physical and chemical sensors algorithmically set to filter for event detection can automatically trigger cameras to record biological events. Imager and audio modes of sensing are activated within a computational network that mobilizes these forms of sensing as distinct and often proxy operations. The possibility to articulate relationships and interactions within environments to a higher fidelity is something that is meant to be generated through sensor applications that join up environments across sensor system hardware, software, databases, and cyberinfrastructures, as well as distinct sites and the more-than-human processes.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - Proxy modes of sensing do not just extend to sensors triggering other sensors or actuators to perform sensing operations but also include proxies that become apparent vis-à-vis more-than-human processes. A not-uncommon technique within environmental study, where climate change in deep time may be studied through ice cores as proxies for past climate events, proxies within sensor-based environmental monitoring are mobilized to infer and detect traces of ecological processes. In the James Reserve, for instance, phenology is a central area of study. In order to capture seasonal relationships, organisms may be observed for the ways in which they “process” environments.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - The perceptive capacities of Violet-Green Swallows and Western Bluebirds, in addition to Star Moss and other organisms, are placed under observation through webcams and Cyclops networked image sensors, which capture images and data related to these organisms often at least every fifteen minutes per day, if not more frequently. The bird cams and Moss Cam, or web camera specifically monitoring the growth of Star Moss, generate a store of image data that can be compared to microlocal temperature and related data, as well as data captured throughout the James Reserve site. The birds choice of a nesting location, or the failure to raise chicks due to absence of food or low temperatures, can be captured in this context where the birds activities are made available as a sort of proxy sensor of phenological processes. Birds may provide key environmental sense data through computational networks that make sensible these registers of more-than-human experience. What is clear is that sensors do not just capture data, they shift the processes of sense across these multiple registers, so that more-than-human perceptive processes concresce in newly relevant arrangements.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - This study provides the following contributions: (1) this study conducted a large-scale survey, and the results suggested that visual memories of the public about movie characters may vary according to the gender of the character; (2) this study proposed an image analysis system that examines the frames of movies to find eight indices of how characters of each gender are portrayed visually; and (3) the implemented system analyzed 40 commercial movies and provided evidence of gender representation biases that were present in image frames of movies.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - Similarly, the Moss Cam generates images and daily records that contribute to a picture of seasonal patterns and “event effects.” These effects might include lack of moisture in the summer, which contributes to mosses “burning through” their CO₂ reserves—in other words, higher temperatures can correlate to an increased release of CO₂ by mosses, as they consume stored energy and move toward states of dehydration and dormancy. Here, what counts as “sensing” is not a simple matter of observing mosses through a web camera over time, but instead involves observing how the moss is a sensor, or a biomonitor that is itself detecting and responding to changes in the environment. The mosses morphological changes to local conditions are an expression of an ecological relationship that is further entangled in the complex shifts of climate change. In this respect, the mosses may be expressing sensory responses to human-altered worlds, yet to understand more fully what those alterations involve, it is necessary to observe sensing organisms in order to register the effects of increasing carbon and temperatures. The delay and resonance within these environments is not as immediate as a typical sensory example might assume. Yet in this study, the ways in which sensing organisms “take account” of environments multiply, where the sensory input and means of detection are distributed and computational. The becoming environmental of computational media then further takes place through organisms and their processing of environments.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - In a sensor-based study of phenology, sense operations are distributed and collaborative. In these forms of collaborative sense, sensors experience and provide proxy experiences across a sensing system that generates distinct occasions of sense. But these collaborative qualities of sense concresce not through researchers primarily but through the dynamic responses of organisms to environments and the sensors that collect data in relation to which algorithms query, filter, and record these changes. The more dynamic sensory modalities that concresce in this relationship are examples of inventive ecological experiences and subject-superjects, as discussed earlier. The timings at which plants leaf out, for instance, might even begin to disrupt and alter scientific models that expect seasonal timings to unfold at times established through prior empirical study. In these encounters and formations of sensory practice across organisms, ontologically prior categories of sense become more mutable and ontogenetic, where more-than-human modalities of sense indicate the shifting encounters of sense in which we are engaged. Sensor systems mobilize multilocated and multispecies processes of sensing, which in part enable the development of distinct capacities to sense change, where the scope of computational sensing and proxy sensing expands to include more-than-technological perceptual processes.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - In an account of ubiquitous computing as distributed cognition, Hayles suggests that distributed computation could operate as machines for aiding, and so enhancing, human perception. Here, however, computational devices are not augmenting human perception as such, and humans are not even the central perceptual processors toward which distributed sensation and computation might be directed. More-than-human proxy sensing points to the ways in which sensor technologies, instead of providing supersensing or cognizing capabilities to supplement human modalities, filter, connect up, and in-form environmental relations in distinct ways, and so change what modes of sense humans may even experience. New ecological arrangements of subjects—and superjects—concresce through these sensory processes.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - Environmental monitoring through sensor networks is a practice of making—and not just capturing—environments as process. Sensor networks are tuned to distributions of relations. They tune into discrete sense criteria and amalgamate these across sensor networks and through proxy modes of sensing to make particular environmental relations more evident and sensible. Environmental monitoring through sensory networks mobilizes and concretizes environments in distinct ways by localizing computational processes of sensing within environments and across more-than-human experiences while also articulating those relations through algorithmic processes for parsing data. As these processes inevitably compose the possibility of sensing environments in particular ways, they also in-form which participants and participatory modes of sensing register in the perceptive processes of sensor technologies. Such sensing practices, moreover, are replete with political effects. Within the context of sensor networks, the sensory arrangements that are identified within data may become the basis for identifying and protecting matters of concern. Yet they might also overlook those “non-sensuous” background events that may still generate new sensing arrangements but which are not interpretable within present modes of sense data.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - As discussed in the introduction to this study, the initial developments of ubiquitous computing are often attributed to Mark Weisers 1991 suggestion for computation to move from desktops to the environment, so that computational processes would become a more integrated and invisible part of everyday life. Yet another possible reference point could be Alan Turings 1948 ruminations on how to build intelligent machinery” with sensing capacities on par with humans. Turing reviews the options for such a project, first considering how to atomize every part of the human sensing ensemble and replace it with equivalent machinery.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - Emulating human vision, speech, hearing, and mobility, such a contraption “would include television cameras, microphones, loudspeakers, wheels and ‘handling servo-mechanisms as well as some sort of ‘electronic brain.’” This project would inevitably be “of immense size,” Turing notes, “even if the brain part were stationary and controlled the body from a distance.” But data would not enter the thinking machine through its remaining static, and so “in order that the machine should have a chance of finding things out for itself it should be allowed to roam the countryside.” But in such a scenario “the danger to the ordinary citizen would be serious.” Add to this the hazards of such a machine taking up all of the usual activities of human interest, and this contraption would be altogether unwieldy. Turings more practical recommendation is to behead the body, to work with the brain as the critical site of processing, and later attend to the sensory apparatus as a secondary concern.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - Even if Turings proposal does consolidate the “thinking machine” into a central and seemingly Cartesian apparatus, his thought experiment on the sensing body in pieces and distributed throughout the countryside remains a potent figure for ubiquitous computing. What is striking about Turings example is the way in which the thinking machine, even when distributed, would emulate the human body, which serves as a template for understanding how sensory data would be captured and centrally computed. While computational sensing technology can now be understood as more than a double of or prosthesis for human sensing, Turings figure of the body in pieces raises questions about how particular distributions of sense might reconfigure environments and processes of sensation.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - Could such distributions of sense point toward modes of sensation where computation reassembles not as a singular sensing subject but rather as a processual and multilocated experience comprised of numerous sensing entities? How are sensing practices individuated, and how do they concresce, across potential sensor networks? In this way, sensing also assembles not as a mental or cognitive operation but as an environmental and relational articulation across multiple bodies and sites of sensing. Within Turings example of the sensing body in pieces, this could mean that we attend not to how the body might reassemble toward human perception and functionality but rather to how the “countryside” and the many inhabitants, processes, and processors of this distributed and distributive milieu begin to rework how the thinking-sensing machine captures, configures, and acts upon its inputs.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - Turings sensing apparatus points to the distributed processes that make sensing possible, even if the sites of sensation do not return to a coherent human processor. Indeed, as Whitehead suggests, perception might be understood to be in the world and distributed through more-than-human processes—it is not the special preserve of a human decoding subject. Instead, multiple participants express and unfold a distinct experience of the world, independently but contemporaneously within an immanent series of events. At the same time, the excitations of environments are fused to all modes of “matter,” where “the environment with its peculiarities seeps into the group-agitations which we term matter, and the group-agitations extend their character to the environment.”
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - “There are numberless living things,” Whitehead writes, that “show every sign of taking account of their environment.” This taking account of environments is a way of capturing what is relevant, and—through being affected—of transforming environments and relations.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - Sense data might be seen as a concrescence of multiple ways of taking account of environments, whether through researchers or devices or environmental events. But these data are necessarily articulations of the ways in which environments are gathered and expressed through varying subjects—here, with subjects understood in the broadest possible way. Sensing systems generate and concresce distinct articulations of environmental relations within and through data and across sensing subjects/superjects.” Rather than take on a Kantian view of how “the world emerges from the subject,” Whitehead, with his philosophy of organism, seeks to understand how “the subject emerges from the world,” thereby constituting a superject,” or a subject that is always contingent upon actual occasions and experience. As Shaviro notes in relation to Whitehead:
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - There is always a subject, though not necessarily a human one. Even a rock—and for that matter even an electron—has experiences, and must be considered a subject/superject to a certain extent. A falling rock “feels,” or “perceives,” the gravitational field of the earth. The rock isnt conscious, of course; but it is affected by the earth, and this being-affected is its experience.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - Sensor technologies are constitutive of sense—they too “experience” the world and generate perceptive capacities. Sensors that map in real time a greater density of ecological relations might generate a processual approach to environments by focusing on interactions and even multiple modes of perception. At the same time, to identify a phenomenon as constituting sense data is to make a commitment to distinct “forms of process,” so that environmental processes are selected and concretized in those forms. The process of selecting sense data involves capturing a moment in time, an “instant,” that is re-sutured with other data to form a pattern of ecological processes. While approximating a more process-based and even real-time monitoring of environments, sensors are also productive of practices of selecting and interrelating discrete observations in order to arrive at an understanding of ecological processes. The selection of temperature, vibration, light levels, humidity, and other measurements across primarily physical (although to some extent chemical and biological) criteria in-forms the instants that are sensed, the forms that are documented, and the processes that might be reconfigured.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - The basis for developing facts” within the sensing experiment then directly pertains to the forms and processes of experience that are generated and connected up across sensing subjects. The concrescence of data also requires subjects that can prehend and experience the data. Subjects may be attuned or resistant to receiving data based on prior or concrescent experiences. But the means of gathering data might also contribute to the possibilities for processing and integrating data. In this way, sense data as experienced by subjects may be generative of superjects where the experiences and perceptions generated are in turn formative of the subjects that experience. This runs counter to the notion that a founding subject is the entity that experiences. If, as Whitehead suggests, subjects are always superjects, then subjects are always necessarily distributed and concrescent in relation to actual occasions. Subjects, whether stones or sensors or humans, become environmental in this way since they are involved in feeling and concrescing actual worlds.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Except for objects that are not explicitly specified or do not represent living things, the following objects appeared (ordered by the number of appearances): chair, car, black tie, necktie, furnishing, seat, furniture, bow tie, commodity, device, motor vehicle, pot, bottle, wheeled vehicle, display, cup, matter, vessel, self-propelled vehicle, book, consumer goods, cellular telephone, truck, electronic device, umbrella, military office, bench, wineglass, equipment, table, etc.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - Approaches to media and sensation often focus on the ways in which technologies train or otherwise attune the human senses within a mediatory or prosthetic relation. But the interactions and processes of sense are arguably not fixed within sensory organs or technologies through which mediations are typically understood to occur. In this way, sensation is not primarily an inquiry into relations between human subjects as they perceive more-than-human objects. Instead, the sensory relations within which sensors are mobilized give rise to a more ontogenetic understanding of perception, where sense and expressions of perception are articulated processually and across multiple sites and subjects of inventive sensation. In this way, new perceptual engagements are distributed across sensing capacities and engagements (perhaps similar to what Luciana Parisi has called technoecologies of sensation”), which give rise to distinct sensory processes, informational-material arrangements, and ethico-aesthetic possibilities.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - Such a condition resonates with what Patricia Clough refers to as the importance of focusing on an empiricism of sensation” rather than an empiricism of the senses. Technologies, including sensor systems, can be understood as generative ontologies that in-form the experience and conditions that make sensation possible and changeable. Rather than studying “the senses” as given, it may be more relevant to study experience and how distinct types of sensation become possible, and to consider further what modes of participation and relation these processes of sensation facilitate or limit. To bring this analysis back to sensor technologies, sense data are not simply items to be read and gathered as machinic observations of environments that scientists process. Instead, sense data are indications of a process of becoming sensible, where environments, humans, and more-than-humans are individuated as perceiving and perceivable entities.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - The modes of sensing that concresce within the context of ecological sensor applications might, as discussed earlier in this chapter, begin to be described as collaborative sensing practices taking place across multiple subjects and through distinct processes of experience. These modes of sensing could further be referred to as types of “intimate sensing,” as Stefan Helmreich has suggested in relation to fieldwork undertaken with oceanographers who employ a complex array of sensing technologies in their research. Sensing, in this account, is comprised of a research “ecosystem,” and involves much more than a device focused on an object of study, since bodies enter into a circuit of sensation with instrumentation technologies. As Helmreich writes, “These scientists see themselves as involved not so much in remote sensing as in intimate sensing. Multiple forms of sensing are articulated across different technologies—and so with researchers involved in studying ocean ecologies: “The mediations are multiple and so are the selves.”
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - Influenced by Charles Goodwins discussion of how forms of collaborative seeing are produced within the space of a scientific vessel, Helmreich develops an analysis of the sensing processes that become concretized within these body-environment-technology relationships, where new registers of feeling might sediment through repeated engagement with these devices. The multiple selves that Helmreich discusses most frequently refer back to scientists and crew members on ocean-sensing expeditions, but by extending this approach through a Whitehead-oriented understanding of experience it is possible to include even more expanded collaborative formations of sense. The experiences provided by and through more-than-human processes, as well as the processes that unfold within sense data, in-form a more environmental approach to what might constitute “collaborative” modes of sensing.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - Within the area of more-than-human theory, sensation is increasingly understood as distributed in and through more-than-humans in the form of organisms and technologies, together with their environments. At times influenced by Foucaults well-known “death-of-man” statement, media scholars as far-ranging as Friedrich Kittler and Katherine Hayles have in different ways undertaken analyses of media that dispense with an assumed human subject as the principal site of meaning-making in order to recast the relations that concresce in and through media technologies. As Hayles suggests, environmental modes of computation—RFID in her analysis-raise questions about the effects of “creating an animate environment with agential and communicative powers.” Such technologies allow us to move toward “a more processual, relational and accurate view of embodied human action in complex environments.” Not just sensing but also what counts as the human” shifts in these scenarios, since computational technologies typically now operate within parallel processes and signal toward a multiplication rather than a centering of subjects.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - The subjects that might be discussed as parallel, multiple, or collaborative within environmental sensing extend not just to entities multiplied through more-than-human technologies but also to the incorporation of more-than-human flora and fauna. More-than-human theories of subjects—or ecological approaches to subjects—are becoming increasingly well established not just in media theory but also in philosophy and feminist studies, particularly as articulated in the work of Braidotti, who develops these notions through the work of Deleuze and Guattari (with an emphasis on the notions of ecology developed by Guattari). Braidotti suggests that we begin to work with an “environmentally bound subject” that is also “a collective entity” because “an embodied entity feeds upon, incorporates and transform its (natural, social, human, or technological) environment constantly. In this account, bodies and subjects are even understood as collective information machines of sorts. For Braidotti, “techno-bodies” may be understood as sensors,” or “integrated sites of information networks; vectors of multiple information systems.”
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Research on gender biases is actively carried out not only for movies but also for other media. Many studies have demonstrated that popular media, such as dramas, radio, news, commercials, or movies, tend to completely omit women or represent them based on stereotypes. Specifically, the media overlooks the growing role of women in society. For instance, the number of male and female workers is becoming equal, with the ratio of male to female workers rising from 2.5 to 1 in the 1950s to 1.2 to 1 in 2009. However, in the media, male characters still tend to hold a wide range of professional jobs, while female characters tend to have rather simple jobs, such as housekeepers. Furthermore, the media often depicts female characters as parents of someone and having lower physical or mental abilities than the male characters.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - Such an ecological approach to subjects resonates with Whiteheads discussion of subjects/superjects, where bodies-as-sensors are expressive and productive of environments. The sensing that takes places is a practice of processing and transforming. If human bodies are sensors, then by extension so too are the multiple more-than-humans that take in, express, and transform environments. As the preceding discussion of the James Reserve suggests, it is relevant to bring these multiple formations of experience to play across human and more-than-human subjects into an examination of the specific distribution of environmental sensor networks in this ecological study site and to consider how sensors are expressive of environments, what new environments and subjects concresce as experiencing entities, and how the sensing experiment might make these experiences possible.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - From an experimental forest, this analysis of environmental sensing turns back to Turings countryside—that apparently static backdrop through which sensing was to take place. While Turing imagines a distributed sensing entity processing its bucolic surroundings, in this analysis of test sensors installed in a forest setting it becomes clear that the surroundings to be sensed are in flux and yet formative to establishing conditions and practices of sense. Through this reading, Turings distributed computer becomes a superject, integrated with and formative of the environments and experiences it would decode. It becomes environmental in that it is an entity that generates the formation for further subject-superject experiences. This approach, as discussed throughout this chapter, provides a way of taking account of the abstractions and entities that lure feeling and settle into forms of environmental engagement.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - The environment or milieu as differently understood by writers from Whitehead to von Uexküll, Canguilhem, and Foucault, has been discussed as everything from the conditions of possibility to a zone of transformation and necessary extension within and through which experience is possible. Within the work of von Uexküll, the now well-cited example of the tick that is provoked to act in relation to certain environmental cues is referenced to signal the ways in which sensation is tied to environments and to suggest the species-specific coupling between these. Sensing beyond the human subject can be figured through more-than-human agencies that unfold within environments. But if we take the provocations of Whitehead seriously, then the milieu is not just a site where sensing joins up. Instead, it is also a transformative and immanent process where modes, capacities, and distributions of sense concresce through the experiences of multiple subjects.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - Any given milieu or subject/superject is expressive not of scripted coupling as the work of von Uexküll might suggest, but of creativity, as demonstrated in the work of Whitehead and Simondon. If inventiveness is a necessary part of perceptive processes, then the environment-as-agitation necessitates a more ontogenetic, collaborative, and extensive understanding of sensing. In this way, perception might also move beyond the notion of hybridities or even mediations of sense and instead focus on the sensing conditions and entities that concresce, as well as that which environmental perceptive processes make possible, and how inventive processes might further generate new forms of collective potential.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - The complex interactions that are the focus of study for environmental sensor systems are transformed through the perceptive processes that these systems generate. The ecological relations that are to be discovered and studied are bound up with the detection of patterns within sense data. Sensor hardware and software do not simply gather sense data in the world, but are part of the process of perceptual possibility, both as more-than-human registers of perception and through making distinct relations sensible as subjects of ecological concern.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - The possibility to relate and to make aspects of relations evident is an important aspect of sensor systems, with political and practical consequences. Sensation might be understood as distributed and automated on one level, yet on another level such automation in relation to environmental processes involves not only running scripted functions but also addressing the open and indeterminate aspects of sensors in relation to environmental processes. This is one way of saying that, whatever the computational program, sensors never operate strictly within a coded space, but by virtue of drawing together expanded perceptive processes they inevitably make way for a generative technics of environments.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - There are political implications to the implementing of sensor processes: relations are not simply discovered in the world, rather they are individuated through these distinct computational sensing processes. These processes further orient environmental practices and politics, where increased data and improved awareness of ecological relationships are expected to translate into an improved ability to manage environments and potentially prevent the spread of environmental damage. These crucial relationships concresce not just through practices of data collection and monitoring, as well as sharing data within larger networks, but also through drawing inferences across data sets that illuminate key ecological relationships that are to become the basis of concern or protection. As Whitehead suggests, that which counts as a form or datum is what endures within a “process of composition, which is expressive of “historic character. What counts as empirical requires acts of “interpretation” but also describes a concrescence that continues to have the force of natural fact. Drawing on Locke, Whitehead notes, The problem of perception and the problem of power are one and the same, at least so far as perception is reduced to mere prehension of actual entities.”
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - The mass media also expresses the personality of women and men in a stereotypical fashion. Coltrane and Adams argue that while female characters are depicted as passive, emotional, and male-dependent on TV, male characters are generally described as well-learned, independent, confident, and successful. Moreover, the media tends to describe women as placing importance on aesthetic or family values and not professional success or personal achievement.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - While Whiteheads analysis works across philosophic and cosmological registers, and does not directly address sociopolitical analysis of environments, his work does point toward potential translations to be made across experiencing subjects to political possibilities. As Shaviro suggests, following on Whitehead, experience is a site of potential: “It is only after the subject has constructed or synthesized itself out of its feelings, out of its encounters with the world, that it can go on to understand that world—or to change it.” In other words, as Whitehead notes, “How the past perishes is how the future becomes.” That which is sustained and that which concresces as a register of novelty are processes whereby experience may give rise to new experiences, interpretative practices, and matters of concern. In a different way, Foucault indicates through his discussions on the milieu that sensory arrangements articulate distributions of power, and involve making ongoing commitments to relations and ways of life. Sensory processes that occur across subjects are expressive of material-political relations and possibilities for participation.
[Author: Jennifer Gabrys; From essay:"Sensing an Experimental Forest Processing Environments and Distributing Relations "] - Environmental monitoring through sensor networks is a technoscientific practice that pertains not just to the study of ecological relations but also to newer modes of participatory sensing and citizen-science activity that rely on the use of the sensing capacities on mobile phones and low-cost sensors to track and gather data from environments. While citizen-sensing applications have developed to move these scientific applications into the hands of the general public, even more questions arise as to how or whether sense data makes an effective traversal from data to action. The implications for sensory practices that are articulated within an environmental monitoring context then have relevance for thinking through the processual, relational, and heterogeneous aspects of sensing. Given that the CENS research has moved “out of the woods” to citizen-sensing applications, while at the same time a whole host of participatory applications such as forest monitoring platforms are materializing to protect forests for conservation, how do forests, “citizens,” more-than-humans, and sensor technologies converge to invent new forms of politics that are attentive to present matters of concern and those that are yet to come? In the next chapter, I consider this question in relation to a seemingly more prosaic “sensor” and the sensing practices it operationalizes: the webcam.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - In film directing, a bias towards the representation of a particular gender can cause the audience to form a distorted stereotype of the gender role. The Bechdel test has been widely used to objectively judge the existence of such bias in films. However, because its analysis is based solely on the script of a film, the Bechdel test is incapable of considering the broad spectrum of bias that films can have as a visual medium. This study proposes a more comprehensive analysis system that quantifies the degree of bias in the visual representations of female and male characters in commercial films. By analyzing the image frames of a movie using the latest image analysis techniques, a total of 40 films were analyzed based on 8 quantitative indices. The result demonstrates that there exists a statistically significant difference in the visual representation of female and male characters. Specifically, female characters showed lower values in emotional diversity, spatial occupancy, and temporal occupancy compared to male characters in commercial films. Further, female characters were less likely to wear eyeglasses and also appeared more in static scenes, such as indoors.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Media significantly impacts peoples values. In particular, a movie is a type of media that includes intense audiovisual stimuli. Moreover, because it requires an audiences deliberate choice to sit and watch the film in a closed environment for a long duration, a movie can largely influence on the behaviors and thoughts of the audience. According to previous studies, adolescents showed a positive attitude toward smoking after watching movies depicting smoking scenes, and they also tended to imitate smoking when they saw their favorite celebrities smoking. Another study found that teenagers who had viewed drinking scenes in movies were more likely to experience drinking at an earlier age than their peers.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Gender representation is one aspect among the broad scope of cinematic influences on society. How a particular gender is represented in a movie can affect the audiences stereotype towards the social role of that gender. For instance, in 2012, archery suddenly became a popular sport among teenage girls in the United States. Interestingly, in the same year, two films, <The Hunger Games> and <Brave>, were released that starred famous female archers. When asked about the effects of the films on the decision to begin archery, seven out of 10 girls said the film had an impact. Further, a recent study revealed that the gender, appearance, or race of computer engineers are biased in films, which may prevent young students from entering into the field of computer science. According to the same study, students and parents witnessed that the media mostly portrays people in the field of computer science as white or Asian men, and female, Hispanic, or Black computer scientists are rarely seen. The same cinematic influence is also valid for male audiences. For instance, after the movie, <Billy Elliot> was released, the enrollment of boys in ballet class increased notably. It is evident that a film can encourage diverse perspectives towards masculinity and femininity and can even lead to essential changes in real society.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Since gender representation in movies significantly impacts peoples perceived gender roles, there have been numerous efforts to assess the amount of gender representation bias in movies. The Mako Mori test, named after a character in the movie <Pacific Rim>, is one of the popular tests to measure gender bias in movies. The test requires a movie to (1) include at least one female character, (2) who has her own narrative in the scenario, and (3) which does not exist for merely supporting the narrative of a male character. The Sexy Lamp test, another type of test proposed by Kelly Sue DeConnick, evaluates whether a scenario smoothly functions after eliminating a female character or replacing her with a sexy lamp.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Hollywood movies: Five movies that passed the Bechdel test (HL-Pass), <The Great-est Showman>, <IT>, <Get Out>, <Blade Runner 2049 >, and <Wonder Woman> and five movies that did not pass (HL-NotPass), < Spider-Man: Homecoming>, <Valerian and the City of a Thousand Planets>, <Baby Driver>, <Kingsman: The Golden Circle>, and <Mummy>.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Among all other tests, the Bechdel test is the most representative and most generally used method of evaluating the bias in gender representation in movies. The concept of the Bechdel test first appeared on cartoonist Alison Bechdels comic <Dykes to Watch Out For> in 1985. This test indicates gender bias in a movie by measuring how active the presence of a woman is in a movie. A movie passes the Bechdel test if the movie (1) has at least two female characters, (2) who talk to each other, and (3) their conversation is not related to the male characters. Since the 2000s, the Bechdel test has improved after extensive discussions and been applied to a variety of media such as comics, novels, and computer games. Since 2013, Sweden has included the degree of gender bias measured by the Bechdel test in movie ratings, in addition to sexual content and violence ratings. A movie will receive a grade of A if it passes the Bechdel test, which implies a fair degree of bias in the representation of gender.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - The Bechdel test has played many positive roles in raising public awareness of gender bias in cinema. However, it has fundamental limitations from the accuracy and precision of the evaluation. First, because the Bechdel test is performed subjectively by a person, it consumes considerable human resources and is highly prone to various errors. For example, although the film <Logan> was officially documented to have passed the Bechdel test, many are still arguing over whether the two female characters in the film actually conversed with each other.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Second, a movie with one female character leading the overall narrative may not pass the Bechdel test because the movie merely features a single female character. For instance, the 2013 movie, <Gravity>, portrays the struggle of a female astronaut, Dr. Ryan Stone, who is trying to return to the earth after accidentally drifting alone in space. Sandra Bullock, the actress who played Dr. Ryan Stones role, was impressive enough to be nominated for the Academy Award for Best Actress of the Year. However, regardless of how dominant the character of Dr. Ryan Stone was, the movie does not pass the Bechdel test because it only features one female character.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Third, the Bechdel test, which only considers dialogues between characters, neglects the fact that a movie is a visual art. A director could reflect the same scene through different visual productions depending on the her or his intentions. Film directors cast actors, construct cuts, and direct scenes with the appropriate mise-en-scène that they intended based on their own interpretation of the scenario. In this process, the director can unconsciously express his or her visual stereotypes about the characters age, personality, costume, or surrounding environment, which are not conspicuously reflected in the dialogue.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - All of the above limitations exist because the Bechdel test analyzes only one single aspect of the film, the scenario, and provides only a dichotomous result of passing the test for multi-layered and complicated gender bias phenomena. It is difficult to fully represent todays various discourse on gender representation bias, which is much more diverse than in 1985 when the Bechdel test was first presented. It can also have the side effect of inducing filmmakers to only include the minimum conditions in the movie that can pass the Bechdel test. For example, filmmakers can easily have their films pass the Bechdel test by introducing female supporting characters who share dialogue and jokes that are not related to the core narratives of the film. <The Hitmans Bodyguard>, released in 2017, manages to pass the Bechdel test through a short conversation between female characters Russell and Casoria, but the conversation has nothing to do with the films core content and is not remembered by most audiences. From this, we can expect that providing a more multi-layered gender bias analysis methodology to overcome the limitations of the Bechdel test will have a large impact on the way films will be produced in the future, beyond reevaluating already-produced films.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Inspired by the limitations of the Bechdel test, this study proposes a novel analysis system that can automatically quantify the degree of bias in gender representations that exist within a movie. Our system, in particular, analyzes the visual information of each frame of the movie, not the script or audio information of the movie. This allows our system to reveal the biases that existing analysis methods have not yet uncovered and to complement existing methods. Specifically, the proposed system analyzes each frame of the movie using Microsoft Face API and YOLO9000. Using raw information about the characters faces and surrounding objects found in each frame, the proposed system computes eight indices (emotional diversity, spatial staticity, spatial occupancy, temporal occupancy, mean age, intellectual image, emphasis on appearance, and type and frequency of surrounding objects) that describe the representation of a particular gender. By comparing these eight indices for male and female characters, our system can quantitatively reveal the bias that exists in gender representation.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - During the development of the system, we conducted two studies using commercial films chosen from the Korean box offices top 100. Conventionally, the box office list is measured in terms of the number of tickets sold or the revenue raised, and therefore, we considered it to be the most representative and influential for the public.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - The first study included surveys of a large number of people (608 participants) regarding the 20 commercial movies released in 2017, and we examined whether the publics visual memory about the characters of the movies actually changed according to the characters gender. In the first study, we found that there was a significant difference in the visual memory of the public based on whether the characters were female or male. Taking into account the results from the first study, we designed and implemented the movie image analysis system. Using the system, the second study analyzed a total of 40 films, after adding 20 that were released in 2018, and identified differences between the representations of female and male characters.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Korean movies: Four movies that passed the Bechdel test (KR-Pass) <The Battleship Island>, <Confidential Assignment>, < I Can Speak>, and <Steel Rain> and six movies that did not pass (KR-NotPass), <The King>, <A Taxi Driver>, <Along with the Gods: The Two Worlds>, <Midnight Runners>, <Memoir of a Murderer>, and <The Outlaws>.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - The proportion of women and men in the media is also distorted from reality. According to a famous study by Gerbner and Signorielli, the ratio of men and women appearing in prime-time television from 1969 to 1978 was 2.5:1. In another study, Hether and Murphy found that males are more dominant in all roles than females. From 1990 to 2005, the ratio of male and female characters in top-grossing, G-rated movies was 2.57:1. Among the top 100 movies released in 2002, the proportion of male characters is 73%. Yet, in 2015, female characters were tallied at 31.4%, among 4,370 characters. Only 34% of the top 100 films in 2016 had leading or co-starring female characters.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - The media distorts the age of women and men as well. According to Lauzen and Dozier, films over-represent women in their twenties and thirties and men in their thirties and forties. , . Among the top 100 movies in 2016, 29 movies featured male lead or co-lead characters that were aged 45 or older, while only eight movies featured female lead or co-lead characters that were aged 45 or older. In particular, they found that female characters were much younger than male characters in movies. Moreover, Smith et al. stated that the older a female character is in the film, the less likely the film will portray her as an attractive person. Such portrayal reinforces the biased cultural view that only young women are worthy.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - The media also sexually objectifies female characters by emphasizing their body. Researchers have been actively tried to capture the extent of the sexual objectification of women by measuring the amount of exposure to body parts in the media. According to a study conducted by Smith et al. in 2014, women are more likely to be described as sexy, naked, or physically attractive than men at all ages.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Alison Bechdels test, first introduced in a comic book called <Dykes to Watch Out For>, is often applied not only to movies but also to other media such as TV shows and books. As the Bechdel test became popular, there were attempts to complement its limitations, to automate it, or to devise better gender bias evaluation techniques.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Kapoor et al. devised the Indian Bechdel test, which reinterprets the original Bechdel test in the context of the Indian cinema scene. To pass the Indian Bechdel test, two female characters must appear and talk to each other, but the conversation cannot contain stereotypical topics for women, such as their families, men, children, cooking, shopping, or marriages. The same study further suggests using the Reverse Bechdel test to analyze male stereotypes in Indian movies. Similarly, to pass the Reverse Bechdel test, at least two male characters must appear and talk to each other, and the conversation should not contain stereotypical topics for men such as women, sports, sex, cars, politics, or jobs.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - There was an attempt to automate the Bechdel test as well. The proposed system answers the first question of the Bechdel test, Are there two women in the movie?, using the Internet Movie Database (IMDB), the Social Security Administration (SSA) database, and Stanfords named entity coreference resolution system. The answer to the second question, Do the two female characters talk to each other?, is obtained by implementing an interaction network for two characters speaking in sequential order. The answer to the last question, Is the topic of conversation between the two female characters irrelevant to the male character?, is found through machine learning models and social network analysis using linguistic features.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Beyond the Bechdel test, newly developed techniques can also assess the degree of bias in gender representations in movies. In 2016, the University of Southern California and Google created software called The Geena Davis Inclusion Quotient (GD-IQ). GD-IQ is software that quantifies the gender bias present in movies, TV, or other media by analyzing the video frame and audio of the movie to determine the gender of the characters and by measuring each characters speaking time. Using this software, they found that female characters appear less frequently than male characters and spend less time talking in movies. Among the commercial software, screenwriting software like Final Draft or WriterDuet recently introduced a new function that checks whether the written scenario is gender-equitable or not.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - USC Annenbergs study found that only 4.1% of 886 directors for 800 movies were female. More seriously, among the filmmakers between 2007 and 2015, only 1.4% of filmmakers were women. It is even more difficult for women to enter a field that requires highly technical skills or has a strong hierarchy within the team. Commercial films involving female producers averaged 22.2% and commercial films involving female writers averaged 30.1%, compared with 3.29% on average for female camera directors. Such representation arises from a strong bias in society that men are the ones who can handle hardware or who have physical strength, such as overseeing cameras and lights. Moreover, the gender distribution of the people participating in the film-making process affects the gender distribution of the characters in the movies. In films with at least one female director and/or writer, women accounted for 57% of the leading characters whereas women accounted for 18% of the leading characters in films with only male directors and/or writers.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - On the other hand, the smaller the gender representation bias, the greater the films commercial success. Only 36% of the top-grossing 50 movies released in 2013 passed the Bechdel test, but they made more money than those that failed to pass the Bechdel test. In addition, movies with female protagonists had 15.8% more sales than films without. Furthermore, when the male and female actors co-starred, the generated revenues were 23.5% higher than movies with male actors alone or female actresses alone. Therefore, as the gender of the protagonists diversifies, the probability that the movie will succeed increases. This suggests that considering diversity in gender representation extends beyond what is equitable for women and men, represents the desire of real audiences, and can also present significant economic profits.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Culture has a significant influence on gender representation, because it is a socially constructed idea. In general, there exists a universal pattern in stereotyped gender roles across cultures, although the magnitudes of such pattern differ among cultures. In TV commercials, even though the patterns of results were similar, Furnham et al. determined that stereotypical portrayals of men and women in Hong Kong TV commercials were more significant and more substantial than in the Western countries. A similar analysis of commercials in Korea and the United States also agreed with the findings from Furnham et al. Moreover, several studies stated that gender stereotyping in commercials seems to be declining only in Western culture, not in the Asian culture.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Taking Disney as an example, Western animation industries have improved gender representation in characters in their recent productions. On the other hand, Japanese animation, which significantly influences Asian nations, still persistently and stereotypically portrays female characters. Moreover, while the Swedish Film Institute and its cinemas incorporated the Bechdel test into their rating systems in 2013, the South Korean Film Council published its first gender statistics about Korean films in 2017. These findings suggest that Western media has made some efforts to remedy the gender representation bias, whereas the efforts from the Asian media are just beginning.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Previous studies of gender representation biases in the mass media show that women are depicted through (1) biased representations more frequently than men, including (2) biased age, (3) biased physique, (4) biased personality, and (5) biased social occupations. To capture such biases, several tests, like the Bechdel test, were proposed. However, the Bechdel test and other methods of analysis that were developed earlier do not fully capture those gender biases in a visual medium like film. There have been attempts to analyze the images and audio of movies using machine learning techniques, such as GD-IQ, but these did not present more diverse indices of gender biases beyond replicating existing known statistics such as the proportion of female and male characters. This study analyzes the image frames of movies with machine learning techniques to complement the limitations of the Bechdel test, and it exposes gender representation bias in movies more widely than previous studies by using eight analytical indices.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - If there is a visual gender representation bias within a film, it will affect the way that viewers perceive and remember characters of different genders. Prior to identifying the presence of visual representation bias in films using image analysis techniques, we conducted a survey to determine whether the audiences memories of previously released films were actually biased by gender. In particular, the gender-recall bias of audiences revealed from this survey will be compared with the results obtained through image analysis.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - This survey has the following hypotheses:
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - H1: The extent and manner in which the audience remembers the character of the movie and the scene in which the character appears depends on the gender of the character.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - H2: The extent and manner in which the audience memorizes female characters and male characters in the movie depends on whether or not the movie has passed the Bechdel test.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - H3: The extent and manner in which the audience memorizes female characters and male characters in the movie depends on the region of the country where the movie was produced.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - The survey was conducted for three months from May 2018 to July 2018 in Korea. The survey participants consisted of Koreans who watched movies. A total of 608 people participated in the survey (278 males and 330 females). Of them, 26 were in their teens (4.2%), 503 in their twenties (83.0%), 59 in their thirties (9.5%), and 20 in their forties or older (3.2%). Among them, 122 people knew about the Bechdel test (20.0%) and 486 people did not know about it (79.9%). Additionally, 87 of them watched one movie per month (14.3%), 132 watched two movies per month (21.7%), 152 watched three movies per month (25.0%), and 77 watched four movies per month (12.6%), and 160 people watched more than five movies per month (26.3%). When asked about their preferred movie genre, 318 answered with action, 310 with sci-fi, 293 with crime thrillers, 262 with dramas, 239 with romances, 212 with comedy and 53 with horror. In this question, they were able to select a number of items, and participants were rewarded with a five-dollar gift card for their participation.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - The materials in the survey consist of specific films chosen by the experimenter and questionnaires for each film. Participants were asked to select the films they had seen from the list of films chosen by the experimenter and to answer the questions provided for each film.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Ten Hollywood films and 10 Korean films were selected from the top 100 box-office records in Korea in 2017. We were careful not to choose only movies of a certain genre. As a result, the selected films were distributed among action (2), sci-fi (6), horror (2), crime (1), and drama (5) following the standard movie genre classification. At this time, movies of animation genres were excluded. For both Korean and Hollywood films, we also tried to keep a comparable percentage of movies that passed the Bechdel test and those that did not. Five out of 10 Hollywood movies passed the test and four out of 10 Korean movies passed the test. Hollywood films were judged on the basis of the data on the Bechdel tests website. In the case of Korean films, three researchers evaluated each movie individually. Only the films upon which all three agreed were included in the list, and controversial films were excluded.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - As a result, the selected movies included the following:
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - The survey was divided into two parts. In Part 1, there were four questions per movie: two for each gender. In Part 2, there were eight questions per movie: two for each gender and each character role (leading or supporting). In each of the questions in Part 2, we surveyed using only movies that included both male and female leading (or supporting) characters. As a result, the following movies were excluded from the survey of leading characters: <IT>, <Blade Runner 2049 >, < Spider-Man: Homecoming>, <Confidential Assignment>, <Steel Rain>, <A Taxi Driver>, <Midnight Runners>, and <The Outlaws>. As a result, the following movies were excluded from the survey of supporting characters: <Mummy>, <The Battleship Island>, and <Memoir of a Murderer>. Among the films listed above, participants had to select the films they had seen and to answer the following two parts of the questionnaire for each film. We devised our own questionnaires and the following includes the specific questions from each part:
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Survey Part 1: Participants were given a look at the movies official title and poster. Then, they answered the following three questions about each gender (i.e., free recall task): Q1. Are there any scenes you remember where male (or female) characters appear in the movie? Possible answers were (1) Yes or (2) No. Q2. What kind of scene is most memorable about the character of that gender? Possible answers were as follows: (1) a daily conversation scene, (2) an action scene (e.g., a fight with an enemy or car racing), (3) a scene expressing emotion quietly (e.g., crying alone, or monologues), (4) a scene expressing a heated emotion (e.g., throwing objects, expressing anger, or dancing joyfully), (5) love and sex scene, (6) do not remember, and (7) etc. These are representative types of different movie scenes in terms of visual presentation and dynamics.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Survey Part 2: Participants were first given the synopsis written in text. Then participants saw pictures of the movies male lead character, male supporting character, female lead character, and female supporting character (e.g., cued recall task). Unlike the free recall task, the cued recall task can minimize noise from participants different experiences or preferences. For each character, participants answered the following four questions: Q1. How much is the character remembered? The answer was performed on a 7-point Likert scale (1 not remembered to 7 clearly remembered). A score of 7 meant that the viewer had memorized the characters face, attire, behavior, and background in detail. Q2. What kind of scene is most memorable about the character? Possible answers included the following: (1) a daily conversation scene, (2) an action scene (e.g., a fight with an enemy or a car racing), (3) a scene expressing emotion quietly (e.g., crying alone or monologues), (4) a scene expressing a heated emotion (e.g., throwing objects, expressing anger, or dancing joyfully), (5) love and sex scene, (6) do not remember, and (7) etc.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - We tested whether each participants answer to each question was independent of the characters gender. In Survey Part 1, all dependent variables (participants answers to each question) were nominal variables. In Survey Part 2, the dependent variable of Q1 is an ordinal variable (7-point Likert scale), and all other dependent variables were nominal variables. The independence was tested by applying the chi-squared test for the nominal variables. To test the association between nominal and ordinal variables, we report Goodman and Kruskals lambda (λ). An alpha level of 0.05 was used.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - The survey was mainly conducted for those who visited the Jeonju International Film Festival in 2018. The festival had a total of about 80,000 visitors over nine days. Participants were selected randomly from among the visitors. The questionnaire form was prepared online in advance and participants were able to answer using a tablet computer in an open space. A survey took about 20 minutes per movie. For films that did not have enough respondents, we recruited additional participants who had seen the target movie through online advertisements, and they visited our lab and responded to the questionnaire.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - There was not a significant association between gender and Q1 responses for HL-Pass (x²(1, N = 312) =0.722, p =0.395). However, there was a significant association between gender and Q1 responses for HL-NotPass (x²(1, N = 324) =8.820, p=0.003), KR-Pass (x²(1, N = 200) =19.207, p <0.001), and KR-NotPass (x²(1, N = 380) =67.059, p <0.001).
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - There was a significant association between gender and Q2 responses for HL-Pass (x²(6, N = 312) =17.825, p =0.007), HL-NotPass (x²(6, N = 324) =73.945, p <0.001), KR-Pass (x²(6, N = 200) =42.082, p<0.001), and KR-NotPass (x²(6, N = 380) =205.397, p <0.001).
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - There was not a significant association between gender and Q1 responses for HL-Pass (λ =0.087, p =0.431), HL-NotPass (λ =0.169, p =0.059), and KR-Pass (λ =0.128, p =0.315). Yet, there was a significant association between gender and Q1 responses for KR-NotPass (λ =0.309, p =0.023).
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - There was a significant association between gender and Q2 responses for HL-Pass (x²(6, N = 208) =31.989, p<0.001), HL-NotPass (x²(6, N = 236) =38.170, p <0.001), KR-Pass (x²(6, N = 94) =12.469, p=0.006), and KR-NotPass (x²(6, N = 162) =35.503, p <0.001).
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - There was a significant association between gender and Q1 responses for HL-Pass (λ =0.244, p =0.001), HL-NotPass (λ =0.197, p =0.046), and KR-Pass (λ =0.277, p =0.003). There was not a significant association between gender and Q1 responses for KR-NotPass (λ =0.165, p =0.072).
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - There was a significant association between gender and Q2 responses for HL-Pass (x²(6, N = 312) =41.976, p <0.001), HL-NotPass (x²(6, N = 274) =17.653, p =0.007), and KR-Pass (x²(6, N = 166) =33.410, p <0.001). However, there was not a significant interaction between gender and Q2 responses for KR-NotPass (x²(6, N = 328) =18.089, p =0.006).
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - In Survey Part 1 (free recall task), based on the total number of responses for each question, audiences remembered 7.7% point more male characters than female characters. Based on the total number of responses for each gender, male characters were most remembered in action scenes (42.1%) and female characters were most remembered in scenes with casual conversation (36.7%). Moreover, male characters were well remembered in scenes with heated emotion (18.3%), while female characters were recalled to a lesser degree (10.0%). These findings suggest that films represent masculinity with bias. Further, in all movies, female characters (4.1%) were more remembered in love scenes than male characters (0.3%). This is a noteworthy bias because female characters usually shoot love scenes with male characters.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - In Survey Part 2 (cued recall task), the response to the first question was measured on a 7-point Likert scale. Based on the sum of the scores, which were multiplied by the frequency divided by the total number of responses, the male leading characters were more remembered (6.4) than the female leading characters (5.8). However, there was no difference in gender-specific recall performance between the supporting characters (male: 5.0, female: 5.0). Based on the total number of responses for each gender, the most memorable scenes for male leading characters were action scenes (34%), and the most memorable scenes for female leading characters were casual conversations (36.6%). However, the most memorable scenes for supporting characters were those with casual conversation for both male (48.0%) and female (44.6%) characters. Female leading characters (3.4%) were more recalled in love scenes than male leading characters (0.29%) This difference was maintained for supporting characters as well (male: 0.37%, female: 5%).
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - All these results support the first hypothesis (H1) that the gender of a character influences the degree and manner in which audiences recall the character.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - In Survey Part 1 (free recall task), based on the total number of responses for each question, audiences remembered 4.5% point more male characters than female characters in movies that passed the Bechdel test, but 10.1% point in films that did not pass the test. Discrepancies between the types of scenes in which the characters were recalled were also greater in films that did not pass the Bechdel test. In films that passed the test, respondents recalled the male character 10.5% point more than the female character in the action scene, but in movies that did not pass the test, the difference grew to 38.4% point.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - In Survey Part 2 (cued recall task), the score of the response in which the leading character was memorized was 0.25 for films that passed the test (male: 6.24, female: 5.99), but increased to 0.78 for films that did not pass the test (male: 6.45, female: 5.66). For supporting characters, the recall score of female characters was 0.86 higher than the male scores (male: 4.55, female: 5.41) in movies that passed the Bechdel test, but the recall score of male characters was 0.63 higher in films that did not pass the test (male: 5.37, female: 4.74). The most memorable scenes in films that passed the Bechdel test were scenes that included casual conversation for both male (41.7%) and female leading characters (37.7%). For films that did not pass the test, the male leading characters were recalled most often in action scenes (51.2%), and the female leading characters were the most recalled in scenes with casual conversation (35.7%). For supporting characters, however, both male and female characters were recalled most frequently in scenes with casual conversation, regardless of whether the movies passed the test.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Regardless of whether movies passed the test or the role difference, female characters were recalled in love scenes more than male characters. These results support the second hypothesis (H2) by showing that audiences memories of characters with different genders can vary depending on whether the film has passed the Bechdel test.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - In Survey Part 1 (free recall task), based on the total number of responses for each question, respondents remembered male characters 2.8% point more than female characters in Hollywood movies but 13.1% point in Korean movies. Discrepancies between the types of scenes in which the characters were recalled were also greater in Korean movies. In Hollywood movies, respondents recalled the male character 21.4% point more than the female character in the action scene, but in movies that did not pass the test, the difference grew to 32.4% point.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - In Survey Part 2 (cued recall task), the gap between the responses who remembered male and female leading characters was 0.49 for Hollywood movies (male: 6.27, female: 5.78) but increased to 0.67 for Korean movies (male: 6.52, female: 5.84). For supporting characters, the recall score of female characters was 0.08 higher than the male scores (male: 5.15, female: 5.23) in Hollywood movies, but the recall score of male characters was 0.04 higher in Korean movies (male: 4.84, female: 4.80). These results support the third hypothesis (H3) by showing that audiences memories of characters with different genders can vary depending on the region where the movie was released. The most memorable scenes in Hollywood movies were action scenes for both male (51.4%) and female leading characters (28.8%). The most memorable scenes in Korean movies involved casual conversation for both male (41.4%) and female leading characters (50.0%). For supporting characters, however, both male and female characters were recalled most frequently in scenes with casual conversation, regardless of where the movie was released. Regardless of published region or role differences, female characters were recalled in love scenes more than male characters.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - The results of the preliminary survey supported all three hypotheses (H1, H2, and H3). Female characters were less remembered than male characters and the discrepancy was greater in movies that did not pass the Bechdel test or in Korean movies. Even though the discrepancy was smaller, the female characters were still less remembered than the male characters in the movies that passed the Bechdel test.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - In addition, the survey showed that audiences can have a broad spectrum of memories of the characters and that may depend on the characters gender. For example, in Survey Part 2 (using the 7-point Likert scale), the recall scores for female characters are spread more than male characters. Also, depending on the gender of the character, the types of scenes that are recalled also differed. These differences could relate to how male and female characters are visually represented in the movie. It is difficult for the Bechdel test, which evaluates movies but has minimal requirements for the script, to uncover such diverse gender biases.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - However, the results in this survey do not directly demonstrate that the visual representation of female and male characters are different in commercial films. The individual differences that exist among participants or the different degrees of character popularity could affect recall performance. For example, the majority (83%) of participants in this survey were in their twenties, which may have affected the outcome. In addition, the difference between the year in which the survey was conducted and the year that the target movies were released may have affected the recall performance of the participants. Nonetheless, the results of this survey provide a strong motivation to objectively measure the visual gender representation bias that may exist within a movie. Based on the results of the preliminary survey, the next section describes the implementation of an image analysis system that automatically quantifies the visual gender representation bias that exists in a movie.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - From the results of previous surveys, we showed that the gender of the characters can significantly impact the publics visual memories of the movie characters. However, traditional methods such as the Bechdel test cannot adequately determine gender bias in a movies visual representation of characters. In this section, we propose and implement a system that can automatically analyze image-based aspects of the gender representation present in a movie.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - The analysis system consists largely of three modules. First, the preprocessing module lowers the frame rate of the target movie through downsampling to increase the efficiency and speed of the analysis, based on the fact that human movement is mainly in the low-frequency bands. Second, the frame analysis module analyzes each frame of the downsampled movie with known image-analysis techniques. This process extracts the information about the faces of the characters and the information about the surrounding objects that appear in each frame. Third, in the gender representation analysis module, eight statistical indices of the gender representation are calculated for the target movie based on the extracted information. The gender representation bias can be revealed by comparing the calculated indices for each gender. The following sections describe in detail the actual implementation of each module.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Movies are usually produced with 24 frames per second (fps). Analyzing all 24 images per second requires much time due to the high resolution of movie images (i.e., 1920×1080). However, our system quantifies gender representation by tracking information associated with the people appearing in the movie. Therefore, because human movements are primarily in the lower frequency bands, our system does not need to analyze all 24 images per second. For example, if a person walks or runs, the main frequency component of the movement stays at 1 to 2 Hz. The Nyquist theorem requires a sampling rate of at least twice the frequency component of the signal to be measured. Therefore, to analyze people, the framerate of a movie could be lowered to about 2 to 4 fps. Our final preprocessing module lowers the framerate of the movie to 3 fps through fixed-interval downsampling.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - The frame analysis module analyzes each frame of the downsampled movie and tracks the characters faces and surrounding objects. For each frame, Microsoft Face API tracked the face information of the characters, and YOLO9000 tracked information about nearby objects. For each frame, the following information was recorded:
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Microsoft Face API: The faces of people appearing in each frame are tracked. If there are logged faces, a total of 89 features are provided for each face. Among them, 19 features are used for the gender representation analysis, which tell the position, size, and the emotional state of the face, whether the face wears glasses, the degree of light exposure, the estimated age, the gender, and the amount of blur on the face. The emotional state of the expression is classified as anger, disgust, contempt, fear, happiness, neutral, sadness, and surprise, each of which is given as a value from 0 to 1 indicating its degree.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - YOLO9000: Location, size, detection reliability, and type can be tracked for more than 9000 kinds of objects. Among them, our analysis module uses object type and detection reliability for the gender representation analysis.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Generally, for a movie downsampled at 3 fps, it takes about seven hours to analyze a two-hour-long movie with Microsoft Face API. It takes about an hour to analyze a two-hour-long movie with YOLO9000. Face API is a paid service and requires about four dollars to analyze a single movie.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Based on the data from each image analysis technique, the gender representation analysis module computes eight indices. These indices are proposed by the authors and consider the bias of the gender representation found in previous studies and summarized in the related work (bias of frequency, age, physical representation, personality, and social occupations). Each index is calculated for each movie and for each gender. The following describes the meaning of each index and how it is calculated. To simplify the explanation, the equations below assume that the target gender is set to either male or female (male or female characters are calculated in the same way).
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Emotional diversity: Emotional diversity is an indicator of how many different emotional states a character has expressed from the beginning to the end of a movie. The previous study found that mentally and physically healthy people had higher emotional diversity. It is also shown that there is no significant difference in the emotional diversity of women and men in society. where s is the number of types of emotions the character shows in the movie, and pᵢ is the i-th observation probability of the overall emotion. This follows the definition of Shannons entropy: the higher the value of emotional diversity, the more diverse the characters emotional state in the movie. The Microsoft Face API returns a value from 0 to 1 for each emotion. To remove noise disturbance, the emotion is counted only if a value of 0.1 or greater is measured.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Spatial staticity: Spatial staticity is an indicator of how static a character has stayed in a frame of a movie. The position of each character on the screen is tracked from the Face API which can be regarded as time-series data, and the degree of its staticity can be measured through its frequency analysis. Our system analyzes the downsampled movie at 3 fps, so from the power spectral density (PSD) of the low-frequency components up to 1.5 Hz, we can quantify how static the character was in the frame, where p(t) is a time series representing the characters position on the frame and is a continuous time series because it is defined only for frames where characters appear. Spatial staticity is calculated separately for the x- and y-direction coordinates. The higher the value of spatial staticity, the less animated the character appears on the screen.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Spatial occupancy: Spatial occupancy is the area occupied by a characters face within an image frame. In theory, film directors often use close-up shots to express objects or characters that they consider important. From the area of the characters face (A) tracked from the Microsoft Face API, the spatial occupancy is calculated, where the square root of the face area is used to convert the unit from area to length. Furthermore, to remove the confounding effect due to the type of appearing scene, the spatial occupancy may be calculated only for frames with different gender characters tracked at the same time.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Temporal occupancy: Temporal occupancy indicates the percentage of time a character has appeared versus the total running time of the movie. This can be calculated by counting the number of frames in which the character appears, where N is the number of frames in which the character appears. Ntotal represents the total number of frames of the downsampled movie. The higher the temporal occupancy, the more time the characters appeared in the movie.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Mean age: The Microsoft Face API outputs age information predicted from the appearance of the faces detected for each frame. Mean age is calculated from the information, where Y represents the age of the faces tracked in each frame. The obtained age information may vary depending on the characters makeup and may be independent of the age of the actual actor. However, the purpose of our system is to measure the bias of the visual production in the film, so it is necessary to look at the age of the actor depicted in the film, rather than the actual age of the actor.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Intellectual image: Microsoft Face API outputs whether the character found in the movie frame is wearing glasses, which could be used to emphasize the intelligence of a specific character. where G indicates the presence or absence of glasses (“1” if glasses are worn and “0” otherwise). Intellectual image increases as more faces with glasses are tracked.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Emphasis on appearance: Even in the same scene in a scenario, the intensity of the illumination light reflected on the actors face can change depending on the direction of the director, which could emphasize the actors face or specific body parts. Emphasis on appearance is calculated from the light exposure of the face obtained from Microsoft Face API, where E is the value of facial light exposure obtained from the Face API and has a value from 0 to 1. The higher the emphasis on appearance, the higher the exposure of the face.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Type and frequency of surrounding objects: When a character is tracked from the Face API, objects around the character can be tracked with YOLO9000, and meaningful information related to gender bias can be found. For example, the types of objects around the character can indicate whether the character is indoors or outdoors. In frames where a character of a specific gender is tracked, our system analyzes the types and frequencies of the surrounding objects tracked together. It was finally visualized as a word cloud.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - The study follows a 2×2×2 full factorial design. The independent variables for the statistical analysis are the Bechdel test result of the movie (Bechdel), the region where the movie was made (Region), and the gender to be analyzed (Gender). The dependent variables for statistical analysis are the 8 quantitative indices output by our system. A three-way ANOVA was conducted with an alpha level of 0.05.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - We analyzed 40 movies released in Korea in 2017 and 2018. The movies released in 2017 are the same as those used in the preliminary survey. Ten additional movies released in 2018 were selected in the same way as in the preliminary survey. As a result, 20 movies were Hollywood movies and 20 movies were Korean movies. Of the 20 Hollywood movies, 10 have passed the Bechdel test, and 10 of the 20 Korean movies have passed the Bechdel test.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - The selected movies are composed of the following genres: nine action, five crime, three comedies, two romances, four horror, nine drama, and eight sci-fi. Movie files for movie analysis were downloaded and paid for at the domestic movie distribution website. This cost was about $160.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - All analyzes were performed on a Linux desktop computer (Ubuntu 16.04 LTS, Intel Core i7-7700 CPU 3.6GHz × 8, GeForce GTX 1080 Ti). It took a total of 15 days to complete all analyzes.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Gender significantly affected Emotional Diversity (F(1,36)=33.553, p<0.001). However, Bechdel (F(1,36)=0.248, p=0.621) or Region (F(1,36)=0.539, p=0.468) did not significantly affect Emotional Diversity. No interaction effect between the three variables was significant (p>0.392). The emotional diversity of female characters was on average 0.897 bits (σ=0.250) and that of male characters was 1.0828 bits (σ=0.237). There were eight out of 40 movies that had female characters with higher emotional diversity (<Wonder Woman>, <I Can Speak>, <Mamma Mia! Here We Go Again>, <Oceans Eight>, <Gonjiam: Haunted Asylum>, <Valerian and the City of a Thousand Planets>, <Sicario: Day of the Soldado>, <The Accidental Detective 2: In Action>).
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Gender did not significantly affect Spatial Staticity for both the x (horizontal) (F(1,36)=3.280, p =0.078) and y (vertical) coordinates (F(1,36)=2.354, p =0.134). For x coordinates, we observed a high effect size, although it was not statistically significant (n=0.084). Moreover, Bechdel (F(1,36)=1.364, p=0.251) or Region (F(1,36)=0.996, p=0.325) did not significantly affect Spatial Staticity. No interaction effect between the three variables was significant (p>0.1). The mean of spatial staticity in the horizontal movement for the female characters was 14.16 (σ=27.67), and the same measure for the male characters was 7.2181 (σ=4.283). The mean of spatial staticity in the vertical movement for the female characters was 5.058 (σ=13.09), and the same measure for the male characters was 2.118 (σ=1.36).
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Gender significantly affected Spatial Occupancy (F(1,36)=10.648, p=0.002). Whereas, Bechdel (F(1,36)=1.281, p=0.265) or Region (F(1,36)=0.044, p=0.835) did not significantly affect Spatial Occupancy. There was a significant interaction effect between Gender and Region on Spatial Occupancy (F(1,36)=11.723, p=0.002). The post-hoc analysis showed that gender differences were significant only in Korean films (p<0.001). However, the p-value for Hollywood movies was 0.910. All other interaction effects were not significant (p>0.386).
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - The mean of the spatial occupancy for female characters was 196.36 pixels (σ=64.73) and the mean of the same measure for male characters was 213.45 pixels (σ=53.03). In Hollywood movies, the spatial occupancy of the female character was 207.23 pixels (σ=48.48) and the spatial occupancy of the male character was 206.39 pixels (σ=42.40). In Korean movies, the spatial occupancy of the female character was 185.49 pixels (σ=77.47) and the spatial occupancy of the male character was 220.52 pixels (σ=62.20).
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Spatial occupancy can also be calculated for frames where male characters and female characters appear together. If we define it as Relative Spatial Occupancy, we get the following results. Gender significantly affected Relative Spatial Occupancy(F(1,36)=36.409, p<0.001). Contrastingly, Bechdel (F(1,36)=0.004, p=0.949)5.2.4 had no significant effects. However, there was a significant effect of Region (F(1,36)=5.021, p=0.031) on Relative Spatial Occupancy. There was also a significant interaction effect between Gender and Region on Relative Spatial Occupancy (F(1,36)=8.766, p=0.005). ). The post-hoc analysis showed that gender differences were significant in both Korean (p<0.001) and Hollywood films (p=0.036). All other interaction effects were not significant (p>0.646).
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - The mean of the relative spatial occupancy was 154.12 pixels (σ=43.90) for female characters and 174.21 pixels (σ=40.68) for male characters. A lower value than the normal spatial occupancy is obtained because the faces of several characters share the screen. The overall relative spatial occupancy of Hollywood films was 178.22 pixels (σ=37.00), and Korean films were 150.11 pixels (σ=44.89). In Hollywood movies, the relative spatial occupancy of the female character was 173.10 pixels (σ=37.36), and the relative spatial occupancy of the male character was 183.34 pixels (σ=36.86). In Korean movies, the relative spatial occupancy of the female character was 135.13 pixels (σ=42.43), and the relative spatial occupancy of the male character was 165.09 pixels (σ=36.86).
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - There was a significant effect of Gender on Temporal Occupancy (F(1,36)=41.92, p<0.001). Yet, Bechdel (F(1,36)=0.014, p=0.91) or Region (F(1,36)=0.877, p=0.355) had no significant effect on Temporal Occupancy. There was a significant interaction effect between Gender and Bechdel (F(1,36)=10.15, p=0.003), and Gender and Region (F(1,36)=23.64, p<0.001) on Temporal Occupancy. Post-hot analysis showed significant differences between the genders, regard-less of whether they passed the Bechdel test or not (when passed: p<0.001, when it did not pass: p=0.009). The post-hoc analysis indicated that gender differences were significant only in Korean films (p<0.001). The p-value for Hollywood movies was p=0.262. The mean temporal occupancy for the female characters was 5.15% (σ=4.39%) of the entire running time of a film, and the mean for the male characters was 9.2% (σ=4.16%). In the movies that passed the Bechdel test, the mean temporal occupancy for the female character was 6.18% (σ=4.23%) and the mean for the male characters was 8.26% (σ=3.3%). In the movies that did not pass the Bechdel test, the mean temporal occupancy for the female character was 4.11% (σ=4.35%) and the mean for the male characters was 10.16% (σ=4.7%).
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - In Hollywood movies, the mean temporal occupancy for the female characters was 6.32% (σ=5.07%) and the mean for the male characters was 7.32% (σ=3.67%). In Korean movies, the mean temporal occupancy for the female characters was 3.98% (σ=3.23%) and the mean for the male characters was 11.11% (σ=3.75%).
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - There was a significant effect of Gender on Mean Age (F(1,36)=43.424, p<0.001). However, Bechdel (F(1,36)=0.527, p=0.472) or Region (F(1,36)=0.166, p=0.686) did not significantly affect Mean Age. No interaction effect between the three variables was significant (p>0.398). The mean age of the female characters was 26.47 years (σ=4.88), and the mean age of male characters was 33.48 years (σ=4.37).
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - There was a significant effect of Gender on Intellectual Image (F(1,36)=12.581, p=0.001) but no significant effect of Bechdel (F(1,36)=0.554, p=0.462) or Region (F(1,36)=0.129, p=0.722) on Intellectual Image. No interaction effect between the three variables was significant (p>0.462). The female characters wore eyeglasses with a mean of 6.13% (σ=8.79%) on the frames they appeared, and the male characters did with the mean of 13.56% (σ=17.06%).
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - There was not a significant effect of Gender on Emphasis on Appearance (F(1,36)=0.46, p=0.502). There was also no significant effect of Bechdel (F(1,36)=0.678, p=0.416) or Region (F(1,36)=0.191, p=0.664) on Emphasis on Appearance. Yet, there was a significant interaction effect between Gender and Bechdel (F(1,36)=6.181, p=0.018) on Emphasis on Appearance. The post-hoc analysis showed that the effect of Gender on Emphasis on Appearance was significant for the movies that did not pass the Bechdel test (p=0.032). The p-value for the movies that passed the Bechdel test was p=0.209. No other interaction effect was significant (p>0.367).
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - The average emphasis on appearance of female characters was 0.7158 (σ=0.114) and male characters was 0.7239 (σ=0.127). In the movies that did not pass the Bechdel test, the emphasis on appearance of the female characters (μ=0.7199, σ=0.0829) was lower than that of the male characters (μ =0.7577, σ=0.07841). In the movies that passed the Bechdel test, the emphasis on appearance of the female characters (μ=0.7117, σ=0.14138) was more notable than that of the male characters (μ= 0.6901, σ=0.15687). Note that Microsoft Face API measures the face exposure in the range from 0 to 1.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Emphasis on appearance can also be calculated for frames where male characters and female characters appear together. If we define it as Relative Emphasis on Appearance, we get the following results. Gender had no significant effect on Relative Emphasis on Appearance (F(1,36)=1.135, p=0.294). Neither Bechdel (F(1,36)=1.573, p=0.218) nor Region Region significantly affected Relative Emphasis on Appearance (F(1,36)=2.481, p=0.124). All interaction effects were also not significant (p>0.176).
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Out of 348 types of objects that appeared more than 10 times in all the movies, people, living animals, clothes, and objects that are not clearly specified were excluded from the analysis (187 excluded objects). The female characters appeared with 154 objects, and the male characters appeared with 200 objects. A chair was the most frequently appearing object for both male (13.25%) and female characters (15.76%). Excluding chairs, the top ten objects that appeared the most with the female characters were Furnishing (9.37%), Seat (9.07%), Furniture (7.93%), Pot (6.18%), Cup (5.41%), Car (4.74%), Bottle (4.63%), Device (4.19%), Vessel (2.65 %), and Display (2.64%). The top ten objects that appeared the most with the male characters were a Car (8.51%), Furnishing (8.05%), Seat (7.06%), Bottle (6.08%), Furniture (5.91%), Cup (4.73%), Pot (4.24%), Device (3.99%), Display (3.39%), and Consumer Goods (2.68%).
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - According to previous studies, the frequency of appearance, age, body, personality, and social role of women compared to men are represented with bias in the media. Our image analysis system reaffirmed most of the important gender representation biases found in previous studies.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - From the results of temporal occupancy, we can see how much less frequently women appear in movies than men. In all movies, the temporal occupancy of female characters was 56% of that of male characters. This reaffirms the findings of previous studies that the ratio of male and female appearances in the media, such as television and film, is about 2.5:1. A 2.5:1 ratio means that women appear at a rate that is 40% of the male appearance rate. This incorrectly reflects the almost one to one ratio of female and male population in the society. Such a distorted population ratio in films leads people to falsely judge that there are more men than women in the actual population, and men are the cultural standard in society.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - In addition to temporal occupancy, female spatial occupancy was also only 91% that of the male rate. This can be interpreted in two ways. First, it may be because the female face is on average smaller than that of the male face. However, according to previous studies on the human face structure, the average size of a male face was 57 cm and a female face was 55 cm (96.5% of the male face).
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - This difference is smaller than what is revealed in our system. Secondly, in the process of casting actors, the film direction may have chosen women with smaller faces than usual. Indeed, there is a stereotype in Asia that women with smaller faces are more beautiful. Due to this aesthetic stereotype that exists in society, film directors may have tried to cast female actors with small faces. The third and most plausible explanation is that the directors intended that women comprise a relatively smaller portion of the screen than men. This hypothesis is supported by the fact that the difference in the face sizes of men and women is larger in frames where male characters and female characters appear at the same time (female face size is 88.5% of a male face). Making a specific character larger than other characters is the most basic cinematic technique for emphasizing the character (e.g., close-up shot). Thus, the fact that women are smaller when appearing simultaneously with men supports the hypothesis that the role of women is under-represented in movies. This includes situations where several female characters are present in a scene to assist one male character.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Our analysis system also revealed that the age of female characters in the movie is only 79.1 of the age of male characters. Such a portrayal strengthens the cultural belief of appreciating only young women. This further reduces the opportunity for older female characters to be on the screen, shortens the lifetime of the female characters, and quickly omits them from the screen. This is more evident from the distribution of ages in 11. In addition to reaffirming the fact that women are depicted in their twenties and men in their thirties to forties, it also shows that male characters have a more varied representation of their age over 50 years of age than female characters.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - An example of the network surrounding Luise Meier who helped Jewish refugees to escape from Berlin to Switzerland. In this network, a total of ten actors were identified as influential. A perfect match between the concept of influence and a measurement of their degree would mean that all of these influential actors would be present in the list of the top ten highest degree scores. Of the total of ten influential actors only Lotte Strauss, Luise Meier, Joseph Höfler, Jizchak Schwersenz, and Willi Vorwalder have also received the highest degree scores.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - The analysis results also reaffirmed the representation biases found in previous studies about the personality of women. Female characters showed more passive emotions such as sadness, fear, and surprise than male characters. On the other hand, anger and disgust, which are active emotions, were measured lower in females than in male characters. Also, happy emotions tend to be measured too much for female characters. As a result, male characters showed more varied emotional states than female characters, resulting in lower emotional diversity in female characters. If the essential nature of the narrative of a movie is to raise and resolve conflicts, female characters may not play a major role in the process.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - The intellectual image and the surrounding object types reveal a representation bias about the social role of women. Specifically, commercial movies put glasses on the male characters to portray professional, energetic, and intellectual images, and Asian and White male characters were more prone to this technique. Our analysis showed that the frequency of female characters wearing glasses was 45.2% that of male characters 12). Especially, as shown in the graph, men wearing glasses are found more at the end of movies. This indirectly shows that males, rather than females, are depicted as subjects for resolving conflicts in movie stories. In addition, female characters and cars were tracked together only 55.7% as much as male characters. On the other hand, the frequency of tracked scenes with furniture and female characters reached 123.9% that of male characters. This reaffirms the existing bias that women have simple and unprofessional occupations, such as full-time housewives, and that men have more social and professional occupations.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - The emphasis on appearance was calculated to determine the biased description of the female body. However, the results showed no significant difference by gender because, while previous studies have examined the degree of nudity of the female body, this study examined the degree of light exposure on the face. If image analysis techniques that detect the skin color of people in the movie frame are applied, a clearer analysis of the bias of female body depiction will be possible.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - The Bechdel test examines only the minimum requirements of gender representation in the movie. Nonetheless, the test has contributed to filmmakers contemplating gender representations within the film and making films that actually have less bias. In fact, our image analysis results showed that movies that passed the test have significantly less bias on temporal occupancy than those that did not. In movies that passed the Bechdel test, the temporal occupancy of female characters was 74.8% that of male characters. In movies that did not pass the Bechdel test, the temporal occupancy of female characters was 40.6% that of male characters. This illustrates the nature of the Bechdel test, which is sensitive to how often female characters appear in the film and interact with each other.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - There were significant gender differences in the following indices, but the tendency of the difference does not change with the Bechdel factors: emotional diversity, spatial occupancy, relative spatial occupancy, mean age, and intellectual image. This demonstrates that the Bechdel test is not sensitive to the bias present in the visual representation of the character. Our system complements those limitations of the Bechdel test through automatic image analysis of movie frames, allowing us to respond more intimately to todays increasingly diverse gender issues.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - The preliminary survey showed that the way the audience remembers the characters can vary depending on the gender of the character, whether the film passed the Bechdel test or not, and where the movie was released. From the results of the image analysis, it was confirmed that the visual representation bias in the film influenced the audiences memory of the characters.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - The lower temporal occupancy of female characters explains why viewers remember women less than men. The reason why female characters are less remembered in action scenes is explained by the fact that objects tracked with female characters are mainly indoors. In addition, viewers remember males more in scenes with heated emotion because of the lower emotional diversity of women. The reason female characters are remembered more in love scenes may be related to the value of emphasis on appearance of female characters. However, as described earlier, the emphasis on appearance needs to be redefined and calculated relative to the body, not the face of the character.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Participants in the survey were more likely to remember male characters than female characters in Korean movies than in Hollywood movies. Because the survey was conducted for Koreans, this difference can be attributed to various confounding factors. However, our image analysis system showed that there is actually a larger visual representation bias between male and female characters in Korean films when compared to Hollywood movies. This indicates that the discussion on gender bias is still lacking in Asian society, compared to Western society.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - The preliminary survey included 10 Korean movies released in 2017, but the analysis of the images further examined 10 Korean movies released in 2018 as well as the movies targeted for the survey. Because the Korean Film Council first published gender statistics for Korean films in 2017, it will be possible to compare how gender bias has improved in the Korean films released in 2018 when compared with the previous year.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Further analysis shows that the temporal occupancy of female characters was 12.3% (σ=8.9%) and that of male characters was 34.6% (σ=5.0%) in the 10 Korean movies in 2017. In the 10 Korean movies released in 2018, the temporal occupancy of female characters was 11.9% (σ=9.4%) and that of the male characters was 32.0%, which remained almost unchanged from 2017 (σ=6.7%). This suggests that the Korean society needs to focus more on gender representation bias within the film, and we envision that this study will be a contribution to such a process.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - This means that five influential actors have received lower degree scores than actors who were considered to be non-influential.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - The gender representation bias within a movie can vary depending on the genre. Although this study attempted to include as many different genres as possible in the analysis, it is beyond the scope of the study to identify bias differences between different genres. However, if the indices measured for each movie are defined as a feature vector, we can represent each movie as a feature point in a 14-dimensional vector space. For the 14 dimensions, only the indices showing significant differences by gender factor are considered (emotional diversity, spatial occupancy, relative spatial occupancy, temporal occupancy, mean age, intellectual image, and emphasis on appearance for each gender). By visualizing how the films are distributed in the feature space, we can get a glimpse of whether gender bias has a meaningful association with the movie genre.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Points in such a high-dimensional space must be mapped to a two-dimensional plane to be visualized. For this, we used the t-SNE algorithm (perplexity = 9) provided by MATLAB. The algorithm allows the relative distance relationship between the points in the high-dimensional space to be maintained after mapping them to a two-dimensional plane. At this time, standardization was carried out for each index. Figure 13 shows the resulting visualization. Movies in the action, sci-fi, or romance genres are shown to be naturally clustered. This suggests that films belonging to the same genre have similar patterns of gender representation bias. This could be further investigated for more movies in the future.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - The film analysis method proposed in this study has several limitations. First, this study extracts raw data for the calculation of gender representation indices using off-the-shelf image analysis techniques such as Microsoft FaceAPI or YOLO9000. These techniques are not perfect and may have already been trained by datasets that are biased toward specific target groups. For example, a word embedding function trained on Google News articles returns a close occupation with a male as a maestro or skipper, but it returns a close occupation for a woman as a homemaker or nurse. Further, recent studies have shown that most face classifiers today are better able to recognize the faces of men than those of women (8.1% difference in error rate for Microsoft FaceAPI, in April and May 2017). That means that roughly 8% of the faces judged to be females could actually be male faces. This implies that the gender representation bias in real movies can be larger than that measured in this study. YOLO9000, which we used to track objects, also has room for improvement in its classification precision.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Secondly, some of the indices that we have proposed tend to be overly simplistic. For example, evaluating a characters intellectual image by whether or not glasses are worn, even if it is a socially acceptable symbol, is a controversial simplification in interpreting the results. Such simplification was intended to expose gender bias more effectively and effectively, but it certainly needs to be improved in the future.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - The analysis method proposed in this study can be supplemented by better face and object tracking techniques in the future. For example, InclusiveFaceNet maintains the diversity of gender and race in estimating face attributes. In addition, recently released YOLO v3 has significantly improved the performance of YOLO9000. However, just because perfect techniques have not yet been developed, new approaches to gender bias using machine learning techniques should not be considered meaningless. Just as better machine learning techniques can improve the methodology proposed in this study, the results of this study may also deliver insight of gender sensitivity into improving existing machine learning techniques.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Through the films, audiences expect to explore and understand the situations that may occur in real life. Therefore, the way the movie portrays the world can have a significant influence on the thoughts and behaviors of the people who watch it. The current study looked at differences in the way female and male characters are portrayed visually in commercial films and showed that differences can actually affect audience memory and perception. Existing methods for evaluating gender bias in movies, such as the Bechdel test, do not consider the fact that the film is a visual medium. The proposed system in this study complements existing methods by analyzing the image frames of movies and automatically extracting eight indices for each gender, which represent the way characters are portrayed in the film. The results revealed that most films portray the female characters with bias, especially on indices like emotional diversity, spatial occupancy, temporal occupancy, intellectual image, mean age, and emphasis on appearance. Furthermore, by analyzing the types of objects that are tracked together when characters appear on screen, our system showed that female characters appear indoors and are described in a less dynamic way than male characters. We also investigated the extent and manner in which the audience recalled each gender character in commercial films, and the results of the survey (with 608 participants) were in line with the gender representation bias presented by the image analysis system. Such bias in the representation of the female characters in commercial films may establish false perceptions as well as inaccurate cultural standards for the audience, leading to solidifying particular images on women in the real society.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - Progress in the contemporary film industry necessitates the abandonment of biases and the transformation as a whole. While typical moviegoers tend to be more women than men and to be more non-white than the white, the major movie production companies still produce in favor of a specific target audience. If they continuously adhere to the action or adventure movies full of male characters without any modification, they will no longer attract new audiences and will soon lose even existing ones. It has been proven through numerous studies that a movie succeeds more commercially when it includes more genders, classes, and races in the scenario. Hence, the contemporary film industry should listen to more diverse stories from dissimilar people and translate these stories into movies.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - RESULTSIn a first step, I measured the matches between high centrality scores and my definition of influence. An example may help to read these tables: The network surrounding Luise Meier had ninety-nine actors in it. Ten of them were considered to be influential. The row Degree indicates that for this measure, five actors were found in the top ten high scoring actors. This means that degree centrality would have identified fifty per cent of the influential actors.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - The system proposed in this study did not distinguish different characters of the same gender within a movie. Our future work endeavors to track and analyze the representation of an individual, specific character within the entire movie. In addition, this future work will attempt to analyze the differences in the representation of female and male characters depending on the film genre and will also strive to measure the duration of the actual conversations among the characters. Today, various video contents inundate our society, and a growing number of people are enjoying video content on platforms like YouTube or Netflix. Therefore, our system can measure gender representation bias not only in movies but also in other kinds of video production. Furthermore, the system can also generate background data for research in film and mass media. These statistics on gender expressions can allow us to understand the periodical trends and even provide a reasonable discourse and reflection for policymaking.
[Author: Ji Yoon Jang; From essay:"Quantification of Gender Representation Bias in Commercial Films based on Image Analysis "] - The ultimate purpose of the study on the portrayal of gender is the pursuit of “gender justice,” and it is highly concerned with sexuality, race, ethnicity, and class. Often, films are described as showing bias not only to women but also to other minorities, such as certain classes, races, sex minorities, and people with disabilities. While we limited our analysis into binary gender classification in the current study, the future work will aspire to extend and analyze the broader representations of these social minorities as well.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - INTRODUCTIONIn social network analysis, centrality measures are used to translate empirical and common sense observations of social behaviour into mathematical expressions. In order to assess how well an algorithm performs in conditions of imperfect data, researchers typically first select either a random or a real-world network, compute a variety of centrality measures, and declare these values to be their point of reference. In a second step, they manipulate these referential networks by adding or removing nodes or ties, again either randomly or following a set of rules. They then compare the centrality measures of the referential network to the ones gathered from the manipulated network.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - This approach helps to shed light on the impact of false and missing data on centrality computations, and also helps us to assess the ability of these algorithms to describe social reality (as we reconstruct it) itself.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - It is surprising that the effectiveness of centrality measures to accurately describe notoriously vague concepts such as power or influence has not been used alongside empirical observations more often. In this chapter, I will compare the performance of common centrality measures with the results of an in-depth reconstruction of six historical networks: in this case, support networks for persecuted Jews during the Second World War. Data were extracted from historical narratives, contemporary and retrospective autobiographical reports, interviews, applications for remuneration, and police interrogations. These sources provide a high level of detailed contextual information about the respective ties and actions they represent.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - THE CASE STUDYIt has now become common knowledge that a small minority of Jews managed to survive the Holocaust in hiding and with support from a small and diverse group of helpers. Soon after the end of the Second World War, historians, sociologists, (social) psychologists, and scholars from many other disciplines began to analyse stories of help and survival and found several answers to what seemed to be the key question: Why did helpers decide to help? A large part of the sources that are available for research were collected by the Israeli memorial and research centre Yad Vashem. The institution is most famous for awarding the title Righteous among the Nations to individuals who were proven to have helped in a selfless manner.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - Many social scientists came to the conclusion that helping behaviour was a consequence of certain common characteristics among all helpers. Samuel and Pearl Oliner argued that they were driven by an intrinsic sense of morality and altruism and that a specific form of upbringing, including strong ethical and political values, could explain their actions. Others looked at their socio-demographic background, for example, their education and wealth.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - However, historians have shown that helpers not only differed with regard to the moral qualities but also the intensity of their actions and the ways in which they were active. Their studies confirm that helpers came from all sorts of social backgrounds, had different motives, gave a large variety of different reasons to explain their behaviour, and had varying incomes and socializations. Case studies suggest that even the self-proclaimed motives of helpers underwent processes of conscious or unconscious reinterpretation and are thus not necessarily to be trusted.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - Both in Germany and in the occupied countries, helpers and refugees acted under extreme pressure in a hostile environment and had to expect to be arrested immediately after their activities attracted the attention of anyone willing to denounce them. Consequences for helpers, scope for action, and available resources, however, varied considerably between Germany and the occupied zones, as well as between the various occupied zones. Probably the most important difference between Germany and the occupied zones was the absence of organizations whose infrastructures could be used to help Jews and other refugees.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - My approach neglects an international comparison in favour of an in-depth analysis of network structures that emerged under similar conditions, namely in Berlin from 1942 onwards. In Germany, the vast majority of people were, or at least had to be considered as, devoted Nazis or compliant to their rules; any requests for help had therefore to be made very cautiously and based on trusted relationships. Refugees faced regular checks by police and Gestapo, first targeted at finding Jews and later at finding young men who had deserted from the Wehrmacht. In addition, they had to fear the so-called Greifer; Jews who were pressured by the Gestapo to find and report other refugees and were promised freedom from persecution for themselves and their families.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - These dangers, together with the regimes efforts to control black markets and any other form of deviant behaviour, meant that any written account of ones activities represented a significant threat. Gestapo agents interrogated anyone they associated with support activities in order to identify all collaborators. Transcripts of these interrogations can be quite informative, although they may contain (consciously) misleading or false information. The majority of the available sources were thus produced after the war. A larger number of helpers and refugees first gave evidence of their actions in the course of applications for reparations. Detailed questionnaires asked about their political activities, experienced persecution, physical and material damage, involvement in resistance activities, religious beliefs, and an extensive résumé. They were then asked to write down their stories. Designed with refugees and resistance fighters in mind, these documents were meant to cover provable participation in anti-Nazi activities and cases of illegal expropriation by the state. They were not meant to cover the practice of help and survival. In fact, regulations at first did not even consider helping Jews to be an act of resistance against the state.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - Any information the applicants provided has therefore to be weighed against their interest in receiving reparation from an institution that was not necessarily acting in their best interest. In 1958, Berlins senator for the interior, Joachim Lipschitz, brought forward an initiative to honour helpers in Berlin. Those who could demonstrate an honourable lifestyle and witnesses to their actions were granted a small pension and a public acknowledgment of their help. Following this logic, prostitutes for example, could not be honoured. Again, administrators collected reports and data about both helpers and refugees that are now available for research.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - Beginning with the applications for reparation, all sources were thus produced in settings which encouraged stories of virtuous helpers, since the respective institutions explicitly ruled out acknowledgment of ambivalent or dubious motives. Somewhat more outspoken are reports by survivors. They, of course, focus on their story of survival; their purpose is to tell their stories from their own, often limited, point of view and are therefore not without omissions, distortions, and false memories.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - Oliner and Oliner showed that roughly two-thirds of all helpers whose cases were documented in the Israeli memorial site Yad Vashem responded to requests for help. The vast majority of all helpers collaborated with others in order to facilitate their support for refugees. This suggests that the decision to help was not only a question of personality but also one of social embeddedness.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - In my research, I understand the decision to help and its practice as a social process. In this process, helpers typically responded to requests for help and used peers to approve of and reinforce their belief systems, which eventually led them to act differently from the majority of the society they lived in. Many of the aforementioned studies aimed to measure helping behaviour both statistically and through the comparison of individual cases.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - I reconstruct in a formalized, and thus comparable way, social networks between helpers and refugees in Berlin, in order to discuss their importance both for the motivation to help and the ability of refugees to sustain a life underground. Relational data is used to map the complex relations which emerged between both helpers and refugees and among helpers.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - All sources are heterogeneous regarding the circumstances of their creation and the purpose they once served. This also means that their view on the respective networks differs: some, like survivors accounts, tend to be descriptions of an ego network with strong emphasis on the narrator. Others, such as historical reconstructions, aim to cover the actions of a specific group of actors. This means that the extraction of relational data is based on a fragmentary patchwork of information and not on a homogeneous dataset.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - This study took into account all available and trustworthy information on explicit acts of help as described in the primary sources that were available, the network boundaries are therefore merely defined by the actors and acts of help described in the material.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - All interactions between helpers and recipients of help were coded into a database which describes the practice of help and the intensity of relations between two actors. Among them are information on the specific form and endurance of help, the date of their first encounter and a rough categorization of their motives. For each class of relations and for each type of tie within this class I developed definitions accompanied by examples. This approach is widely used in qualitative data analysis and acts as a bridge between the often fuzzy primary sources and the rigid coding systems. The type of tie Food and food stamps in the class Forms of help, for example, was defined as follows:
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - Food and food stamps came from many different sources. Among them were dinner invitations, donated food stamps, black market trade and forged food stamps. If the respective food/food stamps were acquired through unidentifiable black market traders, the label Black Market was given. All ties which were concerned with the trade and brokerage of food/food stamps are also covered by this code. Acts of self-help such as restaurant visits or theft are not covered by this code.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - This definition is accompanied by examples taken from primary sources, such as:
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - She soon received her food from these people and felt at home there.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - But she offered to help us, we gave her our food stamps, which we had bought on the black market, she gave us a double portion for the regular price.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - The giver and recipient of such an act of help have been recorded in the database and in the class Form of help the value 4 which stands for Food/ food stamps was entered. This means that helper and recipient share a tie of the weight 4. Similarly, 2 stands for brokerage and 3 for accommodation. Each one of these ties was then further classified with regard to the time range, the duration, etc. This creates a multiplex relation between givers and recipients of help for each act of help. Actors can be connected through various forms of help at the same time. This approach makes it possible to visualize each act of help as a tie between two actors which contains information regarding the form of help but also, for example, the period in which the help was granted.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - The categories of the database were developed during the analysis of four distinct support networks. Each tie has been cross-referenced with other sources and interpreted based on contextual knowledge where possible. Sometimes detailed information about, for example, the duration of an act of help could not be retrieved, in these cases the label unknown was given. Acts of help which could be inferred but addressed unidentifiable actors (a fictitious example: and then she helped three more refugees) were acknowledged by entering ties between the helper and, for example, Refugee_1, Refugee_2, Refugee_3. While without doubt controversial regarding any mathematical descriptions, this approach was appropriate for the primarily visual-exploratory analysis of the networks in this study and deemed preferable to no recognition at all.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - Overall, network visualizations of relations between helpers and refugees aid the exploration of the complexity of these relations and connect the actions of individuals with the developments of larger structures. This way, the complexity of social relations changes from an obstacle to the object of research. However, the resulting relational structures can only be interpreted in a meaningful way when considered together with the detailed information and specifics of the original sources.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - METHODThe networks I selected for this comparison differ considerably with regard to data quality and in the ways in which they were active in helping refugees. Some, like the networks surrounding Franz Kaufmann, connected hundreds of individuals and facilitated the exchange of resources through long network paths and specialized clusters within the network. Other networks were far smaller and mainly connected by a few refugees who received help from a limited number of helpers.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - The title of this paper asks whether centrality measures can be considered as reliable indicators of influence within social networks. Any attempt to test this, of course, requires case-specific definitions of reliability, influence, and social networks. Here, I define ʻinfluential actors as those who were responsible for the emergence and functioning of the covert support networks I have studied and not influential actors as those who played a passive part and were less involved. In order to count as influential, actors needed to share one or more of the following characteristics: they needed to have frequently initiated the helping behaviour of others, provided particularly rare resources such as forged documents, and/or frequently provided more accessible forms of help such as food or food stamps. Accordingly, the following forms of behaviour do not count as influential: merely receiving resources, or one-off or irregular acts of help with regard to accessible resources. Actors that correspond to the definition given above are to be considered influential and were marked as such in the database. With the exception of the Kaufmann network, which is far larger than any of the others, the number of these particularly influential actors is around eleven in each network. Given the nature of helping behaviour as a social practice which was highly dependent on the creation and usage of many far-reaching social ties, we may conclude that this definition of influence is close to the assumptions which underlie common centrality measures. Reliability is understood as the correlation between influence and high centrality scores calculated from the data.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - Measures were then computed with the following settings: there were no restraints concerning time, form of help, etc. Relational data collected for the years 1938-1945 was aggregated. This inevitably leads to a problem of anachronism: the networks may contain paths that never existed at the same time, for example, a tie that existed in 1938 could now bridge two clusters which did not exist before 1944. However, trials with networks collected in six-month steps between 1938 and 1945 showed that the resulting structures do not necessarily produce more adequate representations. This is due to the fact that many ties which were very likely to have existed were not explicitly mentioned for each and every six-month period. Centrality measures will therefore be more likely to reveal influential actors when applied to the aggregated dataset.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - Measures for the frequently used betweenness, eigenvector, and closeness centrality algorithms were computed as well as degree, in-degree, and out-degree. For the sake of comparison, the PageRank algorithm which is not typically used for the analysis of social ties was included as well. Duplicate edges were ignored. With the exception of the computation of degree centrality, the networks were treated as directed graphs which made it possible to distinguish helpers from recipients of help. Note that this functional definition does not correspond to the distinction between people considered Jews or Aryans by the German legislation at the time and that ties between nodes represent acts of help and not mere acquaintance. While it would be highly desirable to produce an overlay of helping behaviour within a pre-existing social structure, the sources do not contain sufficient information to facilitate this.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - The degree of an actor therefore represents the number of acts of help in which he or she was involved. Directed networks help to further differentiate this: the in-degree indicates help that an actor received; their out-degree measures how many acts of help they provided.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - Overall, centrality algorithms performed very well in the case of the Schönhaus and Onkel Emil networks, with 6 and 5 scores above seventy per cent—I thank the reviewers for highlighting this aspect. The networks do not stand out regarding the quality of available data. They do, however, differ regarding the permanence of key actors in the two networks: both networks are characterized by one close-knit cluster in which a stable group of key actors interacted with each other for long periods of time compared to the other networks in which changing constellations of key actors collaborated across several clusters.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - It becomes apparent that the matches are in most cases rather low, averaging 53 per cent across all algorithms and networks. This means that only half of the influential actors also received the highest centrality scores. Out of six case studies, no algorithm was able to top the 70 per cent benchmark more than once or twice. This threshold of 70 per cent is an arbitrary value but represents a significant match between influence and centrality which can still be considered useful in empirical research. As a first result, we may therefore conclude that for this case study, with this definition of influence and this methodological setup, centrality measures are not able to reliably identify influential actors.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - But what if we stop looking for exact matches and ask how many influential actors are present in a bigger group of actors who score high in centrality? In other words, are influential actors at least likely to receive high centrality scores? I increased the size of this group of high scoring actors to twenty per cent of the total number of actors in the respective network. This number was again picked based on the pragmatic consideration that for this still rather small group of actors, it would be feasible to do additional in-depth research on individuals and their activities.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - Influential actors do indeed quite often reach the top twenty per cent highest values of most centrality measures: the match now averages 70 per cent across all algorithms and networks. This means that roughly seven out of ten influential actors can be detected by an algorithm.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - Overall the match between high centrality measures and actual influence according to the definition varies considerably between 100 per cent at its best and 27 per cent at its worst. In practice, this means that no one algorithm is suitable for reliably detecting influential actors in a network.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - The matches between influential actors and highest centrality scores as computed in the first run are depicted in black. Coloured in light grey are influential actors who are part of the group with the twenty per cent highest centrality scores.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - Some noteworthy differences become apparent: degree centrality, the most robust and the simplest measure, yielded the best result with 81.5 per cent accuracy, followed by the out-degree measure. The in-degree measure yielded the worst match. This is not very surprising as, by definition, an outgoing tie signifies an act of help by an actor while an ingoing tie signifies help that was received: it is unlikely that influential helpers would have been the recipients of a particularly high number of acts of help. However, it remains to be seen whether this effect can be reproduced with different data. It becomes apparent that (out-) degree centrality, which only takes into account the ties of an individual actor, performed much better than the other algorithms which take into account the network structure itself: betweenness centrality refers to the entity of shortest paths within a network which go through a node, closeness centrality refers to the shortest paths between a given node and all other nodes in the network, while eigenvector and PageRank centrality both take into account the centrality of neighbouring nodes. This finding suggests that influence and acts of help to many different actors correlate.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - In conclusion, we can argue that centrality algorithms can indeed yield acceptable matches within a more generous range of the top twenty per cent highest centrality scores for any given network. Chances to identify influential actors among the high scoring actors, of course, increase with the size of the respective thresholds. In practice, given the unpredictable variations in how well an algorithm performs on a single network, it would be more advisable to compute measures with several algorithms and to average their output.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - Despite this encouraging overall result, we need to acknowledge that this approach will always miss a minority of influential actors with low centrality scores who were not very well connected within the network as it was extracted from the sources. This can either suggest that they fell into the false-negative category because they did not need to be highly connected to be influential, or that knowledge about their influence was based on contextual knowledge which could not be represented in the database.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - It is worthwhile to examine these actors in a little more detail and to ask why their influence has not translated into high centrality scores. All influential actors were highlighted in their respective networks using larger node symbols. Each network had one or two actors who were of critical importance for the organization of their activities. These initiators were connected to a majority of actors in the network and therefore appear central in all spring-embedder-based visualization algorithms. With the exception of the rather small network surrounding Cioma Schönhaus, there is clearly a tendency for influential actors to be well embedded and close to the networks initiators.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - Still, some highly influential actors are found in peripheral positions, and numerous very well connected but not particularly influential actors are found close to the centre of the networks, in direct contact with the initiators. This raises two questions: why is it that well-connected actors in the centre who must have been involved in many acts of help turn out to be less influential than some actors in the periphery? And how could these peripheral actors be so influential?
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - A re-examination of the actors in question leads back to the histories of the individual networks and the people who were involved in them. Most of the influential actors who ended up in the periphery turn out to have been affiliated with other support networks which were not considered in these computations in order to not distort measures for the individual networks. From this position, they acted as brokers who facilitated the exchange of essential resources across distinct networks. The manually annotated influential actors then may be considered as indicating hitherto undiscovered, yet important parts of the network in question. As such they serve as important pointers towards further analysis.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - In the case of the Kaufmann network, for example, it becomes apparent that many of the less influential actors in the centre had been involved in the Bekennende Kirche, a German protestant church formed in reaction to the Nazified Deutsche Christen. The network was rooted in this social milieu and many priests and church members had been involved with support activities for church members who had been classified as Jewish in the late 1930s and early 1940s. However, their willingness to help was often limited to fellow Christians, and more importantly, by the legal status of their actions. Most of them shared trusted ties with Franz Kaufmann, the initiator of the network, but chose not to get directly involved in any illegal activities. Instead they became passive bystanders or else marginally involved in the networks efforts to produce and trade forged documents and food stamps. This visualization can therefore tell us who had high potential to become involved with the network and raises the question as to why they did not.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - DISCUSSIONThe reference point of this study was the historical reconstruction of the six networks. This approach can only identify actors who made documented contributions and assess to which extent influential actors also receive high centrality scores within their respective network. Two types of actors were often missed by centrality measures: those who as friends of a friend had brokered crucial contacts with other networks and those (often refugees) who had irregular contacts with support networks and provided essential resources on only a few occasions.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - The centrality algorithms which were tested here were designed with simple models of social networks in mind and were not meant to do justice to the fallibilities and inconsistencies of real-world (historical) network data. Quite surprisingly, centrality algorithms performed very differently on the six networks: none of the notions behind the different algorithms seemed to be able to identify influence significantly better than another or could be linked back to the different structures of the networks. The fact that degree centrality, the most basic algorithm, performed best is another indicator of this.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - We must remember that the ties that were studied here are explicitly action based: only acts of help could be extracted from the sources, mere acquaintances were not considered. Influence correlates best with a high number of acts of help to different actors. If the logic behind centrality measures cannot be directly transferred to empirical action-based networks, we can assume that there will also be problems with networks that use proxies as indicators for social relations, such as letter-writing or mutual memberships in social groups, for example.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - The results of this study lead me to conclude that for data of this kind, the notion of centrality and its mathematical and visual expression is most fruitfully understood as a potential for centrality which was either fulfilled, or—as the above-mentioned example from the Kaufmann network shows—for one reason or another remained unfulfilled. A high centrality score in any of these networks is therefore not necessarily a reliable indicator for actual influence. Centrality measures tend to hide less-connected influential actors from us and at the same time promote non-influential actors with high centrality scores.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - Still, we can expect that on average c.80 per cent of all influential actors can be found in the group of actors with the 20 per cent highest scores. In practice, this can be a useful way to narrow down the list of potentially influential actors and steer the allocation of research resources.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - Perhaps more interesting are the contradictions which emerge when a hermeneutic analysis of primary sources is confronted with the simplistic models behind algorithmic computations. These mismatches can bring to our attention previously overlooked parts of a network or reveal an existing but unused potential to act. Such an open approach to network computation and visualization is arguably well suited to making the most of the exploratory and hypothesis-driven work with network analysis tools and concepts.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - CONCLUSIONThis article tested the accuracy of centrality measures using detailed empirical knowledge of six covert support networks for persecuted Jews during the Holocaust. The in-depth historical analysis allowed us to annotate particularly influential actors and to compare this list to the output from common centrality algorithms.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - Centrality measures are useful to narrow down the list of potentially influential actors in a network but will always fail to detect those c.20–30 per cent of actors whose influence does not correspond with above average connectedness.
[Author: Marten During; From essay:"How Reliable are Centrality Measures for Data Collected from Fragmentary and Heterogeneous "] - This chapter suggests that a more promising way to integrate the otherwise too simplistic centrality models in historical research: the comparison between empirical observations and computations itself and the new questions it raises is an example of how traditional historical research methods and computations can benefit from each other.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Over a generation of historical scholarship, the way in which historians research and write has dramatically changed. While many of these changes have been individually small, cumulatively they represent a transformation in the way that historical scholarship is researched, written, and published. These changes have been seldom theorized or explicitly discussed. Younger historians may not know what research was like before the Internet, important when they critique earlier scholarship. Older historians have experienced a process that eludes comprehension, as everyday interactions with the Internet shape how they search and engage with sources. Historians must take stock of just how drastically technology has transformed their scholarship.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Imagine a historian researching in the year 2000. The historian would first choose a project, explore the secondary literature, and then find primary sources of interest by looking at what other scholars had cited or by phoning or arranging interviews with subject-specialist archivists. They might even consult massive tomes which aggregated available resources and listed archival repositories. The historian would then travel to the archive, perhaps carrying out some photocopying (at expensive prices of 25 cents a page or more), but would primarily carry out in situ research: taking notes on a (still-then bulky and expensive) laptop computer or notepad. This archival work would be complemented by microfilm work, either in the archive or at home thanks to the inter-library loan of reels: day after day of scrolling through microfilm reels, painstaking work that incidentally also exposed the historian to rich historical context. While what had been microfilmed reflected the biases of a previous generation, the way in which a historian manually explored documents resembled earlier paper-based exploration.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Historical research in 2000 was slow and laborious compared to today. This was thanks to the considerable outlay of time to navigate information and the specialized expertise to triage and search it effectively. Archives were chosen sparingly, microfilm reels with care, reflecting the intensive labour needed to explore these repositories of information. This is not to paint an unduly utopian situation. Archives have always varied in their accessibility (hours, location), organizational acuity (funding and comprehensiveness), and beyond. Crucially too, while the year 2000 is posited for the purposes of this thought experiment, it could just as easily be 1990 or 1980. The changes over the last two decades have been dramatic.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Fast-forward to the present day and historical research is carried out very differently. Comparing these two historical workflows is useful to see just how dramatically historical research has changed. At 40,000 feet it may look the same. Historians identify problems, find sources, study them, and publish. But the detailed on-the-ground work of a historian has undergone a digital revolution.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Historical research in the digital age begins in familiar fashion: choosing a project and exploring the historiography. Yet the way in which a historian finds primary archival sources is very different: Google searches for archives, consulting archival websites, and navigating extensive digitized finding aids. At all stages, the historian balances what is and what is not digitized. This was in play with microfilm too what was and was not filmed – but more material was microfilmed than is currently digitized. Some archives are still worth traveling to, but there is a different and new cost-benefit analysis at play. Online resources are consulted more, the non-digitized less, a historical application of the Matthew Effect where the digitized get richer (in citations) and the non-digitized poorer. Given the high cost of digitization, this process tends to – some projects have consciously tried to counter this trend – privilege sources held by affluent institutions across the Global North, with implications for the ensuing diversity of voices and perspectives in our scholarship. While many of these shifts build on pre-digital trends, historical information is always mediated, whether this is due to the choice of what to microfilm or to broader archival biases yet the transformation of the digital age represents a dramatic acceleration.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Historical information is mediated through new and emerging technology. When a historian does travel to an archive, their reading room activity is different than what it would have been just two decades ago. Trips are quicker, with the scholar standing over archival fonds with digital camera in hand, collecting thousands of photographs (along with incidental back pain). The historian does most of their actual reading and analysis at home. The same is true with other periodicals, newspapers, and journals: historians explore large repositories of information, from places such as JSTOR, ProQuest, or HathiTrust, through the constant lens of search. They use keywords rather than expert indexes. Here too historians find themselves consulting the digitized rather than what might be most relevant. This is not out of laziness, but rather the diminished returns of consulting a newspaper that is not digitized while a roughly equivalent periodical might be. It might not seem like a significant decision to explore the Toronto Star rather than the Toronto Telegraph, but if every historian makes the same decision, this represents a dramatic shift. These thousands of individual decisions mean that over time scholarship begins to homogenize in terms of what we cite. While these forces are most pronounced for historians who draw on typeset documents – those most amenable to optical character recognition algorithms – recent advances in handwritten text recognition also portend the continuing expansion of technologys impact. While microfilm had the effect to some degree on an earlier generation of scholars, digitization is so much more circumscribed and its access via keywords represents a change of an entirely different magnitude.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - The possibilities offered by digital sources came to the forefront of many historians minds during the prolonged COVID-19 pandemic. With reading rooms closed, travel restricted, and physical access to libraries intermittent during various waves of lockdowns and restrictions, historians adapted as best they could. Historical scholarship was impacted (also by increased caregiver responsibilities and the pandemics trauma itself), but thanks to digital media, in general research did not come to a complete standstill in most cases. Many historians continued to research. As with everything, COVID accelerated but did not invent trends: it underscored how digitally mediated historical scholarship now is. Historians were able to leverage processes that had been unfolding over decades.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - This dramatic transformation has unfolded over the span of two decades. It is difficult to think of a single element of a historians research workflow that has not changed over this period. We look at old acknowledgements and remark on how they underscore the research practices of a past generation: the wife of a famous historian who apparently did all their work, or a scholar who churlishly dis-acknowledges an archive. But they are windows into how work is carried out. These transformative forces are accelerating, especially as historians begin to leverage the vast arrays of born-digital sources (those that begin life as digital objects, such as a website or a Word document) that will reshape the landscape of historical records for topics studying the 1990s and beyond. Indeed, this was the subject of my last book. But the forces described in this Element are applicable to more historians than just the (current) minority drawing on born-digital sources. Indeed, an understanding of how historical work has been transformed is especially important as historians tend to neglect methodological discussion. Let us explore these transformations – and learn how we can be better scholars by making the digital explicit.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - This Element explores how this technological transformation has unfolded and what its impact will be. Yet it is not a how-to guide. This Element rather explores how this digital turn is changing historical scholarship and practice. The public - and even some historians can sometimes see historical scholarship as objective. From this perspective, archives are understood as passing along to historians stories from the past from which they write history. Yet historians are of course influenced by their working conditions. Do they read documents on a screen or not? Can they access funding to travel to the archives that they want to? Do they have technical ability and knowledge to understand the knowledge systems they are using? Can they understand how to conduct keyword searches properly, do they grasp the underlying constraints of optical character recognition (OCR) in the database they are searching, or grasp what was or what was not digitized? Do they have children, constraining working hours or travel opportunities (making databases an especial godsend)? Are they in the developing world and accessing a website too overbuilt for their tenuous internet connection? All these mundane questions shape historical scholarship.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - This transformation is neither wholly negative nor positive. Few revolutionary shifts are. Gains include rapid access to sources, quick fact-checking, the ability to search over decades and continents of historical sources, and – significantly the ability to spend more time with family, teaching, and other duties and less time in faraway archives scribbling in notepads. The democratizing potential of these shifts cannot be ignored. Similarly, historians can use keyword search to amass large corpora of information. They now generally operate on new, larger bodies of information. Yet there are losses. Notably, historians can lose an understanding of historical context when keyword-searching directly to sentences or individual documents removed from their broader context. Historians are also using information retrieval systems that are not understood, the contents of which are in turn shaped by digitization bias. Similarly, this new scope of digitized research brings advantages and disadvantages. Search brings us many more results, but we still tend to evaluate research claims on a pre-digital level of sourcing. As historians now systematically explore thousands of articles with algorithms, perhaps our norms need to change and now require a half-dozen or more ‘hits to rise to the same level of significance we might have looked for in a pre-digital period. The way in which we evaluate scholarship via peer-review and scholarly assessment also need to take this new information ecosystem into account.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Sitting at a computer all day also represents significant change in how historians approach their research questions. We are perhaps losing the experiential knowledge of a place at a time when parts of our profession are underscoring the importance of community and place-based research. Yet there is no going back; the digital genie is out of the bottle. Whether this transformation is positive or not depends in no small part on the critical ardour with which historians approach their digitally transformed world.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Through careful and critical use of digital technologies we can ensure that this transformation is a net gain. Issues of context present the biggest challenges. Most digital systems are not designed to reveal context and are instead focused on keyword search. With better training and conscious digital research methods, however, we can countenance this to some degree. This may also require a collective change in the ways in which historians approach the level of citation needed to establish a scholarly claim, thanks to the larger amounts of information that we all increasingly operate on. With more user demand, more platforms can adopt an approach which facilitates context-aware reading, an approach adopted by the Internet Archive. Digital historians have also pushed the literature forward on the provision of context, informing primary documents through network analysis and trends. Experiential knowledge also raises questions, requiring a deeper conversation about what we as a profession value. As a self-governing profession regulated primarily through peer review, we can collectively choose the direction that we want historical scholarship to go and what our values are.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - This will require a transformation of our profession in four main ways, explored in this Elements conclusion. These are a recognition of digital literacys importance, valuing interdisciplinarity, a prioritization of methodological discussions, and changing how we train future historians to incorporate new and emerging technologies. Despite the challenge before us, historians have a good foundation, especially vis-à-vis our use of context, by which to rise to these challenges.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - This Element brings the conversation around the transformation of historical scholarship together into a sustained micro-monograph. Its goal is to equip historians to be self-conscious practitioners in a digital age. The Element will do so through three substantive sections after this introductory one, followed by a substantive conclusion.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - This was the ‘Matthew Effect of digitized resources in action: the rich (digitized) got richer in terms of use, whereas the poor (un-digitized) got poorer. It was clear that Huggins and Cold North Winds digitization efforts had substantially reworked the ways in which Canadian historians carried out their research. Given the discoverability issues attendant with early-2000s OCR (a word accuracy rate of below 90 per cent was likely), historians were using tools they did not really understand and allowing it to reshape their scholarship.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - The second section, Libraries and Databases, explores how aggressive digitization, especially of newspapers and microfilmed resources, has created massive exploratory databases. Historians need to think about their construction and explore these platforms consciously. Algorithmic bias and selective digitization practices have comprehensively transformed how historians parse information. How has everyday technology transformed the work of historians, from 1930s microfilm to twenty-first century databases? We have gained dramatic access to primary sources, but historians need to ask questions about what has been digitized, and what has not? How has something been digitized? What are the impacts of copyright on these repositories?
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - The third section, Archives and Access, explores the long, intertwined existence between archives and historians with special focus on digital technology and source mediation. Historians are expert users of archives, although growing estrangement has led scholars increasingly to consider the interactions between the two parties as interdisciplinary encounters. This section thus explores how technology has changed the relationship between historian, archivist, and archives. What has the impact of partial collection digitization been? Online finding aids? Digital photography? Over the last two decades, historians are spending less time than ever before in archives, yet never have we had such powerful tools and platforms at our disposal.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - The fourth section, ‘Publishing in an Interdisciplinary Age: From Journal to Social Media, explores the changing relationship between historians and their audiences. When historians think of digital history, many think of digital public history, thanks to the historical professions long lineage of public-facing engagement. Public scholarship has taken the shape of CD-ROMs, exhibit sites, memory banks, social media, and engagement on Wikipedia. Yet traditional career progression – hiring, and in North America, tenure and promotion – compel scholars towards traditional markers of career success as embodied in certain publication types (especially traditional books). How has technology changed publishing? This section briefly explores new formats, the changing approaches towards idea circulation, and the potential for interdisciplinary engagement.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Historians have rarely been transparent enough about many of the above topics: we treat scholarship as finished products, and methods are too often relegated to footnotes or informal discussions. We could all do better history by reflecting on the ongoing technological transformation that is changing how we research.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Scholars have been exploring how to explicitly use new and emerging technologies for humanistic and historical research since the 1950s and 1960s, under the auspices of what we today call the Digital Humanities or more specifically Digital History. The relationship between these two DHs is complicated. Digital History owes its lineage both to the Digital Humanities and also to currents and trends within the broader historical profession, particularly public history.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - The Digital Humanities, broadly defined, explore the intersection of technology and the humanities. In many ways, as Adam Crymble has explored at length, the Digital Humanities grows out of a digital literary studies tradition (by way of the Text Encoding Initiative and humanities computing scholars). Historians have traditionally been underrepresented within the broad scope of the Digital Humanities and its earlier intellectual approaches.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Digital History can be expansively defined as the intersection of historical scholarship with new and emerging technology. In practice, this can be broken into two subfields. First, some Digital Historians use technology to reach new and different audiences with new media, continuing a lineage of historians using new media to carry out the mission of social and public history. An understanding of the democratizing potential of technology has been foundational to this approach. Secondly, other Digital Historians have used technology to do historical scholarship. These scholars in part emerged from earlier approaches to quantitative history and historical demography – Social Science historians – as well as a smaller group of historians influenced by the more literary-focused computational studies approaches.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Today, Digital Historians engage in a wide variety of scholarship. They may write programs or leverage computational platforms to assemble sources en masse, such as from records housed at the Internet Archive or the Library of Congress. Data are analysed by extracting features (i.e., word frequency, detected items in images, place names) before being subsequently analysed and visualized in a variety of ways. Other Digital Historians challenge conventional norms of scholarship, exploring new methods of scholarly communication. While it is impossible to do justice to this field in a few sentences, it is a vibrant subfield adopting explicit new methodologies and approaches.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Yet what about other historians who use computers to do their work but who do not fall into the above categories? Historians now all use databases, run keyword searches across millions of documents, and take digital cameras to turn scholarly documents into electronic files to be analysed at home. Are we now all digital historians?
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - There is a fruitful distinction to be made between Digital History the delineated field of study bounded by academic journals, conferences, and pedagogical approaches – and digitized history, or the broader transformation prompted by technology. We are not all Digital Historians (my capitalization is deliberate). But we all engage with digitized sources and workflows. It is unwise to silo historical engagement with technology as a subfield given its sweeping impact on the entire profession. A focus on Digital History can make the rest of the historical profession think that ‘we are not digital historians, as they fire up their web browsers and research over the Internet. We are all digital now.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Let me close this introduction on an optimistic note by imagining a digitally aware historian who takes the content of this Element to heart. What kind of work would a historian who was fully cognizant of the work that technology was playing in mediating their work do?
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Historian Tim Hitchcock made a similar but broader point in 2013. He argued that historical work was being transformed without accompanying reflection:
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - They begin to research their topic in a newspaper database. But rather than haphazard keyword searches, they instead look at which newspapers and years are included in the database – and which ones are not. They then explicitly consider selection bias (why was this newspaper digitized but not that one) and do some contextual research on the periods newspapers. Perhaps at the end of the day they believe that the accessible advantages of the digitized newspapers outweigh the limitations of their selectivity bias, and accordingly write a few sentences to that effect in their introduction. This sets the tone for their engagement with sources throughout their manuscript. The historian thinks about what is present and what is missing. For an event, they go page-by-page for a few weeks before and after the event, developing a contextual sense of the source as well as to ensure that the OCR has not missed salient keywords. Scholarship is still assisted by databases, but every step is deliberate and thoughtful. When a button in the interface is clicked, it is done so deliberately. The database is no longer a black box.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - The time then comes to work with other digitized primary sources, and many of the same questions come to the historians mind: whats there? Whats not there? They respectfully email an archival colleague and ask these questions. The sources that are used are deliberately framed and contextualized. They understand their sources and are self-reflective about their use.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - The historian then goes to the archive. They anticipate a follow-up archival trip, to follow theoretical rabbit holes that will inevitably arise when looking through their digitized photographs at home after their trip. As they leave the archive, they offer to share photographs with the archive as well in case it might help. After finishing their research, they tweet about their work, think about primary sources, and are transparent in their writing about argument, method, and approach.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Maybe they will become a Digital Historian. This could entail adopting new analytical lenses, moving away from the typical approach of ‘close reading to one of distant reading. Alternatively, they could draw on metadata to visualize interconnections between sources, or challenge norms of scholarship by publishing a database, a map, or another novel form of scholarly communication. New frontiers may then present themselves. But most historians will not go down this route. Instead, what we have seen in the above hypothetical of a digitally conscious researcher is one that pays heed to the mediating influence of technology on their work. The researcher understands that the way by which sources are mediated has had profound impact on how they are read, understood, and contextualized. In short, this is a digitally-aware historian, actively using technology rather than being shaped by it.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - The 1990s witnessed the rise of primary source mass digitization projects. Building upon the intellectual foundation provided by earlier projects, such as Project Gutenberg (a volunteer book transcription effort dating back to 1971 and the ARPANET), by the 1990s there were projects such as the Library of Congress American Memory Project, American historian Ed Ayers pathbreaking web-based Valley of the Shadow civil war document compendium, and the pioneering Who Built America? Textbook and primary source reader which, for a time, was included with every Apple Computer. Yet this early wave was limited, compared to what was set to come by the first decade of the twenty-first century. Technology in the 1990s was insufficient for the task of mass digitization. Transcription projects were limited due to the effort of typing everything. Taking digital photographs of primary documents to share was hampered by comparatively low-quality and expensive digital cameras, limited storage, and bandwidth limitations (by todays standards). Sharing high-quality photographs was not possible at any real scale until widespread high-speed Internet (a factor to which we will return, as there are still access issues today in much of the world). Early projects made it clear that there was an interest in both providing material and using it, but the 1990s state of technology did not yet allow for mass digitization.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - All of that would quickly change at the dawn of the twenty-first century. Large arrays of digitized sources were created, both directly and via the scanning of microfilm, and transformed historical scholarship. We need to understand the implications of this process, including the ultimate impact on historical scholarship of these multi-layered primary source repositories. A dive into these databases, from their historical roots to the layers that comprise them today, helps historians gain an essential understanding of how their sources are mediated in the digital age.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Perhaps the best parallel to our contemporary moment is the early-twentieth century move to store documents on microfilm. Indeed, microfilm in the 1930s raised utopian hopes around universal access to all knowledge, as well as its long-term stewardship and preservation. Yet reception by historians was mixed. While many historians recognized the value of having sources conveniently microfilmed, others complained about eyestrain and research difficulties. An understanding of this earlier moment, especially given the roots of digitization in earlier microfilming projects, lays the foundation for thinking about libraries and databases today.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Microfilms are scaled-down documents on film strip. Documents are reduced to roughly 1/25th of their original size, and then viewed through microfilm viewers which both illuminate and magnify them to original (or larger) size.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Microfilm is a beautiful system for information retrieval. Indeed, of all the rooms in a modern research library, the highest volume of analogue information is found in the microfilm room. Entire print runs of government documents, newspapers, print repositories, and dissertations from around the world are available for immediate consultation. A run of the New York Times from its 1851 founding to present would fill a small warehouse. In microfilm format, it occupies a shelf or two.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Importantly, properly stored microfilm reels have a long lifespan, and they can be accessed with any combination of magnification and illumination.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Despite this power, in an age of instant search-and-retrieval, microfilm seems increasingly antiquated. Priceless cultural heritage sits on microfilm reels, inaccessible by contemporary standards. They are separate from where we expect to find information: the Internet. In 2021, after our library had been closed to on-site access at the University of Waterloo for a year owing to the global pandemic, I asked our library staff how many requests they had received to use the microfilm machines. The answer: one student, who happened to be doing her doctorate with me. Among over 40,000 students and over 1,300 faculty members, only one request for this treasure trove of information had been made. Times have certainly changed.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Microfilm originally promised democratic access to all information. While a nineteenth-century technology, it was in the early twentieth century when thanks in part to the work of technologist Robert C. Binkley – microfilm began to be seen as a scholarly solution to the problems of needing to expensively travel to access scarce source information. In the 1920 and 1930s, the American Library of Congress microfilmed millions of documents held by the British Library, and brought them back to Washington, DC. An American researcher could now do research without crossing the Atlantic. During the Second World War, microfilm was used to facilitate transatlantic correspondence, as well as to courier secrets. Microfilm hit a conceptual pinnacle in 1945, when Vannevar Bush the American inventor and then head of the United States Office of Scientific Research and Development – articulated his microfilm-driven ‘Memex machine in a famous Atlantic Monthly article.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - The Memex, conceived in part to harness the revolutionary potential of microfilm-based information, would inspire the idea of hypertext – foundational to todays World Wide Web.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - In other words, the advantages of making information accessible were clear. You can see what this looked like in Figure 1, a collection of digitized microfilm reels held by the Internet Archive that illustrates the production quality and scope. Researchers could go to their local university library or a major public library and consult millions of otherwise-inaccessible documents. Microfilm is a beautiful analogue system and led to a moment of massive excitement: the idea that mass amounts of information could be universally accessible. But what happened to the promise of microfilm? Two factors help explain why it fell a bit short of utopian projections.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - First, microfilm is difficult to use. If you have not used a microfilm machine before, imagine watching a long streaming video that you want to find a particular scene in. However, you can only press play, fast-forward, and fast-rewind. You might fast-forward too quickly, then you need to go back, and then forward, until you find the content you are looking for. Now, of course, imagine that these are thousands of individual pages that you are skimming through, and you can see how this would quickly frustrate. With more modern digital microfilm readers, while they are generally more pleasant to use, content is also often taken out of focus while one fast-forwards and rewinds, making it difficult to read headlines when the reel is in motion.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - This problem was in part what underpinned Bushs conceptual Memex. He imagined the revolutionary potential of being able to unlock the power of microfilm by quickly navigating to and displaying material of interest. The Memex envisioned the rapid retrieval and projection of microfilm within a users desk one would make an inquiry and the material would whirl and pop! - immediately appear. Yet these rapid retrieval systems for microfilm never came to fruition, now having largely been eclipsed by the digital turn.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Secondly, and perhaps as a portent of what would come later with digitization, microfilm was largely commercialized by a handful of big corporations. ‘All of the worlds information was not microfilmed in one utopian push, but rather microfilms were increasingly produced so that they could be sold to libraries an important distinction. While early microfilm pioneer Robert C. Binkley saw the democratizing potential of microfilm – facilitating cheap, widely distributed copies and lowering publishing costs salesman Eugene Power saw the commercial potential of microfilm and envisioned the market for selling entire periodical runs to libraries. Internet Archive founder Brewster Kahle notes that this transformed small libraries into holders of collections that only the largest libraries could dream of.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Commercial imperatives and historical legacies dictated choices; however, something especially important today as yesterdays microfilms often serve as the foundation of todays mass digitization projects. This is perhaps best encapsulated by Kirschenbaum and Werners warning that the digital is a frankly messy complex of extensions and extrusions of prior media and technologies. As Stephen H. Gregg has shown in a recent Element on the history of the Eighteenth-Century Collections Online, this important database (published by Gale) owes its own provenance to decisions made earlier. It was a 1970s digital book cataloguing project that led into a 1980s–2000s microfilming initiative which then evolved into todays online database.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - The same is true of periodical collections. Consider the case of Victorian periodicals. What we have today was shaped by forces including initial accessibility decisions, the impact of the London Blitz, and which were selected for microfilming in the postwar period. All of this together decides what is digitized today.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - The commercial and scholarly landscape for microfilming has naturally transformed in the digital age. While a boon for libraries, Powers company University Microfilms was later acquired by Xerox, and then Bell & Howell, to become publishing behemoth ProQuest (in turn acquired by information conglomerate Clarivate in 2021). Databases became ProQuests priority, microfilm became yesterdays technology, and the companys massive array of microfilmed cultural heritage was neglected. Microfilms transformation from revolutionary technology to mundane background object helps obscure the pivotal role it played in democratizing and facilitating access to knowledge.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Even in the twenty-first century, the ability to go into the microfilm room and scroll through thousands of pages of documents continues to be a boon for scholarship. While microfilm revolutionized access by bringing documents to the scholar rather than requiring them to travel to the archives themselves, it did not dramatically change the ways in which scholars worked with documents. Microfilms are read page-by-page. A scholar advances the reels forward rather than flipping pages, but still has eyes on all of the information as it flows by. In this respect, microfilm was a less radical transformation than the keyword-search-based databases that followed.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Microfilm illustrates that the mediation of historical sources through technology has a long history: projects became feasible thanks to microfilm, and undoubtedly sources were selected depending on whether they had been put onto reels. The historical profession constantly engages with and is shaped by new and emerging technologies. In most cases, after an initial flurry of debate, new technologies become commonplace and unremarkable. Few historians had spirited debates about the role of microfilm by the middle to the late twentieth century – we just used it. Microfilm became part of the background of scholarship, machines used in the library basement without thinking, even as the reels profoundly changed the ways in which scholars chose and researched topics. The selection bias in what was microfilmed also played an important and often unrecognized role in shaping scholarship as it would serve as the basis of many digitization projects. Just because it became commonplace does not mean that its revolutionary impact was muted.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Two major features arrived in the late 1990s and early 2000s that dramatically changed the relationship between historians and primary sources. The first was the advent of optical character recognition and searchable historical databases, much of which was created by scanning microfilm en masse. The second was the advent of personal digital photography, which transformed archival trips from lengthier in situ experiences to short gathering missions followed by the actual reading of photographed documents at home. Combined, these two factors represent dramatic change in the day-to-day research workflows of historians.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Optical character recognition, or OCR, was a critical factor for allowing the use of digitized primary sources to proliferate. OCR made large bodies of historic documents accessible in new ways, ensuring that these large collections of text would become more discoverable than non-digitized ones. One could now keyword-search across hundreds of years and thousands of pages. OCR arose in a context of needing to make sense of large bodies of documents, such as in corporate legal discovery. Enterprising individuals realized that this technology could be used in other domains such as searching decades of historical newspapers. Throughout the 1990s, pilot projects were carried out (at Yale and Cornell) which explored the scanning of microfilm.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - In 1999, however, R. J. (Bob) Huggins saw a business opportunity in scanning digitized newspapers.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Huggins, a Canadian entrepreneur, founded his company Cold North Wind to scan entire microfilm reels, run them through the OCR process, create a searchable full-text index, and make them accessible to fees-paying customers. In 1999, high-speed internet access was not widespread, a precondition for sending larger images between locations. Huggins, however, anticipated the widespread revolution in high-speed internet access in the developed world and thus came online at just the right time. Thanks to this, the Toronto Star apparently became the first fully searchable and accessible digitized newspaper in the world.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Compared to Project Gutenbergs earlier volunteer efforts, or even the digitization efforts of pioneering digital scholars and library-based digitization projects, Cold North Wind adopted a different approach. It represented a shift away from internal teams digitizing material themselves towards external vendors doing the work for them. If the roots of mass digitization had been laid in volunteer projects like Project Gutenberg or scholarly grant-funded activities, the story would have been different (engaging in hypotheticals: digitized holdings would be more accessible but rarer). Similarly, the use of OCR meant that high-resolution images were necessary, as the unreliability of automated transcription meant scholars needed to look at the original and not just a text representation. This made high-speed Internet essential for access, exacerbating an internet access divide between Global North and South as well as between well-served urban areas and the less-connected rural. Finally, as noted, the use of microfilm as the foundation of these projects is also significant. Given that certain material was microfilmed over other material, reflecting both the scholarly interests of past generations as well as commercial imperatives, these earlier biases were written into contemporary databases.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Furthermore, access to massive repositories soon facilitated scholarship but led to a patchwork of vendors (ProQuest digitizing these resources, Gale a different set of documents). Silos formed, antithetical to the broad approaches that historians tend to take with sources. It also made digitized culture especially vulnerable. While independent companies like Cold North Wind were a boon for scholarship, commercial success was elusive. Huggins was not alone in facing difficulties around sustainability and monetizing access to digitized culture: Google also shuttered its Google News Archive in 2011 after only three years.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - One exception to this has been the increasing role played by genealogy companies such as ancestry.com in digitization. Moving beyond census rolls to newspapers, in 2012 ancestry.com spun off newspapers.com, providing access to millions of digitized articles. While their core business may have originally been genealogists looking for birth and death notices, the OCR layer opened up hundreds of periodicals to researchers. Over at ancestry.com itself, a large array of material is digitized, moving beyond traditional census data to information such as ship and prison registries.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - [A]cademic historians have yet to effectively address the implications of the online and the digital for their scholarship, or to rise to the challenge that these resources present. We need to know about OCR and metadata, and we certainly need to learn how to use the tools of data-mining, GIS and corpus linguistics; and we need to be able to wield the tools of large-scale visualization, as spearheaded by the hard sciences, network theory and big data analysis of the sort implemented in the Google Ngram viewer.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - It all comes at a cost, however. To safeguard their intellectual property and investment, these growing bodies of digitized sources would be hidden behind legally enforced password-protected paywalls. What would be a minor frustration for institutionally backed scholars became a major barrier to those without institutional affiliation. External library borrowing status often did not include access to large and expensive databases, exacerbating the divide between richer and poorer universities. Microfilm had been a one-time purchase, open to all who could physically attend the library. In other words, the consumption of historical sources is now closer to a streaming model (think Netflix or Spotify) rather than a one-time purchase (like a DVD or music album). These databases brought ongoing subscription fees and additional barriers such as the necessity to have a high-speed internet connection.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Indeed, sites like ancestry.com or newspapers.com are somewhat democratic in that they allow individual researchers to subscribe. This makes them accessible in a way that the large publishing organizations and their institutional focus are not. Yet in some cases, the genealogical use case can collide with the historical one. The UK-based company Find My Past, for example, digitized the 1921 UK Census with The National Archives. They charge $4.90 USD for each image and $3.50 USD for each page transcript. This works for a user seeking a few relatives, but is unusable for any historical research of a larger scope.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - The other factor which led to a dramatic increase in digitization was the advent of cheaper digital photography. While early digitization projects – such as Valley of the Shadow or Who Built America? – digitized (and in many cases transcribed) documents for broad use, most digital photographs taken of historical sources would soon become proxies for photocopies and disappear into private research collections.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - In both respects, the ramping up of digitization which slowly began in the early 2000s would accelerate by the end of that decade as search engine behemoths contemplated what they would gain by having print material appear alongside born-digital results in search engines. Companies felt that it would enhance their search engines, help increase their user share, and have a ‘wow factor as well.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Soon there were three book digitization and search coalitions – one led by Google, another by Microsoft, and a third by Yahoo! While Google emerged victorious, with the other projects being abandoned by their corporate partners (the Microsoft-led Open Content Alliance lives on in the Internet Archives Open Library project), the forty million Google Books titles represent a vision of what mass digitization could make possible.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - There are three components of Google Books that have influenced how historians work: snippet search, Culturomics, and HathiTrust. The ability to search across forty million titles using ‘snippet search, when the book is under copyright but a few lines around a keyword result can be shown in many cases, facilitates fact-checking and snap decisions on whether a book is worth exploring in full. Additionally, works not in copyright can be downloaded in their entirety in several formats including PDF and ePUB. Secondly, the high-profile Culturomics project, launched in 2010 to fanfare, provides relative word frequency popularity over centuries of a substantial subset of the Google Books corpus and has increasingly become a staple of conference presentations and scholarship. Thirdly, HathiTrust emerged out of Google Books scans and illustrates the potential of a fully featured digital library.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Sitting in the Global North, of course, means that it can sometimes seem as if barriers to access have been uniformly lowered. Yet while Huggins had correctly predicted broadband access revolutionizing database access for North American academics, much of the world still lacks connections necessary to access these resources. Ever-growing technological requirements, from up-to-date software licenses to cutting-edge hardware, add to these barriers. The minimal computing ethos, driven by a pioneering group of digital humanists, aims to ask us to think of computing done under some set of significant constraints of hardware, software, education, network capacity, power, or other factors, with implications for increased access, less environmental impact, and ultimately for the development of more impactful digital projects. All platform designers could benefit from this approach.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - The sheer amount of digitized materials, however, obscures the reality that not everything is digitized. Most of it is not. Digitization is a resource-intensive process, both in terms of the scanning process and also in describing and making resources discoverable. Undescribed and thus inaccessible data are nearly useless. There are also considerable costs to preserving data in perpetuity. Accordingly, any user of digitized resources needs to ask: what has been digitized? What has not?
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - What is digitized? The holdings of well-funded institutions, from the Global North, are overrepresented. Even affluent institutions must make hard choices about what to process, balancing user interests and institutional priorities. Digitization thus proceeds unevenly and tends to favour richer countries and more popular collections. Conversely, specialist collections in less-resourced institutions tend not to be digitized. As archives aim to enhance the discoverability of their collections, they are integrated into search engines. Sources appear in Google results, and are cited in Wikipedia articles, with ripple effects on visibility. Critical reflection is necessary when thinking about things that are not digitized.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - A 2013 exploration of these questions made a case study of Canadian newspapers and how relatively often they appeared in dissertations on Canadian historical topics between 1997 and 2010. The goal was to measure if a given newspapers usage increased after being digitized, and whether newspapers that were undigitized were conversely mentioned less. The findings were stark. In 1998, a pre-digitization year with 67 dissertations, the Toronto Star appeared 74 times. In 2010, once the paper was online, it appeared 753 times across 69 dissertations. Controlling for sample size, this was a 991 per cent increase. Similar trends were found with the Globe and Mail, another paper that was digitized early. However, the non-digitized Montreal Gazette and Toronto Telegram decreased by 16 per cent and 72 per cent respectively.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Finally, the uneven landscape of digitization is also influenced by copyright law and policy. The United States is a global behemoth with long (and occasionally growing) copyright terms. There, 96 years must elapse between the publication or release of a film, book, or other copyrighted work before it enters the public domain. This profoundly impacts what is available in open repositories. Public-domain work can quickly proliferate across the Internet Archive and other repositories, whereas copyrighted material is secured behind controlled digital lending walls, paywalls, or is just plain accessible. Just as a digitized newspaper is more citable, material in the public domain is also more discoverable and thus more citable. We perhaps see this effect in citation patterns, where there is some (but not overwhelming) evidence that open-access papers receive more citations. In other words, historians need to understand mediating forces. This becomes more apparent as we dig into the details of digital objects themselves.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - With the overall contours of what has been digitized (and what has not) having been established, the next question is how historical documents go from being primary sources to being put into a database and made discoverable by keyword search. This question also has significant impact on historical research.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - There are several ways that primary sources are made accessible for full-text search. First, they can be manually transcribed by a human – this works well for typewritten as well as handwritten documents, although it naturally cannot scale as easily due to resource limitations. There are only so many words a person can type in a day, and it is monotonous, detail-oriented work. In some cases, this has taken the shape of volunteers transcribing items they deeply care about. With Project Gutenberg, individuals selected books that were of personal interest and transcribed them. Starting in 2000, Gutenberg added an additional layer of crowdsourced proofreaders.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Volunteers are finite and are more inclined to pursue projects of personal interest.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - More recently, a second form of volunteer labour has arisen: crowdsourcing. This is most successful in the case of corpora that might be most useful to a community if it has been made fully searchable in its entirety. For example, census records or immigration files benefit the genealogical community writ large if made accessible. Crowdsourcing has also enabled large-scale translation projects. For example, volunteers are translating over 70,000 French-language articles found within the Enlightenment Encyclopédie into English (in an interesting point of continuity, the original Encyclopédie was itself collaboratively written by over 140 different authors).
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Yet crowdsourcing brings ethical concerns such as whether it is fair to create large corpora without paying people for their time, and it skews towards volunteer interests. Finally, a project can pay for transcripts. If done property, a project can create an impeccable resource. The Old Bailey database, a repository of nearly 200,000 English court cases, used ‘double-entry rekeying to create part of its corpus. Two typists type, and if they diverge, a third individual comes in to resolve the conflict. This process creates nearly flawless transcriptions, but at high cost.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Alternatively, as we have seen in newspaper digitization, projects can turn to algorithms such as OCR. OCR, developed in the late 1970s and continuously refined, has a wide array of applications, from reading license plates to processing thousands of pages of corporate documents. Cold North Winds application of OCR to historical newspapers was only the beginning of a process which would see these algorithms applied to historical books, microfilm reels of vast arrays of documents, and beyond. Today, OCR is an active area of vibrant research throughout the digital libraries and information retrieval communities. While accuracy has improved, at the scale deployed in historical repositories, even a few infrequent character-level errors (a n being misidentified as an m for example) has dramatic impact. Simon Tanner has outlined what a seemingly exceptional 98 per cent success rate really means. This is a reasonable if optimistic figure when working with microfilm:
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - For example: [take] a page of 500 words with 2,500 characters. If the OCR engine gives a result of 98% accuracy this equals 50 characters incorrect. However, looked at in word terms this could convert to 50 words incorrect (one character per word) and thus in word accuracy terms would equal 90% accuracy. If 25 words are inaccurate (2 characters on average per word) then this gives 95% in word accuracy terms. If 10 words were inaccurate (average of 5 characters per word) then the word accuracy is 98%.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - While some platforms allow for users to correct OCR errors that they find (such as the National Library of Australias Trove platform), most repositories do not allow you to make corrections, or to even see the underlying raw text that is being searched. OCR mediates text, adding an interpretive layer. In Figure 2, we can see an example of good OCR – with high accuracy; in Figure 3, we can see an example of bad OCR, where the algorithm has been confused by noise in the scan and tight, early twentieth-century periodical columns.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - As in so many computational domains, machine learning – a form of artificial intelligence – suggests new approaches for automated text recognition. This promises the expansion of keyword search into historical domains previously imagined beyond its scope, such as the Medieval and Middle Ages. The Transkribus project shows how machine learning can help computers parse handwriting, previously largely beyond OCRs scope due to its lack of standardization. A scholar trains Transkribus to understand the writing of a particular scribe or authors handwriting. In this way Transkribus learns how, for example, one author writes the letter a versus the letter b, as opposed to how another author might handwrite those characters. After several hundred pages of teaching Transkribus how to read handwriting, a Handwritten Text Recognition model is trained and can be used on future pages. This technology is already being put to fruitful use. For example, a team has trained Transkribus to recognize Michel Foucaults handwriting and is now creating a searchable repository of his profligate handwritten corpora.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - In August 1938, American President Franklin D. Roosevelt visited Queens University in Canada and gave a speech pledging American support for Canada should the country be threatened. In 2021, I was giving a lecture at Queens and wanted to use his campus visit as an example of finding information in newspaper databases. To my surprise, a search for ‘Kingston and Roosevelt in 1938 led to no results in a search of the Toronto Star. Was Roosevelts visit not significant? Was the OCR faulty? Digging into the newspaper, hosted on ProQuest Historical Newspapers, produced the discovery that there were no results from August 1938; indeed, the month was entirely missing. Was this a result of late Depression-era austerity, a publication stop? It seemed unlikely, as in early September letters to the editor referred to articles published only a week before. This was worrisome as it was the first event that I had searched for. If a user just keyword-searches their way through this historical collection, they would not know what was missing. A user could interpret the lack of hits as a null response. They would think of it as a true negative, whereas it might be a false one.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Perhaps, as we used to do with microfilm by necessity, we should return to skimming? ProQuest makes this difficult. A user needs to load a page of the newspaper (via drop-down menus, selecting year, month, and then year). The resolution is too low to read, meaning a PDF needs to be downloaded or manipulated in the browser. Click to download. Click to zoom. Read. Click to the next page. Wait for it to load. And so forth (and then, frustratingly in ProQuest, when you need to select the next issue, you sometimes need again to select year, month, and date). ProQuest is designed for keyword search, with the interface compelling you to access data in that manner. Yet the keyword search is a black box. There is no indication of OCR quality or missing data.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - These problems are present in non-profit environments as well, mostly due to copyright. Consider HathiTrust, a repository of digitized books and primary sources. Throughout COVID-19, HathiTrust gained recognition for its Emergency Temporary Access Service (ETAS) which allowed member research libraries to allow their borrowers access to digitized books which they themselves had in their (temporarily inaccessible due to campus closures) physical holdings. This was an essential service for scholars who could now read digitally what they could not physically access. In doing so, however, they confronted the reality that skimming on HathiTrust was different than reading a print book. For copyrighted content, there was no download to PDF option. A user again faced load times, albeit far quicker than ProQuest. For slower, line-by-line readers, the interface worked, as it did for those doing keyword searches (although the text layer was again a black box in copyrighted works). Yet skimming was difficult. Researchers were compelled to keyword-search on large bodies of OCRed text, primarily due to copyright. While the interface appears to have improved by the pandemics waning days, this still underscores the pressures facing platforms that provide access to copyrighted material.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Enter the Internet Archive. Thanks to its emphasis on public domain materials, the Internet Archive has taken an open approach which empowers researchers to engage with documents and books in various ways. The interface is flexible: one can skim in their web browser, or can download documents either as a PDFs, images, or – importantly – plain OCR text (which is useful to see how messy the text layer is). It is a platform that facilitates research in all of its forms and presents a vision of what could be possible absent copyright and profit motive. It illustrates possibilities.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - ProQuest, HathiTrust, and Internet Archive cover a spectrum, underscoring the degree to which copyright and platforms shape our approach to accessing knowledge in the digital age. Yet covering several platforms is important as historians usually require information from across many platforms in a single project. They may locally download from one website, access another through a different paywalled platform, and may be drawn to keyword-search a third. Accordingly, an astute understanding of these interfaces and how they shape our research is critical.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - The impact of digitization on scholarship is hard to measure. Historians are largely opaque about their methods. This is not due to nefarious motives but rather disciplinary norms. Frustratingly, citation practices cite the source sans mediating platform. For example, historians cite the newspaper article but not how they accessed it. While archival citation is better, historians still often cite documents as if they were found in person even if they were accessed online (a bane for those who digitize documents and seek to measure impact).
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Despite this citational opacity, changing technology and digitization has affected projects and research questions. Students and faculty can carry out previously impossible projects thanks to their ability to reach quickly and inexpensively across oceans and time. At my Canadian university, where we have shorter, one-year long masters degrees, students can carry out thesis projects without physically entering archives, despite needing to base their research in primary sources. Digital photography means that archival trips are quick, surgical strikes.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Despite the unevenness of the digitized source base, however, it has had considerable implication on the kinds of questions that historians can explore. Instead of having to go deep into one or two newspapers, we can go shallow across dozens – bringing the ability to connect disparate points of information together. This has been a boon for genealogists, as they can find traces of an individual across previously disconnected sources. International connections can be quickly drawn as well. New topics of broader scope can now be pursued. For example, one can easily trace the evolution of public sentiment in a dozen newspapers (and, indeed, these are now questions routinely explored by undergraduates in term papers). Yet these newfound powers need to be better paired with interpretive frameworks. Scholars need to think deliberately about where they are searching and which results to draw on, especially as search results can be drowned out by duplicates. They may also lack the understanding of an articles contextual placement within a periodical.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Lara Putnam examined how source digitization enables transnational and global history scholarship. Previously the domain of senior researchers, who had amassed a body of knowledge after a career of scholarship, such sweeping work is now routinely done by any scholar with pertinent research questions. They can quickly use Wikipedia, HathiTrust, Google Books, or other digitized repositories further to investigate an interesting archival discovery. Imagine: a researcher can sit in a New York City archive, find something of interest that happened in Paris, and quickly Google their way to a few (certainly unrepresentative given digitization bias) sources. Putnam considers these ‘side glances as having dramatic impact. As she notes, the impact of such side-glancing – formerly rare, as each glance would have demanded hours or days of effort with no likely return; now quotidian, requiring nanoseconds to search and minutes to read – has been profound.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Ultimately, according to Putnam, while technology ‘has exploded the scope and speed of discovery ... our ability to read accurately the sources we find, and evaluate their significance, cannot magically accelerate apace.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - In our own fields we know what has been digitized and what has not been, but this level of critical engagement cannot be extended to every field that we incidentally explore on the Internet.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - The digital turn is thus transforming scholarship in three respects. The first is geography, as global projects at least those drawing on repositories in the Global North, given the costs of digitization are now possible in previously impossible ways. The second is digitization bias, the Matthew Effect of historical sources. The third is the transformation in the way in which sources are used, a shift from contextually aware skimming to surgical keyword search. An understanding of these forces can mitigate the negative effects of these changes. These important concepts cannot be left to be magically solved by the next generation of scholars, but rather need to be actively developed. Our current professional apprenticeship model fails to capture technological shifts, revisioning how we train our students.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - What sorts of knowledge must historians acquire in order to properly contextualize their use of digitized primary sources? The first is understanding and being aware of algorithmic bias. Scholars, both to be good historians and citizens, need to think critically about the role that search engines play in their work (and life). Why is one website, for example, the first hit on Google (and thus accordingly cited) whereas another is relegated to the ninth page of results (unlikely to be seen)? What has been digitized and how has it been made discoverable? What voices and perspectives are reflected in digitized materials, and which ones are absent? How was a database constructed? In short, on what information is the scholar basing their arguments on?
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - The second factor historians need to understand is source mediation and context. How a document is mediated matters as much as its content. A newspaper article read in its original form, or in a clipping file, or via microfilm, or via ProQuest keyword search are all mediated differently – and that matters. This needs to be built into our citation practices as everyday transparency. At the very least, citing the trails that we follow reveals these decisions and spurs reflection around whether the medium was influencing the way in which historical knowledge was constructed.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Similarly, the role that search engines play in shaping scholarship is significant. As Ted Underwood has noted, this is not just the shifting level of evidence that might be required to sustain a query – its rather the fact that a researcher already has a thesis when they enter the keywords with which to uncover. As he notes, researchers guesses about search terms may well project contemporary associations and occlude unfamiliar patterns of thought.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - This is compounded by the lack of context on search results that historians provide when we cite our findings – these critical dimensions are largely left uncited and not discussed. Was a source a needle in a haystack, or was it chosen to be representative – and from where was it cited? The platform layer needs to be made more visible in historical scholarship. Jo Guldi argues that scholars must adopt a critical search methodology. She notes that [c]ritical thinking about the words that supply a digital search lends strength and rigor to our research process ... Iterative approaches and multiple tools are essential for controlling for the scholars own subjectivity in encounters with the archive.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - By transparently documenting choices, research is strengthened and made (somewhat) replicable. Finding the right balance between transparency and overwhelming a reader can be difficult. Yet providing context of search results, a sense of the relative prominence of ‘hits within a database, and other relevant information helps make scholarship intelligible. Databases and interfaces may be completely different and unrecognizable in five or ten years, meaning a historian must always write with this future audience in mind.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Finally, historians need deeper and more substantial digital literacy skills. We need to dismiss the misleading cliché of the digital native, and realize that these are actual skills that need to be taught. Unlike our general approach to palaeography and language acquisition, often driven by project-specific needs, the ubiquity of search boxes and digitized documents means that such awareness needs to come early in the historical curriculum (or, arguably, in the base liberal arts curriculum of the twenty-first-century university). By recognizing that all scholarship has been transformed by these forces, we can begin to see that digitized history underpins our contemporary profession whether or not we choose to be Digital Historians.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Digitized sources will continue to dominate and shape the historical profession. As historians get a deeper understanding of this shift, they will hopefully look back on the first quarter of the twenty-first century as an aberrant period of unreflective digital practice. With more training and attention paid to the mediating influence of platforms, historians need to go down a mental checklist and consider the roles of mediation, algorithmic bias, and context both when they write, read, and evaluate scholarship.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - This requires an openness to exploring new forms of scholarship, including moving away from narrative-centric books and articles and thinking broadly about the role of argumentation in the digital age. One of the more important historiographical interventions of the last decade was the Digital History & Argument White Paper, a collectively written document by a group of twenty-four historians looking at digital scholarship, argumentation, and – most importantly – the role of the discipline. Aimed at digital scholars, many of its central points are essential reading for all historians. Consider:
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - A framework for historical argument that gives little space to methods is increasingly untenable for all historians. A gap has opened up between the assumed method of historians – consulting archives or published material to find sources and then using close reading to identify evidence for an argument – and their actual research practice.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - We need to be conscious and understand how our publications and arguments come together, not simply in terms of content and argument, but mediation. And, once understood, we need to write about it in our work.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Archival work looms large in the professional identity of a historian. Archives are where many historians work with traces of the past that have been accessioned, catalogued, and described by archivists, and subsequently shape them into historical arguments and scholarship. Surprisingly, however, historians do not tend to engage critically with archives as an institution (as opposed to our engagement with a conceptual ‘archive, which has been indeed critically discussed and centred over the last decade or two). Indeed, uncritical reflection on the archive can see it implicitly understood as an unfiltered pipeline. History, however, is not simply a reconstructive exercise, nor is it a synonym for the past.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - In this uncritical conception, the understanding is that an event happens in the past, a record is generated, is archived, and is then read and interpreted by the historian.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Reflecting on archives is particularly important as an interdisciplinary gulf between historians and archivists has emerged over the last half-century. Historians need to theorize and recognize the active role played by the archive, which now includes digitization. In this more robust conception of the role of a contemporary archive, we still begin with an event happening in the past. But we then consider the process by which the record came to be: how an archivist selects only a tiny percentage of the scant records given to them by the document creator (who in turn only passes along a fraction), describing them in particular ways, and eventually a small subset is digitized for online consumption. Choices at all stages have dramatic impact. Instead of there just being one intellectual actor (the historian), in this revised conception we see that many of the active decisions come from the archivist and document owner. During the selection process, as well as the generation of finding aids and metadata, as well as the selection of material to be digitized, the archivist is an active intellectual actor as much as the historian who follows in their footsteps.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - While the role of the archivist has been evolving, their role in shaping historical understandings of the past has not been limited to the modern period. Historian Patrick Geary, for example, has argued that what we think we know about the early Middle Ages is largely determined by what people of the early eleventh century wished themselves and their contemporaries to know about the past, meaning that the popular understanding of a ‘dark age may have more to do with record keeping practices than historical fact.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - While archival work is continually evolving, the digital age has served as an accelerant to exiting trends. The selection role of the archivist has evolved, as archivists worry about triage by IT professionals as well as the sheer explosion of digital records. Finding aid and metadata also vary in quality, meaning that the relative discoverability of items varies depending on the time and resources put into its generation.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Compounding this are changes that have taken place in how historians engage with archives. While historians have traditionally approached archives by physically visiting reading rooms, consulting documents, taking notes and photocopies, and discovering new tangents to request new archival collections to then explore, this process has also been transformed by digital technology. This traditional approach should not be overly idealized: some historians had to make do with reams of photocopies, which even at twenty-five cents (or more) a sheet could be cheaper than working in situ in an archive. Yet there was a real element of place-based research that saw historians working in archives over a period of weeks or months. Historians working on similar topics would discuss their topics with each other, recognizing familiar faces and discussing documents over coffee, building community as they worked. While one should not idealize this – such prolonged archival visits were the preserve of a fortunate cadre of historians – extended in situ research served as the ideal state of historical scholarship.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - The situation is very different today. Today, almost all historians – in a survey of historians who work in Canada, about 95 per cent of scholars use digital photography in the archives; 90 per cent significantly - research by quickly traveling to archives, taking hundreds or thousands of photographs, and then returning home to read them there. Digital photography has led to the creation of discrete collection and analysis stages. Rabbit holes are harder to follow, often requiring a trip back to the archive rather than submitting a request slip at the reading room desk.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - It is important to understand what this archival transformation means for historical research. To do so, we need to explore two questions. First, what are the implications of bifurcated collection and analysis? Secondly, what is the impact of widespread archival digitization? This requires an understanding of what has been digitized and what has not. By doing so, we can understand how these emerging bodies of archival collections can be leveraged to transform scholarship.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Archives have constantly evolved over the last century. In the 1920s, archives were understood through the ‘custodialist model, a gendered vision of archivists as the handmaidens of history. This was an idealistic view of archivists as neutral actors who transmitted documents from the past to the present for analysis. In some ways, this was mirrored the then-prevailing empiricist ethos of academic historians, where a historian was understood as being able to reconstruct the past as it essentially was through hard work and conscientiousness. Yet, just as this objectivism was always a noble dream for historical research, archivists have always had to make choices about what to collect and what not to.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - The post-custodialist archival theory turn of the 1970s explicitly recognized the activist role played by archivists in shaping the historical record. One of the leading theorists of post-custodialism, Wisconsin State Archivist F. Gerald Ham, argued that archives were now in an age of abundance and that archivists needed to thus recognize their agency. The early advent of digital records drove this in part, as did the broader growth of bureaucracies. Unfortunately, while archival theory moved into new directions, historians and archivists underwent a professional divorce. In 1975, for example, the Association of Canadian Archivists emerged from its previous professional home as the Archives Section of the Canadian Historical Association. Archivists were correctly conceptualizing their facilities as sites of active engagement and construction, whereas historians tended to see archives as static places. Even today, as Alexandra Walsham has noted, historians can understand archives ‘as neutral and unproblematic reservoirs of historical fact. This divide forms the context for the transformation of historians archival work in the digital age.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - How have archives changed for historians in the digital age when it comes to the in-person research process? It is important not to reify a mythical ‘golden age of archival research. Historians now spend less time in the archive than they had before, but the old model of in situ research had weaknesses, as it excluded those with financial limitations and caregiver responsibilities. As noted, only some (or local) scholars could spend months away at archives. For others, mass photocopying and research assistants offered a solution. And for many more, substantial amounts of archival research were largely unattainable. We will never know how much historical scholarship was never written as a result. Imagine the many historians born a few years too early to benefit from the broad democratization heralded by digital cameras and digitization, forced to scale back or abandon projects.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - None of what follows should be read as a lament for what we have lost with the advent of digital technology. Rather, I am arguing that mediation affects historical research. Even things that are good on balance can have negative features or unfortunate side effects. While focusing on the archive in this section, it would be remiss not to note that just as the digital turn has transformed our relationship with the archive, it has also affected place-based research and our understanding of material culture. We can now explore the places we study through Google Maps and can engage with local community groups through email and social media. Some of our relations with a historical topic are deepened, other weakened, but all are transformed.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - The first change has been the mass digitization of finding aids. Many finding aids are now digitized, but some are not. Even across the Global North finding aid digitization is uneven. At Library and Archives Canada, for example, some finding aids are only available by request (staff presumably photocopy, scan, and email). Similarly, the National Archives of Ireland warn that while recent accessions are part of their online catalogue, older material is not always available online and must be searched using hard copy finding aids or card indices. Compounding this, early finding aids may have been converted into PDFs. However, if they have no text layer, they are inaccessible to search engines and hard to discover outside of institutional webpages. Internationally, some archives still require in-person visits to access finding aids, such as the National Archives and Records Service of South Africa.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - This means that those who Google for sources may be unaware of what they are missing. This is compounded by the fact that as archival research has transitioned to strategic digital photo gathering missions, trip and time planning is especially important. As with the Matthew Effect of newspaper digitization, archives that have better online discoverability get more visitors and citations. These in turn drive more scholars to want to access these collections. Certainly, digitization efforts at archives are designed in part to serve researchers and increase metrics.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Then there is the most dramatic change of them all: archival digital photography. Since 2009, my archival work has largely consisted of a week rapidly taking photographs to be read at home. This is common practice: almost all archival researchers use digital cameras, from handheld iPhones to (if allowed) elaborate systems of camera tripods and remote controls. While cameras have had their place in the archives for decades – Fernand Braudel famously used a film camera to capture thousands of archival photographs a day - digital cameras and storage have made this approach accessible to all historians.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - The shift from historians being prohibited from taking photographs in the archives to almost all historians being permitted to do so occurred within the span of a decade. At Library and Archives Canada, it was only in November 2005 that the Self-Serve Digital Copying Pilot program replaced the need to order reproductions at twenty cents a page. Under that scheme, scholars could take photographs but needed to sign a formal reproduction agreement, keep a roster of their photographs, and do so only under the supervision of reading room security. The pilot program was made permanent in 2007. Some other large memory institutions made this shift even more recently. The British Library, for example, only allowed personal devices in 2015.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Past pitfalls illustrate the dangers of adopting digital publishing without sufficient attention to sustainability. In 2003, digital historian Roy Rosenzweig published an American Historical Review article entitled Scarcity or Abundance? Simultaneous to its publication, the journal hosted an online discussion forum. While it was promised to form part of the journal record, when the journal moved from the community-supported HistoryCooperative.org site to Oxford University Press, these discussions disappeared and were thus only serendipitously preserved by the Internet Archive.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - In 2019, I put numbers behind anecdote. How many historians were using digital photography and how many photographs were they taking? I surveyed 1,466 Canadian-based historians and from the 253 responses learned that 95 per cent used digital cameras in the archive (3 per cent did not by choice and 2 per cent noted that their archives did not allow cameras). These historians were taking many photos. 40 per cent took more than 2,000 photos in their last major research project. I had not properly calibrated my question: some were undoubtedly taking many more than 2,000. Indeed, if one were to take my 253 respondents and assumed the most conservative outlook (i.e., if a respondent indicated that they took more than 2,000 photos count that as 2,001), these respondents alone took at least a quarter of a million photographs.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - A dramatic transformation has taken place that has been unaccompanied by training, support, or much conscious consideration. Seventy per cent of my respondents noted that they used their personal device, which may or may not have been selected for its camera. Notably, 90 per cent of historians noted that they received no training and over half were at least open to the possibility of receiving some. My survey demonstrated an undercurrent of constant anxiety around archival photo practices, from best storage practices, to arrangement, to practical questions around ensuring high-quality snaps. Most of this training happens informally between historians in graduate programs and reading rooms. Formal training would help.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - The troubling shift here is the way in which arguably one of the most consequential decisions of a research project – what documents to select and read – happens at the beginning of a project when the historian knows the least about their subject. This has always been the case at first, but pre-digital research saw more expertise developed over time in the archive. Follow-up boxes could be requested. Historians could thus shape their project in connection with the archive. It is more difficult to do this when collection and analysis are bifurcated into discrete stages. With digital photography, this deep expertise develops at home, away from the easy ability to follow tangents in the archive. For historians, collection and analysis work best when connected. An oral historian uses follow-up interviews or correspondence to maintain their relationships with interviewees, and place-based historians make return visits to their communities. Connection with research sources is critical for historical practice.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Underscoring this, one of the survey respondents noted that their use of digital photography was haphazard. They noted that they were following in the steps of a senior doctoral student in their program who had done the same thing in the archives a few years before. To this interviewee, the process was fairly random! Well see if I took the right one when it comes time to write things up. Indeed, the rhetoric of writing results up harkens to a research model more commonly found in the social sciences or hard sciences, rather than the humanities (history bridges the social sciences and humanities, but most mainstream historical writing tends towards the latter).
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - As with other changes, these are not necessarily bad. Shorter research trips allow researchers to save money as well as spend more time with their families and on other work obligations. Traveling less during our climate emergency is also a benefit. Having the documentary record at ones fingertips for immediate recall also facilitates fact-checking. Historians can also incidentally collect information that may later prove important. Yet the landscape of historical work has changed without accompanying reflection and training, despite openness to it among historians.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Recognizing two important factors will help historians better to take advantage of digital archival photography. Much of this as usual comes down to context, framing, and explicit recognition that the tools through which we mediate our research matter. First, historians need to explicitly recognize the central role of digital photography in their research. It is now core to the historians craft. In dissertation proposals, grant proposals, and project plans, historians need to be explicit about how they will photograph documents. What is their estimate of roughly how many photos will be taken? What selection criteria will be used? What photos should be taken? Return trips to follow up on material read at home are essential to budget and need to be understood as crucial parts of the research project.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Data management of this information also raises new questions. What device will be used? How will one ensure quality assurance on photos, so that there are no disappointing moments of illegibility when they return home to explore them? What is a researchers personal data management plan to ensure the long-term sustainability and stewardship of photographs? Just as some funding agencies or institutions expect researchers to preserve their research notes, our photographs form an essential part of our historical documentation. Could these photographs be shared with the archive or other researchers? These novel forms of documentation as opposed to notes or photocopied copies – can be easily shared, presenting opportunities for new forms of peer collaboration. These questions are currently rarely explicitly addressed in the planning process, whereas they need to be dealt with alongside questions of content and historiography. Much of this could be dealt with in training, either in graduate programs or as part of professional development. Granting agencies, or more specifically the historians who peer review applications, can play an important leadership role in this respect.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Secondly, these photographs mean that historians are amassing large private research collections. Could any of this be shared for the collective benefit of historians and archivists? During the COVID pandemic and archival reading room closures, I wondered if historians would find ways to share these millions of photographs? Yet there was no groundswell of energy for this.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - The digitization landscape is uneven. Historians saw this firsthand during the COVID-19 pandemic, when reading rooms closed for long periods of time (and when they reopened, they often did so with serious capacity restrictions). Some projects could continue almost unaffected due to digitized archival resources whereas others came to a standstill. There are of course areas that have more information digitized than others, both due to size (fields with less extant material can have more coverage) and politics (the choices we make around what to digitize). Yet most archival resources remain undigitized, a state of affairs that will likely persist.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - There are critical obstacles to sharing archival photographs. The first arises from metadata. Sharing photographs without metadata or description is nearly useless. Research requires an understanding of a documents context. Description, taxonomy, and full citation information are all important and require in some cases specialized archival support and training to make them publicly useful. Secondly, the original order of documents as arranged in archives matters. Researchers taking photographs can be selective at times, ignoring material that is obviously not of use – yet in doing so the integrity of the collection for others is compromised. Thirdly, archives need to demonstrate engagement with collections: random photographs online could divert traffic from reading rooms and archival websites, imperilling their ability to deliver on their mission. Finally, donors may be uncomfortable with the digital delivery of their collections. The original vision of a scholar consulting documents in a reading room is very different than the decontextualized posting of documents on the web. On the archival side, workflows are seldom set up to receive this material and there is often a reluctance to entrust a core aspect of their profession (making information available) to relatively untrained outsiders such as historians. This is not undue gatekeeping: standardized taxonomies and clear metadata are foundational to good digitization programs.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Yet these worthy objections stand against the clear benefits of sharing. We have the potential of saving a great deal of time. Do we really want researchers taking the same photos as countless other researchers? Working in this way could open inaccessible collections. Secondly, Library and Archives Canada is also pioneering a new model with their DigiLab program. Researchers can sign an agreement and receive access to a dedicated digitization workstation. In return they fill out a spreadsheet to generate workable metadata to the copies that they take. Both researcher and Library and Archives Canada benefit from this relationship.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Tropy, a software project from the Centre for History and New Media, helps facilitate both the research use of archival photos and their sharing. Designed to organize photographs into a comprehensive and searchable database, Tropy provides customizable metadata templates. Historians fill out essential citation information, such as fond, box, or file, as well as other relevant fields, to help organize their information. All of this can be exported. In theory, metadata and photographs could then be exported en masse to an archive, helping bridge the gap between researcher and institution. However, this vision has not come to fruition. Yet as a personal research tool, Tropy is invaluable. With Tropy, however, users are forced to think about metadata and its importance when developing their personal research collections – and make their lives easier as they cite, take notes, and later recall the photographs that they assembled.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Another laudable model is the community archival portal. Archivists and historians who work on Indigenous histories grapple with the colonial nature of most archives. Canadian historian Thomas Peace has reflected on this challenge. He has highlighted the number of archives necessary to piece together the life of Louis Vincent Sawatanen, a Wendat school teacher who graduated from Dartmouth College in 1781. The ‘promise of the digital archive, for Peace, lies in the creation of new archival relationships in order to recover historical interconnections by bringing together material related to people, places, communities, or cultures not envisioned by any single archives organizational structure. Peaces observations build on projects such as the Native Northeast Portal, which draws together archival collections related to Indigenous nations whose homelands form what is known today as the northeastern United States. The portal allows scholars to explore ‘related documents that have been separated either as an inherent function of the purpose they sought to serve ... or by collecting practices that cared little for maintaining a collections integrity.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Crucially, this portal (and others such as the Great Lakes Research Alliance for Aboriginal Arts and Culture) collaboratively works with Indigenous communities themselves, who review their digital heritage to ensure the ethical stewardship of cultural objects. For scholars contesting the original archival order of the colonial archive, bringing these digital documents together in new arrangements preserves documentary context (users can quickly refer and return to the original archive) while also working with Indigenous communities to recentre their voices and narratives. Working with communities to re-order and re-combine archives into community-driven collections also serves to decentre traditional scholarly and historical authority. As the colonial archive is decentred, historians need to develop new approaches to understanding their own authority and role in constructing knowledge.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - A final consideration: what does it mean that historians are spending less time in the archives and more time at home, working with historical documents on screens? Could one be a French historian if one only visits the country for a week, frantically taking photographs in a Parisian archive before a return flight? Could someone be a historian of the American Civil War if they have never been to the United States? One might well be uneasy with this. There are limitations in having historians with little contemporary understanding of the places they study, especially as historians increasingly recognize the importance of community-engaged history. We run the risk of exacerbating fraught relationships between historians and the people and groups that they study. In Indigenous history, for example, historians are increasingly expected to spend time in contemporary Indigenous communities. Ongoing relationships and taking the time to be present in these communities is essential, even if the scholar is studying events from hundreds of years ago.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Grappling with this ambiguity requires recalibration of implicit professional norms and a discussion around what is gained through place-based knowledge. Given there is benefit to be gained both from the knowledge of another place and culture, as well as being in proximity to similar scholars in reading rooms, we should make sure to explicitly note the importance of tacit knowledge. This may mean residential fellowships (even if shorter, as the three-to-six-month model hurts those with caregiver responsibilities), or when it comes to grant adjudication, an understanding that quicker is not always better; that more time in an archive brings intangible benefits beyond photographs taken or documents consulted. By making research processes explicit, we raise the prospect of opening new frontiers of historical research.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - The main reason for this is the expensive nature of digitization. Digitization might seem as simple as just scanning material. Yet this is only a fraction of the process. Costs largely stem from the same reasons that a collection of random digital photographs are not useful: material needs to be described, preserved, and made accessible in long-term storage. Some collections are more amenable to digitization than others, such as when a donor is hesitant to having their material placed online. Compounding this, archives are often under-resourced. In an environment of tight budgets, adding resources too digitization typically requires cutbacks elsewhere. As a result, not everything can be digitized.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - What tends to be digitized will be material that reflects user interests and institutional priorities. For example, in 2020, as the death of George Floyd spurred protests and institutional reckonings around the world, many archives committed themselves to in part rectifying past bias and imbalance by focusing on the digitization of Black voices. These decisions also shape the historical record, and indeed recognize the importance of digitization in shaping the conversation (otherwise there would not be done). Indeed, the Native Northeast Portal offers one model forward, as it has deeply considered the ethical and professional implications behind digitization. There is considerable work to be done, however, to rectify past imbalances: a factor which always needs to be top of mind when using digitized resources. Such portals are examples of a broader approach by Digital Historians, Digital Humanists, and librarians to create digital portals or collections. Sometimes erroneously understood as digital archives, these web-based projects often aggregate resources from many different places into one interface.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Complementing traditional digitization, organizations are beginning to digitize and make accessible miles of microfilm reels, opening new opportunities for research. As early as the first decade of this century, many graduate students did not have to travel to archives. This was not owing to digital photography or digitization but rather their good fortune that many archival institutions had microfilmed many popular record groups. Scholars could place inter-library loan requests and read the microfilms from their home libraries. This was the fruit of efforts throughout the 1980s and 1990s to microfilm archival collections and brittle books for preservation and access. However, many of these reels have now been scanned en masse and placed online. The reels now have a second life, albeit just as with periodicals historians must reflect on the impact of digitization bias. As noted, a document may have been microfilmed because of its popularity in the 1980s, leading to its digitization today.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - This digitized microfilm is in many ways easier to access than the actual microfilm. The first reason for this is that digital interfaces are easier to use than the actual microfilm machines. Unlike vendor platforms, many archives facilitate downloading of these digitized microfilms and are easier to use. Not needing to lock material down due to copyright reasons incidentally often makes for an easier user experience. Accordingly, OCR and search engines can be enabled. While one needs to keep in mind the lower success rate of using OCR on microfilm as opposed to print documents (streaks and other errors interfere with the algorithm), this is a real boon for scholars as they can easily skim and search. Notably, it can enable transformative digital scholarship: running programs to extract images, work with the text en masse, and so forth. Yet while the vast array of digitized content has in general been a net positive in aggregate, it has unfortunately unfolded in private silos.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Responding to the high cost of digitization, huge swaths of cultural heritage have been effectively privatized. Any professor at a research university will see this firsthand. A free trial will be announced by a university library, or a salesperson will email a faculty member, offering them access to a cutting-edge array of primary documents and vendor-specific platforms through which to analyse them. Yet nothing is free. Costs for these platforms are beyond the reach of individuals, and the goal in many cases is to pressure an institutional library to subscribe to them. Indeed, the faculty member would rarely even know the cost of these platforms. Rather than one-time purchases, these are usually ongoing costs. For example, the Wiley Digital Archives platform has invaluable records such as the British Association for the Advancement of Science, the New York Academy of Science, and the Royal College of physicians (1200–1970). Some of these services are very sophisticated. Gale, another vendor, offers their Digital Scholar Lab to carry out transformative digital scholarship – yet only on material to which a library subscribes.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - This has led to a siloing of digital archives, problematic given the needs of scholars. Figure 4 demonstrates this in terms of what a historian wants. Yet, with vendor and platform silos, what historians confront is seen in Figure 5.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Each interface is different, leading to an upfront learning curve. And, of course, this is just digitized sources, as there are of course in-person sources to also explore. We have a landscape of scattered silos. This mitigates against both conventional and digitally enabled scholarship, where vendor agreements end up shaping the resources used. Research questions are shaped by commercial interests, as opposed to fundamental questions. Fortunately, there is one inspiring model to look to: the Internet Archive. While I focus on the Internet Archive given its scale and broad utility, we can understand community-run tools and portals (such as the Native Northeast Portal) as offering similar potential.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - The Internet Archive has been creating the largest collection of openly accessible digitized material: books, documents, microfilms, and beyond.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Since its 1996 founding as the Internets memory bank, it has dramatically expanded in scope and size. In 2004, the Internet Archive began to digitize books - working with libraries to add scanned public domain works to their website - and shortly thereafter in 2005 was part of the founding effort behind the relatively short-lived Open Content Alliance (alongside Yahoo! and several research libraries; it was a public domain and open-access alternative to Google Books). Indeed, today many cultural heritage organizations use the Internet Archive to either freely host their collections, or contract them to physically scan and host their paper materials. In sum, the Internet Archive illustrates what almost-unfettered access to digital culture could look like.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - The big-thinking nature of the Internet Archive is worth underscoring. In 2011, for example, Internet Archive founder Brewster Kahle announced his organizations bold vision to digitize all of Icelands heritage and put it online (while evidently this did not get full Icelandic buy-in, a similar project collaborating with the Indonesian province of Bali – via their Bali Cultural Agency - soon followed with more success). Today these efforts proceed thanks to a network of regional digitization centres which digitize material at low cost. Importantly, they look beyond the holdings of affluent research libraries in the Global North (one project is exploring how to build web archiving capacity around the world). With an eye towards copyright, the Internet Archive maintains their Physical Archive in California – owning physical copies allows them to legally loan them out. The statistics and holdings are impressive: 28 million texts, over six million videos, and 16 million recordings.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Critically, however, the Internet Archive also enables transformative digital scholarship. Documents can be browsed on the screen like a book and can be downloaded as PDFs to work with locally. Raw text can also be accessed directly (and in so doing can allow scholars to gauge the underlying quality of the OCR), and in some cases can even be downloaded as ePUBs. This leads to a flexible workflow: a user can download all the PDFs or plain text in a collection to work with it in the manner that they want. A scholar wanting to read just a few documents might do so on their web browser. In my case, I carried out a recent research project by using the Internet Archives programming library to download tens of thousands of PDF, text files, PowerPoint files, and images; I could then search it using my operating systems search engine, and stitch them all together to explore much more conveniently than would otherwise have been possible.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - The ability to bring all this information together, in a variety of formats, forms an important counterbalance to the uneven digital landscape. It is a portrait of what might be possible absent the silos and barriers that we have thrown up between primary source collections and serves as a worthwhile counterbalance to the uneven digital landscape.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Given the excitement around the digitization of primary sources, it is important to underscore that most documents will remain undigitized by default. Painstaking archival research will likely always be necessary to some degree due to the varying nature of the historians craft. Decades from now, one suspects that historians will still be physically traveling to repositories to consult documents. Yet scholars know the limitations of the historical record in our area of expertise, making the great undigitized more worrisome when we side glance (per Putnam) to other geographic or temporal domains with which we are less familiar. A scholar taking a peripheral glance at a relatively unfamiliar period or country may not realize the sheer expanse of what has not been digitized in that domain. Their lack of deep knowledge of a field prevents them from being self-reflective about archival silences. This means that whenever we carry out a database search for primary sources we must always ask: what is and is not here?
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - The way in which historians engage with archives has dramatically transformed over the last two decades. We consult digitized documents from around the world (unevenly due to digitization bias), and we go to archives and take thousands of photos to study at home. Significantly, we allow research to be mediated and shaped by platforms that impact how we answer and explain historical questions. Technology shapes the way we approach our research process, leading to bifurcated collection and analysis stages. This will accelerate over the coming years, as handwritten text recognition begins to make non-typewritten documents discoverable at scale. This acceleration will proceed unevenly, as contemporary issues and policies shape the record. To rise to these new challenges, we need explicit theorization and training. We will also need new publishing models to share our increasingly digital findings.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Historians, especially but not only those working in the public history subfield, have a long track record of using new technology to disseminate historical knowledge. Historical work is published not only in books and articles, the traditional career publishing milestones, but also databases, online exhibits, new media projects, blogs, and other platforms such as Wikipedia.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Yet such engagement has been unevenly adopted. Part of this is due to the North American system of tenure and promotion, which takes books and selective journal articles to be the hallmarks of scholarly productivity. This is not due to administrative fiat but rather the need to garner enthusiastic endorsement from external letter writers who may affirm the disciplinary norms of a book for tenure and promotion. There are related concerns with the United Kingdoms Research Excellence Framework. Historians can profess a helplessness in the face of this focus on books and peer-reviewed articles, yet as a self-regulating profession we are our own worst enemies.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Historical publishing is being transformed by digital technology. We can see examples of this in several new and emerging venues. First, some historians are sharing their findings and results through open notebooks and blogs. While individual scholarly blogging has declined, group blogs such as Europeanas engaging blog on European history topics, the generalist Canadian history blog ActiveHistory.ca, or the nursing history-focused Nursing Clio, form an important outlet for established and emerging scholars alike.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Second, in part owing to the decline of formal blogging, many historians are turning to social media platforms such as Twitter or the digital newsletter platform Substack. These range from historians with mega-followings such as Heather Cox Richardsons Letters from an American (the largest Substack newsletter as of 2021) or Princeton historian Kevin Kruses half-million Twitter followers which have propelled him to national public attention, to the more muted followings of hundreds or thousands for others. Yet it forms an increasingly important venue for historical research and discussion. While not all scholarship needs to speak to the public, these new venues help build connections between historians and a broader audience.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Third, responding in part to the pressures of granting agencies, institutional and subject-based repositories – from local institutional repositories to discipline-specific ones such as Humanities Commons – help share pre-prints and other research findings.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - These approaches are increasingly a way to make scholarship more accessible by avoiding publisher firewalls. For scholars without institutional access including those in the global south where subscriptions can be financially out of reach - this is invaluable. Digitization in general has contributed to the time savings discussed earlier in this Element: the days of having to visit the microfilm room to read an old dissertation have passed, giving way to full-text search and discoverability through libraries and other aggregators. This in turn has given way to debates around how long dissertations should be embargoed. All of this means that projects can be shared beyond close peers and conferences as they would have been two decades ago, but increasingly with the public if the author wishes (or, in some cases, does not but has it publicized regardless).
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - When it comes to traditional outputs such as journal articles or books, traditional publishing has also been transformed. This can involve new developments such as online publishing or open review. It can also be as straightforward as new marketing approaches. While all publishers are different, a recent article published by the present writer led a publication assistant to ask him about social media hashtags, Twitter accounts to target, the option of selective open-access windows paired with a blog post, all with the aim of increasing visibility and thus citations. Just as newspapers are cited more frequently if they are digitized, an article might be cited more often if the editorial assistant or author has better social media skills, if the author has dedicated open-access funds, or if a marketing department as opposed to the academic editor believes that a blog post or open-access window could bolster the articles visibility.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Even something as seemingly traditional as a book has been transformed. The fact that you are reading this as part of a Cambridge Elements series bears this out. Books are increasingly found in varying lengths and formats, with accompanying datasets hosted on websites and in repositories. Some are even digital-first experiments, presenting arguments in new ways but maintaining the sustained lens and focus of a monograph. As should be familiar by now, these changes are neither inherently good nor bad – but they are transformations needing exploration. In this section, we will explore the changing landscape of historical publishing in the digital age.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Publishing, broadly defined, lies at the heart of what it means to be a professional historian. This interpretive work defines the discipline. Historians publish for many reasons, none mutually exclusive: a deep commitment to the field, engagement with communities, professional advancement, or (rather rarely inside the academy) monetary gain. Publishing allows a scholar to share knowledge, recover lost stories, reach an audience, and to strengthen their scholarship through critical engagement and conversation. In many cases this means that historians validate and strengthen their scholarship through a process of peer review. These are reasons why historians are drawn to publishers, including academic or trade presses. For the last decade, barriers to online and self-publication have been low enough that almost anybody could just put their own work on the web or self-publish. Yet, most historians realize they are not suited to the task of self-publishing. Self-publishing lacks external quality validation and peer review, necessary for the process of improving ones work. Many authors also look for the professional recognition that comes with a publishers imprimatur.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Secondarily, historians publish because they are members of a self-governing profession that prizes certain types of publications. In other words, we publish not only in the service of history but also in the service of History. Despite advocacy around the expanding scope of what it means to be a successful historian, career milestones remain somewhat narrow and conservative. Publishing operates in a prestige economy, where prestige is associated at least in part with the venue or press itself as opposed to the intrinsic value of the work. In some evaluation contexts, where the work cannot all be read by evaluators, the perceived selectivity of a publishing venue provides a proxy for the value of the work. While prestige and quality may be linked at the macro level - there is some truth to more prestigious venues tending towards more rigorous peer review – this link does not always hold true for individual books. In any event, the prestige of a press is not intrinsic, but rather ascribed by professional peers and networks (indeed, the relative prestige of a press varies dramatically between fields and subfields). Finally, publishers have their own incentives, notably the need for marketability (will a book sell well and recoup costs, or even rarer make money).
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - The complexities of publishing emerge at the nexus of author, profession, and press. Understanding the transformation of historical research in the digital age then requires attention to all three of these factors. An author moving on their own without the validation of professional or press recognition risks muting the impact of their work, but presses and professions rely on individual authors to effect change. All these factors are changing in this new age of digitized history.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Digital dissemination enables the diverse spread of ideas, using mediums such as blogs, social media, online visualizations and databases, online exhibits, and even innovative digital-first presses which lower bars to innovative scholarship. This has dovetailed with increasing attention being paid to the impact of publicly funded scholarship, such as the United Kingdoms emphasis on ‘public impact in the Research Excellence Framework. Yet this can often exist uneasily alongside disciplinary pressures to focus on traditional books and journal articles.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Scholarly blogging declined throughout the 2010s. Institutionally funded platforms such as The Conversation – launched in Australia in 2011 but since having spread to international editions for countries and regions including Africa, Canada, Indonesia, the United Kingdom, and the United States – also emerged as a venue for reactive short-form pieces. Yet institutional support meant that The Conversation could leverage both professional editors as well as connections to local and national media platforms. Yet, even in an overall context of blogging decline, the group blog model shows that it could be more sustainable in the face of these broader shifts.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - In Canada, historians have seen the transformative impact of this firsthand thanks to the Social Sciences and Humanities Research Council of Canada (SSHRC) funding agency. While other international funding agencies may have calls for knowledge translation or public impact (such as the National Endowment for the Humanities Digital Humanities programs, which often stress public engagement), SSHRC now embeds this in all of their funding calls. Historians may still want to publish a monograph as research outputs, but SSHRCs emphasis on diverse and accessible ‘knowledge mobilization compels scholars to explore new methods of reaching audiences. These are usually digital. Applications are submitted with diverse ‘KM plans that often include blog posts, a podcast, or a website to provide intermediate access to research findings in advance of the monograph. While follow-through is somewhat limited owing to the lack of professional recognition for some of these outputs, it demonstrates the degree to which diverse outputs are increasingly at the core of funders visions for our (and other) discipline. There is increasing recognition that scholarly outputs should be accessible. Thanks to digital technology, historians are thus actively engaging publics through the web.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - In 2009, a group of doctoral students in a graduate program at York University founded the website ActiveHistory.ca. The idea was that so much of historians work, informed by great scholarship and research, was inaccessible: overly lengthy, hidden behind paywalls, and written inaccessibly. What if a website could help overcome this?
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Our goal was for ActiveHistory.ca to be akin to an open-access online journal which could be a hub for diverse forms of knowledge mobilization: scholars would submit short, accessible, 1,000–3,000-word summaries of their work which would be peer-reviewed and published. Uptake, however, was limited due to the process to which authors were subjected. Going through the peer-review process for a prestigious journal might be worth it, but less so for a website run by graduate students. Onerous revision requests and long turnaround times frustrated contributors, who responded by either not submitting work or by withdrawing work from consideration after the inevitable Reviewer #2 made a pointed request for revision. The editors were arguably asking too much of their authors, lacking both an established audience and more importantly a pedigree to offer in return.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - ActiveHistory.ca hit its stride when it turned to scholarly blogging a year later. By 2010, blogging was in its academic heyday. While setting up blogs might seem too daunting for individual scholars, especially those with no technical background, a group blog would help people publish without much investment or experience. They could write their post in a Word document, send it to editors, and it could be edited and published. This worked well: it was a small request (send a few hundred words on your topic) and by abolishing the peer review stage and replacing it with light editorial review, there was a lower barrier to acceptance. Authors could write short and responsive hot takes which provided commentary on contemporary issues. They could also use these posts to help publicize other formal, peer-reviewed publications. In an age before Twitters adoption, this paved the way for ActiveHistory.ca to become a premiere blog for the Canadian historical profession (it was recognized with the Canadian Historical Associations 2016 Public History Prize). The citation noted its impact:
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Activehistory.ca has established itself as a hub of conversation among emerging scholars, senior historians, students, teachers, the media, and other practitioners of public history on a wide range of historical topics. Since 2008, this innovative website has brought historical context and critical commentary to a broad range of political and social issues, and in 2015, it launched many new initiatives, including a digital exhibition page. With 13,000 unique page views per month, Activehistory.ca is committed to making history public and accessible, while setting a high bar for the quality of scholarship it delivers.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - ActiveHistory.ca had transformed from traditional publishing to scholarly blogging, serving as a way to understand the ongoing process of digital transformation. Blogging drew on the capabilities of the digital. Rather than trying to make a website conform to the scholarly process (formal submissions, peer review process, author revisions) ActiveHistory.ca embraced the conventions of blogging. This was coincidentally part of the heyday of academic blogging.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - As with all transformations, this blogging shift brought good and bad. In the case of ActiveHistory.ca, there was a lot of positive news: the blog garnered large audiences, could be responsive to emerging events, and was open to a wide variety of authors from senior professors to early career researchers to independent scholars. As a web-based medium, it lent itself well to images and embedded digital objects. The group blog format also allowed for editorial improvements and provided a veneer of legitimacy: that is, publication here was not just posting on a personal blog but rather occurred under the curated ActiveHistory.ca imprimatur. This gave authors access to existing social media channels and avoided the stigma that self-publishing may have brought.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - There were also disadvantages: the light editing and lack of expert peer review did mean that authors could write things that would not have passed (for good reason) through a peer review process. Similarly, the lack of peer review also meant that few contributors received official credit. This was unpaid work without formal recognition by the profession, a factor affecting contributors and editors alike. Finally, open comment sections meant that while interesting conversations were fostered, authors especially early career researchers were occasionally subject to robust and occasionally harsh discussions. With the rise of Twitter, these conversations moved off moderated comment threads to social media, where they would exist independent of the platform (and thus moderation), in both the formal and informal sense. Yet on reflection, scholarly blogging helped to create robust scholarly commons.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Disadvantages notwithstanding, a transformation is underway. The advantages of this form of scholarship include making historical argument more accessible and understandable by releasing data, thus building a foundation for future scholars, allowing diverse interpretations, and using the technology as intended. Readers can dig into maps themselves, for example, and visualize data as best suits them. Yet there are disadvantages: arguments can be lost; sustainability concerns appear; the lack of recognition; and dataset citation.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Some successful group blogs have even, in light of these pressures or as a result of growing prestige, transformed into websites that increasingly resemble peer-reviewed magazines or journals. Nursing Clio, for example, refers to itself as a blog project but in substance and style is an accessible scholarly periodical. They stress their peer review processes. Similarly, the Black Perspectives blog has a roster of over fifty regular contributors, and high-quality editing processes. Its homepage looks more like the Atlantic Monthlys homepage than a blog. In some ways, projects like Nursing Clio and Black Perspectives represent what ActiveHistory.ca originally sought to be. Their path demonstrates the importance of building from a solid ‘bloggy foundation and evolving from there. Yet while these vibrant blogs show the continued relevance of the medium, other historians have turned to publishing via micro-blogs such as Twitter.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - It is hard to generalize about Twitter as it is an ever-changing platform. Indeed, as of writing, Elon Musk had just proposed purchasing the company, which prompted a fraught discussion around the platforms future. Even at the level of the user, Twitter is very different by virtue of with whom a user engages. Academic Twitter is a different place than Young Adult Fiction Twitter or Left Politics Twitter or, to say the least, something like ‘COVID Conspiracy Twitter. Yet there is no avoiding the reality that as blogs have declined, social media platforms (like Twitter) have grown in popularity. From historians engaged in historiographical conversations, exchanging ideas about primary sources (such as how to decipher handwriting), it effectively serves as a community hub for the minority of historians actively engaged there. It affords several benefits: low barriers to entry, audiences that can be amplified through retweets or critical engagement, networking opportunities (from forming conference panels to exchanging scholarly sources and publicizing scholarship), commentary on ongoing issues, and threading tweets together to form longform threads more akin to blog posts. Many of these were functions previously occupied by H-Net discussion boards, which have (with notable exceptions) also been somewhat eclipsed first by blogging and now by social media.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - The immediacy of Twitter, however, makes for a risky user experience. ‘Hot takes that play well to a small group of historians chatting about an issue can escape that community to be discussed out of context by others. Some historians lurk, rarely participating but watching conversations with interest and, again, can miss context that comes with being an active participant. Vitriol can be especially directed at women and minorities, combining the worst of internet culture with academic insensitivity. If Twitter is a water cooler conversation, it is a water cooler on a stage where the audience is hidden by the stage lights. Yet Twitter shapes research: historical scholarship takes place there and is shaped by it.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - There are advantages to Twitter engagement and knowledge dissemination: a large audience, the ability to embed media, use of hashtags such as #twitterstorians or #CdnHistory to see related conversations, the immediacy of gatekeeper-less communication, and the ability to forge networks not limited by geography, institution, or field. Historians who are the sole members of their subfield in a department can use Twitter as a venue for regular professional conversations. Live tweeting, while occasionally controversial, opens events beyond their small immediate audiences. Yet there are also disadvantages: the outrage economy of Twitter, a sense of omnipresent surveillance, and the prospect of online harassment. Indeed, the context of ‘this is a junior colleague or student, maybe they should be treated gently can be lost in a sea of decontextualized text and avatars. While Twitter plays a complicated role in the historical profession, it is nonetheless a site of considerable importance to the contemporary profession, and worth considering as a historian in the digital age.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - As part of researching a project, historians generate what is broadly defined as research data. In Canada, SSHRC defines such data as quantitative social, political and economic data sets; qualitative information in digital format; experimental research data; still and moving image and sound databases; and other digital objects used for analytical purposes, which – generously interpreted would include a wide variety of data created through historical research. Such data come in many forms: oral history transcripts and recordings, spreadsheets of historical information (such as map coordinates or rosters of people), Digital Humanities-style visualizations, and geospatial data created through Geographical Information System (GIS) software. Perhaps it would even include photographs taken at archives, although this is a bit more complicated due to archival policies, copyright, and the many factors discussed in the previous chapter.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - What to do with these data? Traditionally, historians would keep this information private while preparing their book, dissertation, or article. They would then keep it mostly private afterwards. This was partly due to fears around being scooped, still a pressing problem today, as well as seeing it as the fruits of their labour. There was also no easy way to share data. In other words, research data in raw form was essentially unpublishable. Even if, for example, oral histories would eventually be deposited at an archive, this would happen only at the end of the project – and in many cases, even interviews would be destroyed after the projects end. The digital age dramatically changed all of this.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - These forms of intermediate outputs are now more common, both due to historians increasingly wanting to engage communities as well as firmer direction from granting agencies. SSHRC, for example, mandates that all research data be preserved and made sharable:
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - All research data collected with the use of SSHRC funds must be preserved and made available for use by others within a reasonable period of time.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - SSHRC considers ‘a reasonable period to be within two years of the completion of the research project for which the data was collected.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - While compliance is not yet systematically tracked or enforced – the policy process in Canada is still under active development – the increasing professional recognition of these deliverables will hopefully shift the conversation. That is not to say that all data must be shared – there is a default colonialist worldview around openness that has been rightfully contested – but that such decisions to share or not share are taken deliberately.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Much of this comes down to the perception of whether something is making a scholarly contribution. Do arranged data make an argument and thus a valuable intervention in scholarly debate? The collaboratively written 2017 Digital History and Argument White Paper, published by George Mason Universitys Centre for History and New Media, explored this, arguing that online databases and visualizations make scholarly arguments and thus need to be recognized as valuable contributions. The examples in the White Paper were wide-ranging, making compelling cases for recognizing the substantive contributions made by outputs such as curated exhibits, datasets, maps, and 3D models. The labour in selection, description, and framing all combine to make these outputs understandable and argumentative scholarly contributions, even if it is not as readily apparent as a thesis-driven book or journal article. Arguably, the decisions we make in constructing a map make it no less an argument than a journal article (just as decisions as part of the archival creation process underscore the crucial role of archivists). Importantly, the White Paper put the onus on author and evaluator alike:
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - On the one hand, it aims to demonstrate to the wider historical discipline how digital history is already making arguments in different forms than analog scholarship. On the other hand, it aims to help digital historians weave the scholarship they produce into historiographical conversations in the discipline. The responsibility for integrating digital history with argumentation thus rests both with the digital historians who make implicit or explicit historical arguments and with the rest of the profession who must learn to recognize them.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - In other words, the wide range of diverse outputs – datasets, models, maps – were making positive contributions. Common understanding of the labour and thought that goes into datasets and visualizations would help recognize the value of these contributions. Given the importance of professional recognition and framing for historians publishing behaviours, this conceptualization is more effective than mandate by fiat. Publishing these data can also help reach new audiences.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - An exemplar of a project publishing research data is Cameron Blevins Paper Trails: The US Post and the Making of the American West. It is an example of how research data can be shared when paired with traditional scholarly outputs. Released in 2021 by Oxford University Press, the book was presaged by a variety of publicly accessible digital interventions and accompanied by a digital companion site. Blevins explores the expansion of the American postal network, arguing that the sheer scale of the postal system – almost 60,000 post offices and 400,000 miles of mail routes – requires digital methods to understand. By mapping the post on a year-by-year basis, Blevins reveals both the networks geography and also ‘how its machinery worked and the way that it shaped the occupation and incorporation of western territory. Through data and maps, Blevins illustrates the portrait of what he calls a gossamer network, a gauzy web, rapidly spinning out new threads to distant locations.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Importantly, Blevins released the dataset that underpinned the project. Discussed in passing in a Note on Methods at the beginning of Paper Trails, his companion dataset is fully downloadable. It is comprehensive, described data - 166,140 post offices between 1639 and 2000 – and the code used to generate it is also available. This data also powers a companion website, ‘Gossamer Network, which allows readers to explore interactive versions of maps that support the evidence in the book, or are enhanced versions of figures in the book. Blevins is not alone in sharing data. Historians Kellen Funk and Lincoln Mullen, for example, provided code, underlying programming language packages, and datasets in support of their 2018 American Historical Review article The Spine of American Law: Digital Text Analysis and U.S. Legal Practice. Unfortunately, these data-rich historiographical contributions are relatively rare. They are exceptions rather than the norm.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - This limited uptake reflects some of the disadvantages to this approach. It is not sufficiently recognized in contemporary professional frameworks. Blevins and his collaborators have clearly done considerable work on these websites, which may or may not be captured by the hiring, tenure, and promotion process (fortunately, Blevins blockbuster book is recognizable by peers as a marquee contribution). Resistance remains within the professional network to understand anything other than a book or peer-reviewed article as a gold standard of research output. Evidence on this is scant, as tenure decisions happen behind closed doors: a departmental chairs unwillingness to engage external reviewers versed in methods and data, for example, as opposed to formal subfields would vary dramatically by institution. However, published tenure-and-promotion guidelines in humanities departments across North American research-intensive universities tend to point towards the necessity of a book for tenure.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - While the American Historical Association has guidelines for evaluating digital scholarship, they are perhaps a bit short on substance. Yet their existence, coupled with conversations at the AHA annual meetings and in professional publications, give hope for future change. Similarly, the growing acceptance of ‘digital dissertations – unique forms of scholarship that have contested the traditional paper (and now PDF) tome – suggests there is growing openness to change. The profession is currently at the stage of exemplar projects, such as Jeri E. Wieringas 2019 dissertation.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - A related concern is the hesitation to cite datasets. This is both part of the previously discussed tendency to ignore source mediation, as well as an implicit tendency to see data provision as service and not properly constituted research. Finally, sustainability is also a concern. Blevins data are hosted on GitHub, whereas Kellen and Mullens are hosted on Oxford University Presss website. Both are relatively safe, long-term choices for storage: but what happens in twenty years if the Press reorganizes its website? Datasets will likely be preserved in multiple places, including by the Internet Archive in its periodic internet crawls. Interactive visualizations have shorter lifespans as underlying software packages sunset and end, and it is more difficult to preserve dynamic web content.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - A journal reader might be forgiven for thinking that little has changed in the publishing world. Journal articles continue to be denoted conventionally by volumes, issues, years, and page numbers, and in most but not all cases can be downloaded as typeset PDFs. Sometimes the hyperlinks in these articles work, but not always. Indeed, individual journal articles often do not look dissimilar to those articles published decades earlier. Disciplinary pressures have contributed to this situation. Peer review is essential for the professional recognition of scholarship, typeset PDFs lend credence to publications, and evaluators of CVs often want to see page numbers. Similarly, while open access is an increasing part of the landscape, many journal publications remain closed: limited to institutional subscribers or those who might choose to pay for individual articles. Gold open-access models, requiring authors to pay article processing charges to make a publication available to all readers, are an awkward fit with the modest funding ecosystem available to historians. These pressures are also present in the case of books: publishers take author manuscripts, transform them into typeset documents, and have them bound and published for sale.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Granting agencies, scholarly foundations, scholarly associations, and individual researchers have been spurring a slow transformation of book and journal publishing. MIT Press, for example, now publishes new monographs and edited collections as open access. They are fully downloadable for free, thanks to a combination of support from the Arcadia Fund as well as fees from participating libraries (who receive access to the otherwise closed back catalogue). In 2015, the University of Minnesota Press partnered with the City University of New Yorks Digital Scholarship Lab to launch the Manifold Scholarship platform. The Manifold platform allows scholars to host rich web-based books - often but not always as a complement to a print book. These rich digital editions facilitate reader interaction with primary sources and images. They allow readers to interact with the author and with each other through in-line annotations and commenting. While the platform has been used by several publishers, the University of Minnesota Press also works with authors to publish iterative drafts of books. These publicly take shape, allowing authors to engage with readers as a draft is written.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Journal and book peer review is also changing. Public participation in open peer review, where drafts are publicly posted and readers invited to comment, either informally through in-line comments or annotations or more formally through comprehensive reader reports, aims to produce stronger scholarship through productive open conversations. This process provides diverse perspectives on scholarship, more so than just the two, three, or four standard peer reviewers that evaluate most articles. This model has been around for a while now. In 2014, writing a Digital History textbook alongside my co-authors Shawn Graham and Scott Weingart, we posted the draft of our draft manuscript as sections were written online. These helped build confidence in our textbook many eyes made for stronger work – although we did note in a retrospective for Perspectives in History that it took a bit of a thick skin. It was one thing to receive [reviews] quietly in your office when a peer review arrives, but another to undergo the process in public. Ultimately, our experiment with open review became one of the most rewarding elements of the book.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Recently, even long-established journals have experimented with these new peer review approaches. In 2020, the American Historical Review hosted an open review of the article History Can Be Open Source by Joseph L. Locke and Ben Wright. The review process combined two evaluation approaches. The journal editor solicited readers who would provide traditional reports and had those publicly posted. The journal also complementarily hosted an open review platform for paragraph-level comments on both the initial as well as the revised manuscript. Anybody could provide feedback on the manuscript. The dozens of comments are testament to an openness towards these new experiments in publishing.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Some recent books are beginning to push at the boundaries of traditional publishing, forcing a rethink of what a ‘book in the digital age might look like. Stanford University Press has emerged as a leader in this space, rethinking digital monographs not just as enriched PDFs or websites but as fully digital publications. Elaine Sullivans Constructing the Sacred: Visibility and Ritual Landscape at the Egyptian Necropolis of Saqqara, winner of the 2020 Roy Rosenzweig Prize for Innovation in Digital History, exemplifies this type of publication. Designed as a digital object first, this argument-driven digital monograph contains videos, high-resolution maps, and even interactive 3D models where you can explore Saqqara, Egypt, and move a timeline slider to explore changes over time. The integral nature of the integrated digital visualizations means that any book version of Constructing the Sacred would be a pale imitation.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Journals are also adopting new forms in the digital age. In 2021, the Journal of Digital History launched articles which were essentially code-driven ‘notebooks. They combined interactive code snippets and narrative prose. One can interact with a database while reading the article, for example, allowing a user to interact with the primary sources or information on the very same page as the article itself. This approach to transmedia storytelling allows researchers to feature their methodological approaches through a hermeneutical ‘layer, resulting in a new form of scholarship. While still a new journal, their first issue contained articles on text mining in newspapers, Twitter mining to understand public commemoration and computational explorations of Tacitus works.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - These new approaches present challenges. Sustainability is first and foremost of these. New forms of dynamic and interactive scholarship require new methods of digital preservation to ensure a book or journal is replayable in a decade or century from now. Workflows designed to preserve print scholarship or static PDFs do not always work with dynamic, interactive projects. At Stanford University Press, a years-long project funded by the Andrew W. Mellon Foundation witnessed a collaboration between the Press, a web archiving project (Webrecorder), and the Stanford Digital Repository to ensure the long-term preservation of digital books. Their project blog underscores the sheer difficulty facing even a very well-resourced team in ensuring the high-fidelity replay of these books in the future.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - The final challenges present in all of this as well are the financial challenges: book and journal publishers rely on purchases and subscription fees to recoup expenses (and make a profit in some cases to enable the cross-subsidy of less-commercially successful works). In the case of book publishers, this extends far beyond producing a print object, but include the work that goes into selection, peer review, marketing, editorial feedback, and beyond. Open access requires different financial models to succeed. Perhaps the success of the MIT Press open access approach will spur broader sectoral change.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - To end this section on a meta point, let me turn to forms of publications such as the one that you are reading: linear publications that take the shape of a book or journal article, but which take advantage of new opportunities presented by digital technology. There are a variety of these kinds of publications. Short monographs such as Cambridge Elements, Palgrave Pivots, Oxford Very Short Introductions, and beyond; 20,000 to 30,000 words, often consumed digitally rather than via print. This is made possible by the digital turn. While open-access publishing in traditional historical venues, such as books and journal articles, is relatively rare due to its cost and smaller grant agency support in the humanities, it is growing (especially in the United Kingdom where funders are increasingly mandating this). Naturally, we are now seeing the rise of digital editions: dynamic digital versions to complement (rather than replace, in the case of Stanford University Press digital projects) traditional print versions, although experiences have been uneven.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Challenges aside, it is clear that traditional forms of the book and the journal article are beginning to be reshaped in new and interesting ways. Perhaps this represents a useful pathway forward for the profession. Such scholarship is intelligible to history departments, external evaluators, and deans, but draws on new and emerging technologies. Ultimately, the digital age will perhaps be most visibly demonstrated in the changing ways in which historians publish and share their research.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - We are in a transitional stage of digital publishing: innovations of novel approaches are combined with the linear argumentation and recognizable form of a monograph or article. It can accordingly make a strong professional impact. Indeed, it is this calculus that lies in the Element you are reading: a linear argument in a recognizable form, but accessed and shared online.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Ultimately, the shape of how historians exchange our findings and engage with the public determines what history will look like in the digital age. It is exciting: historians no longer simply need to turn to a handful of presses and venues through which to publish work in the same typeset, linear fashion. Yet it is also a bit worrying: choice overwhelms, and the variety of mediums means that their reception amongst the public and the profession are uncertain. The final question then is: what can we do to transform the historical profession so that it can meet the challenges of the digital age?
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - The profession of history has undergone dramatic transformation in the digital age an ongoing process accelerated even further by the COVID pandemic. New and emerging technologies have changed how historians engage with libraries and archives, as scholars increasingly avail themselves of new approaches which allow them quick access to information, such as digital photography or keyword search. Increasingly, archivally focused historians are part of a desk discipline where data collection and data analysis are bifurcated stages. Compounding this, historical work with documents more generally has changed as researchers are drawn towards digitized sources and away from contextually aware research. These digitized sources are in turn shaped by algorithms that are rarely seen or understood. Finally, the way in which historians disseminate their findings, from blog posts to books, has also shifted, requiring new approaches to how the profession values and understands scholarship. And these changes continue, with dramatic implications for how we carry out our craft. What should historians do as a professional discipline to respond to these changes?
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Our profession needs to transform in four main ways: understanding the importance of digital literacy, recognizing the value of interdisciplinarity, prioritizing methodological discussions and reflections, and finally, changing part of our training process. The challenge of digital sources may at first overwhelm, but several small, incremental tweaks and recognitions can put historians on a more rigorous path. As historians have long understood the value of context, we are well positioned to rise to these challenges.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - We need to first recognize the value of digital literacy, something which is necessary not just for historians but also for all scholars. All historians have been transformed by digital technology, whether they are Digital Historians or historians influenced by the digital turn (digital historians). Given the wide array of digitized primary and secondary sources, ultimately all historical sources are mediated to some degree through technology. Even sources still consulted exclusively on paper have been made more easily discoverable by digitized finding aids, or they are contextualized and read within a larger body of digital material. As noted at the beginning of this Element, the genie is out of the bottle. We are not going back to a pre-digital era of scholarship where we eschew the advantages of digital technology, any more than scholars of the early sixteenth century were going to revert to manuscript-only publication. Yet we need to consciously think about the role of the digital. By citing sources as they are mediated a newspaper from the Globe and Mail as found in ProQuest, for example, or via a microfilm reel can both make historians think consciously about their research methods as well as open opportunities for peer reviewers to push authors to be more self-reflective. Through this, historical methods will be explicitly spelled out rather than left implicit. This in turn can spread to the classroom, as educators encourage students to critically reflect on the mediation and selection of their sources (and as exemplified by the texts they read). This in turn can help make our students more critical and conscious consumers of information.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Accordingly, the ways that we publish and write also need adjustment. Historians too often shy away from explicit methodological discussions, relegating such conversations to footnotes or the scholarly equivalent of the cutting-room floor. Yet if historians want consciously to reflect in our writing on the role of technological mediation, more discussions are needed. Perhaps that can take the shape adopted by Blevins study of the post office: a note at the front of the book, inviting the reader to explore more in companion material. Or, in journal articles, it could become fundamental to research practice a recognition that process and mediation are as significant as the finding themselves, given the scale of the repositories we are exploring. While historians enjoy good, engaged writing, scholarly journals are sites of professional, specialist discussion. Surely there is more room for methodological discussion.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - The second necessary change is that we need to understand the role played by everyday interdisciplinarity. As we saw earlier, the growing gulf between archives and historians that has emerged since the 1970s means that as Blouin Jr. and Rosenberg argued a historian using an archive is engaging in a form of interdisciplinary research. Historians need to embrace this. We should cite archivists, drawing on the development of archival theory which is happening in venues such as Archivaria or the American Archivist, as well as on an array of scholarly blogs and edited collections. This scholarship explores the way in which archives (digital and traditional alike) are constructed, with profound impact on historical scholarship. This logic extends to other platforms and interfaces. When we use a search engine or a database, we are also engaging in a form of interdisciplinarity as the products of other academic fields shape the structure of our knowledge. Why is one result ranked #1 and another #100? What technology was used to transcribe a document? There are robust scholarly literatures on these questions, which can help us open the black boxes that control and shape scholarly research. Interdisciplinary engagement with the library and information studies field can better complicate digitally informed historical work.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - The third necessary transformation is a need to privilege methodology more generally. This must extend to how we conceive of, and organize, our profession. In hiring fields and curricula, and even the way we present ourselves on departmental webpages and each other, historians privilege geographic fields and temporal periods. One is a twentieth-Century historian of the United States, a nineteenth-century historian of Canada, or a historian of Postwar Britain, a global historian, and so forth. Even when digital history is a central consideration, job advertisements tend to specify a geographic area as well. Yet the forces discussed in this Element transcend geography and era. Digital technology affects everybody. Methodological discussions also struggle to get onto the content-heavy curricula of courses. While we might see geographic context or period as keys to building a professional foundation, algorithmic or methodological context is equally important. Just as we need to have room for methods sections that do not get cut by editors, we need openness to hiring, tenuring, and teaching by method rather than geography. Part of the solution to this might be the embracing of more interdisciplinary offerings across the curriculum. Considering the common methodological needs of historians, English language and literature, and anthropology, for example, might help broaden curricula and help regenerate our teaching approaches for the twenty-first century.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - This leads to the fourth and final factor: the need to change how historians are trained. Required undergraduate methods courses, especially those in digital methods, are rare in North America. Methods courses are not just needed to teach undergraduates about how to become historians, but more importantly, how to become attuned to issues of context, algorithms and beyond that can equip them to be good citizens. Graduate education primarily focuses on content, not craft. In general, historians learn to become professional historians through an apprenticeship model. They learn from their supervisors and committee members how to be rigorous researchers. This approach is generally sound, but its emphasis on reproducing past patterns of scholarship and research means that major paradigm and medium shifts can be missed. The role technology has played in historical scholarship has happened so slowly, that it has almost happened invisibly. It is only when looking back over twenty years that we can see how profoundly our research workflows have changed.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Historians need to recognize that they are living through a major change in how historical research is carried out, and accordingly change the way in which they train, write, and think about the past and its mediation. There has been some sporadic discussion of this in the profession: special sections exploring digital methods in flagship journals such as the American Historical Review, as well as well-attended conference roundtables at conferences such as the American Historical Association. More needs to be done, however, given that these digital transformations are arguably the defining issue of the profession today.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - There are obstacles to these changes. Many of these stem from the hierarchical nature of the historical profession. As a self-regulating profession, senior members make decisions about what will or will not be valued. These include conversations about whether interdisciplinary scholarship belongs in a given journal or conference, to the role of methodological discussions in published scholarship, to the organization of graduate programs. Institutional change comes slowly. Academics need to be engaged in their service and leadership networks, so as to ensure our profession can rise to the challenges before it.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - These issues cut across the disciplinary silos of the modern university. All students need an awareness of algorithmic bias, an understanding of how content is mediated and contextualized, and broader digital literacy skills. We do a great deal of implicit leaning on the idea of a digital native, which ignores students uneven technical skills. We all need to engage with these cross-disciplinary problems that are transforming our world.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - The potential benefits reach beyond the academy. Indeed, these skills might make a history student better able to understand the provenance of a digital source they are looking at, but they might also make them a citizen better able to understand and parse the plethora of (mis)information they are confronted by daily. Thinking about the context and mediation of a newspaper article from decades-old newspapers develops skills to evaluate the trustworthiness of a tweet or a newspaper article that pops up on Twitter or TikTok. Perhaps this is ultimately less about making good historians than making us all better consumers of information.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - The importance of context has been an important theme throughout this Element. Historical scholarship is often driven by historical documents from tweets to government paperwork to private correspondence. In all cases, historians seek to understand them in their historical context, including authorship, reception, environment, culture, and period. To this, I would add source mediation and algorithms. As we increasingly rely on digital information, historians, and our grasp of context, will become more important than ever.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Consider the pressing problem of deepfakes, fake yet realistic-looking videos manipulated or generated by artificial intelligence. As videos are considered trustworthy, these present a very real challenge to how we understand what we seem to be seeing with our own eyes. These range from viral yet fake videos of movie star Tom Cruise saying and doing odd things such as magic tricks or praising bubble gum, to bringing historical figures to life so that it seems that Abraham Lincoln was captured on video. They portend an increasingly fraught historical record to come. Yet historians have always been experts at parsing misinformation, bias, misrepresentation and beyond in traditional archival repositories (the provenance of material in Library and Archives Canada might be assured, but historians still interpret individual documents there with care). If we read an archival document that surprises us as an expert reader, breaking with our accepted understanding, a good historian does not rashly jump to conclusions. We instead try to contextualize what we have just learned. A historian attuned to historical context should not be hoodwinked by an outlier.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Historians are specialists in critically reading documents and then confirming them. Findings are synthesized and contextualized among a broad array of additional sources and voices. Historians tie together big pictures and findings. The work of a historian might look different in the twenty-first century – exploring databases, parsing digital information – but the application of our fundamental skills of seeking context and accumulating knowledge will serve both society and the historical profession well in the digital age.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Changes are rarely ever simply good or bad. Rather, these transformations require further contextualization and conscious deliberation. In many cases, being self-conscious and self-reflective in the way that we carry out our scholarship ensures that we will better benefit from the resources and approaches that we draw on. Historians are all digital now: they need to embrace that reality. There is room for a robust field of Digital History that can use computational technology in new ways to push forward historiographical frontiers, but it cannot be the only site of critical engagement with technology across our profession.
[Author: Ian Milligan; From essay:"The Transformation of Historical Research in the Digital Age "] - Next time you pick up a digital camera in an archive and take a picture of a document, or search for a document on ProQuest, or tweet about your research, pause and reflect on what you are doing. Ask yourself these questions. Is the platform changing what I am doing? How can I make sure I am controlling my research workflow? A series of critical questions can make us better consumers of information. Similarly, the next time you see a viral tweet prompting you to hesitate to take a vaccine or to panic about something, perhaps you will be better equipped to understand how it is being mediated. If we are all digital, imagine the opportunities that lie ahead.
[Author: Javier Cha and Ian Miller; From essay:"Digital Humanities and the Energetics of Big Data "] - In 2008, the National Library of Korea prepared to open its Digital Library annex, a key milestone in realizing its vision of becoming a “ubiquitous library”. However, this transition was met with an unanticipated obstacle: energy consumption. The deployment of an online database with 100 million electronic items, RFID tagging, and computer vision-based tracking systems resulted in a marked escalation in electrical demand. This increase was further intensified by the installation of user terminals, backup systems, and creative media studios. Consequently, the National Library of Korea had to seek an additional \400 million Korean won (approximately $350,000 US dollars) to ensure the provision of adequate power for these new services.
[Author: Javier Cha and Ian Miller; From essay:"Digital Humanities and the Energetics of Big Data "] - The National Library of Koreas experience serves as a reminder of the substantial energy demands tied to digital infrastructure and foreshadows similar challenges now emerging in digital humanities (DH). As DH moves beyond personal computing into the realm of big data, the ecological impact of the systems researchers rely on regularly has become a pressing concern. As per Doug Laneys 3Vs and the Oxford English Dictionarys formal definition, big datas dynamic and distributed nature, along with its vast scale and complexity, require extensive, energy-intensive infrastructure. The DH community has promoted minimal computing as a strategy to reduce the environmental costs associated with creating, operating, maintaining, and preserving DH projects. However, minimal computing did not anticipate the recent surge in large-scale artificial intelligence (AI) systems trained on big data, such as large language models (LLMs) and image-generation models. The shift toward centralized computing clusters calls for updated approaches to ensure sustainability, yet balancing technological advances with eco-friendly practices remains a daunting task for DH.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - Deleuze and Guattari introduce a radical notion of multiplicity into phenomena which we traditionally approach as being discretely bounded, structured and stable. Assemblages consist of a multiplicity of heterogeneous objects, whose unity comes solely from the fact that these items function together, that they “work” together as a functional entity. They comprise discrete flows of an essentially limitless range of other phenomena such as people, signs, chemicals, knowledge and institutions. To dig beneath the surface stability of any entity is to encounter a host of different phenomena and processes working in concert. The radical nature of this vision becomes more apparent when one realizes how any particular assemblage is itself composed of different discrete assemblages which are themselves multiple.
[Author: Javier Cha and Ian Miller; From essay:"Digital Humanities and the Energetics of Big Data "] - Our investigation into the intersection of DH and the energetics of big data is structured into two sections, each aimed at fostering further discussions. First, we argue that big data marks a radical departure from pre-digital media in terms of resource and energy usage. While recording information on epigraphs, paper, woodblocks, and movable types require raw materials and human labor, no external energy is necessary to read it. Conversely, hard disks (HDDs), solid-state drives (SSDs), and optical discs depend on electricity for both data storage and retrieval. Although the energy demand for a single personal computer can be met with a solar panel the size of a backpack, hundreds of workstations have already begun to pose logistical challenges, as the National Library of Korea discovered. Corporate data centers, which form the backbone of big data, and their attendant information communication technology (ICT) infrastructure have much greater power requirements. Kak, South Koreas tech giant Navers flagship data center, handles a data volume equivalent to one million National Libraries of Korea. This facility consumes an immense 156,875 MWh of electricity annually, sourced from six nearby hydroelectric power stations, and the 120,000 servers hosted in the facility generate a tremendous amount of heat as a byproduct of data processing. Todays age of big data prompts DH to address the direct links among data flows, power generation, grid networks, carbon footprint, and cooling.
[Author: Javier Cha and Ian Miller; From essay:"Digital Humanities and the Energetics of Big Data "] - The second section turns to paradoxes and tradeoffs. The big data turn in DH must grapple with the massive capital and energy flows required in handling exabyte-scale data streams and beyond. The National Library of Koreas new Data Preservation Center in Pyongchang, with the capacity to store 14 million items of cultural significance including archival-grade optical discs in a climate-controlled setting, is currently under construction with a budget of \61 billion Korean won ($46 million US dollars). In comparison, Naver has invested a staggering \1.9 trillion Korean won ($1.5 billion US dollars) in ICT infrastructure from 2019 to 2022. Meanwhile, the global expenditure on cloud services reached a remarkable $178 billion US dollars in 2021. These figures raise critical questions about the relationship between DH and big data. Given that only a fraction of todays big data is likely to be preserved for posterity, it remains uncertain whether future humanities scholars will have the financial and electrical resources to access the hypothetical mega-archives of tomorrow.
[Author: Javier Cha and Ian Miller; From essay:"Digital Humanities and the Energetics of Big Data "] - From an environmental standpoint, leading cloud service providers, including Amazon, Microsoft, Alphabet (Google), Meta (Facebook), Naver, and Alibaba, are at the forefront of innovations and investments in energy efficiency and renewable energy sources. However, their business models inadvertently contribute to a modern Jevons Paradox, where increased efficiency leads to higher overall consumption, output, and a greater reliance on centralized infrastructure. Despite ongoing initiatives to decentralize the web and reduce the dominance of Big Tech, practical alternatives remain elusive. In fact, at the time of writing this chapter, the rise of large-scale Al has accelerated the concentration of computing power and energy use in Big Tech infrastructure to an unprecedented level. Our research suggests that a viable approach to addressing these challenges lies in drawing insights from historical perspectives, reliable statistics, technical analysis, regional differences, and the coexistence of older and newer information regimes.
[Author: Javier Cha and Ian Miller; From essay:"Digital Humanities and the Energetics of Big Data "] - The Energetics of Data Production and Preservation
[Author: Javier Cha and Ian Miller; From essay:"Digital Humanities and the Energetics of Big Data "] - To understand how the energy consumption of big data stacks up against that of non-digital media, we build upon the insights of Gilbert Shapiro, John Markoff, and Silvio R. Duncan Baretta. Their framework, which assesses the likelihood of a historical documents survival, encompasses various phases: recording, reproduction, preservation, cataloging, and publication. Our back-of-the-envelope calculations estimate the energy costs associated with each juncture, factoring in both the embodied energy inherent in the materials used for crafting these records and the cumulative human and machine labor for their reproduction and preservation.
[Author: Javier Cha and Ian Miller; From essay:"Digital Humanities and the Energetics of Big Data "] - When considering the energy overheads of non-digital and digital media, a dichotomy emerges. The creation and replication of non-digital artifacts, while more energy-intensive initially, starkly contrast with digital materials, which demand consistent energy inputs for their sustenance and access. The history of recording media uncovers a trend towards the diminishing of energy needed to record a given amount of content. The process of inscribing text onto bronze, such as those found in early Chinas ritual vessels, required about 3 GJ and an equal amount of energy was needed for its replication. A substantial part of this energy was invested in the creation of the bronze Itself. In contrast, transcribing manuscripts onto paper or parchment drastically slashed this energy demand to between 0.5 to 2.5 MJ per page, even though the same amount of energy was still needed to make copies of the original.
[Author: Javier Cha and Ian Miller; From essay:"Digital Humanities and the Energetics of Big Data "] - The introduction of print marked a leap in reducing energy usage, particularly for reproduction. While engraving a page onto a woodblock was about six to sixty times more energy-demanding than manuscript writing, the subsequent printing process drastically cut the energy cost to about 200 kJ per page, a mere tenth of the energy needed for manual transcription. Advancements in printing technology continued to drive down these costs. The energy required for producing the first copy of a page dropped to about 2 MJ for hand letterpress and under 1 MJ for rotary print. The digital age has ushered in even greater efficiencies. Digital text production outpaces the finest print technologies by a factor of ten in energy efficiency, and digital replication surpasses a two-thousandfold increase in efficiency. This monumental decrease in energy consumption has catalyzed an unprecedented expansion in digital media, eclipsing all prior media forms in volume and reach.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - Assemblages, for Deleuze and Guattari, are part of the state form. However, this notion of the state form should not be confused with those traditional apparatuses of governmental rule studied by political scientists. Instead, the state form is distinguished by virtue of its own characteristic set of operations; the tendency to create bounded physical and cognitive spaces, and introduce processes designed to capture flows. The state seeks to striate the space over which it reigns, a process which involves introducing breaks and divisions into otherwise free-flowing phenomena. To do so requires the creation of both spaces of comparison where flows can be rendered alike and centres of appropriation where these flows can be captured.
[Author: Javier Cha and Ian Miller; From essay:"Digital Humanities and the Energetics of Big Data "] - The remarkably low production and reproduction costs of digital media come with a noteworthy caveat in the form of reduced longevity and higher upkeep demands. In standard personal computers, CPUs and GPUs process data loaded into volatile memory, such as L1, L2, L3 cache, and DRAM, which is vulnerable to immediate data loss in the event of power interruption. Non-volatile storage technologies, including HDDs, SSDs, optical discs, and magnetic tape drives, require electricity, albeit for different reasons. To avert data deterioration, these devices are typically housed in climate-controlled facilities. Optimal conditions for magnetic tapes and optical discs are within a temperature range of 15 to 25°C and humidity levels between 30 and 50%. HDDs, particularly helium-sealed drives, demonstrate greater resilience and tolerate broader a temperature range from -40 to 70°C, yet should still be shielded from direct sunlight. Modern cloud computing categorizes data as hot, warm, or cold according to usage patterns and energy use. The typical lifespan of SSDs, primarily designed for handling high-demand or “hot” data, averages between 7 to 10 years. Unpowered SSDs may risk data loss over time due to the degradation of their NAND flash components. Cold-storage data centers, such as Metas Prineville facility, store infrequently accessed data on low-energy archival-grade HDDs and Blu-ray discs, boasting shelf lives of 30 and 100 years, respectively. Millenniatas M-DISC and Microsofts Project Silica exceptionally provide data preservation on media designed to last more than 1,000 years, but their adoption remains limited. In the ICT sector, LTO tape drives, with about 30 years of service life, are a favored backup solution. Most of these digital memory and storage systems rely on continuous power for data longevity.
[Author: Javier Cha and Ian Miller; From essay:"Digital Humanities and the Energetics of Big Data "] - In contrast, preserving paper primarily involves maintaining a dry environment and avoiding temperature fluctuations, while bronze requires minimal upkeep. The lifespan of digital media, usually only several years to a couple of decades, is significantly less than the centuries that paper can last or the millennia for bronze artifacts. This stark difference in power usage between creation and ongoing maintenance across various media types is noteworthy. The ICT industrys data preservation strategies revolve around redundancy and regular duplication, which mirrors the way historical records have been preserved through multiple copies and iterations. For example, updates to early modern Chinese genealogies occurred approximately every sixty years, while reproductions of classical texts took place about once per century. Digital media, however, stands out for its frequent need for replication to ensure data preservation, diverging from the slower duplication processes seen with other media. In todays age of big data, this emphasis on data redundancy and frequent replication has become even more paramount, especially from an energetics standpoint. Despite each act of digital replication consuming minimal power, the aggregate effect of these repeated operations leads to substantial energy use over time.
[Author: Javier Cha and Ian Miller; From essay:"Digital Humanities and the Energetics of Big Data "] - The differing energy requirements of various media types have profound consequences for scholars in the humanities, particularly historians who engage with materials older than several decades or centuries. The power involved in digital publication is considerably less than that for a printed book, which in turn is more energy-efficient than manuscript production, and far less than the energy needed for creating epigraphic materials. Media that demand higher energy for creation typically boast longer lifespans, while those requiring less energy facilitate a greater volume of work. This dynamic results in digital materials outnumbering printed media, with printed works surpassing manuscripts and epigraphic artifacts in quantity. Scholars specializing in early periods tend to depend on materials preserved on more enduring media or those that have been replicated over time, typically missing out on ephemera. In contrast, researchers focused on more contemporary periods have access to a plethora of ephemera, but face uncertainties regarding their long-term preservation.
[Author: Javier Cha and Ian Miller; From essay:"Digital Humanities and the Energetics of Big Data "] - Paradoxes and Tradeoffs
[Author: Javier Cha and Ian Miller; From essay:"Digital Humanities and the Energetics of Big Data "] - The modern DH researchers work is increasingly intertwined with the technological advancements and infrastructure of major tech corporations, impacting not simply research methodologies but also energy consumption and environmental footprints. This growing dependence aligns with scholarly discourses on the material and energy exigencies of digital devices, server farms, and telecommunication networks, which collectively challenge the perception of digital technologies as immaterial or disembodied. Friedrich Kittlers “There Is No Software has opened the door to understanding the physical underpinnings of digital technologies. Matthew Kirschenbaums Mechanisms and Bitstreams have demonstrated the application of forensics in the study of electronic literature and digital collections. Kate Crawfords Atlas of AI has, in turn, taken a critical look at the AI industrys dependence on precious minerals, water, and electricity, extending beyond Kittlers perspective limited to silicon. Thomas Mullaneys introduction to Your Computer is on Fire reminds us that “nothing is virtual”. Further, Nathan Ensmenger urges a reconsideration of “the Cloud as a factory, and not as a disembodied computational device”.
[Author: Javier Cha and Ian Miller; From essay:"Digital Humanities and the Energetics of Big Data "] - DH researchers have thus called for engaging with technology in a discerning and informed manner. This approach entails a critical examination of the material basis of digital tools and an acknowledgment of their significant energy demands. The insights of Mullaney and Crawford prompt deeper reflections on the materiality of technology. Mullaneys advocacy for a critical stance towards the tech industry, described as a “call to arms” with “unapologetically direct and bold arguments”, merits careful contemplation. Crawfords examination of the AI industrys reliance on finite resources underscores the need for a grounded understanding of the physicality and environmental impact of digital technology. However, it is crucial to recognize that in AI and cloud computing, the primary resources are semiconductors, data storage units, and electricity, rather than lithium, which is more central to battery production for mobile devices and electric vehicles.
[Author: Javier Cha and Ian Miller; From essay:"Digital Humanities and the Energetics of Big Data "] - The Jevons Paradox, which observes how technological progress counter-intuitively escalates resource utilization, offers a compelling lens to examine the contemporary ICT landscape. According to a principle recognized in many parallel instances,” William S. Jevons astutely noted, “new modes of economy will generally lead to an increase in consumption. Originally observed in the context of coal use, this phenomenon is also pertinent in understanding the burgeoning of Web 2.0 services in which massive capital investments and the development of centralized infrastructures drove efficiency gains. The technological strides in recent years have resulted in the seamless weaving of digital technologies into various facets of daily life, including academic research.
[Author: Javier Cha and Ian Miller; From essay:"Digital Humanities and the Energetics of Big Data "] - Recent market data appears to reinforce this trend. The period from 2010 to 2020 witnessed a notable uptick in global server shipments, from 8.9 million to 12.15 million units, with each generation of servers surpassing its predecessors in performance and efficiency. On the consumer front, despite a marginal downturn in sales, about 1.3 billion personal computers were operational globally in 2016. Meanwhile, the realm of mobile technology has seen a surge in smartphone subscriptions, escalating from 3.6 billion in 2016 to 6.4 billion in 2022 and an anticipated 7.7 billion by 2028. This meteoric rise in device usage has consequentially fueled an exponential increase in global data volumes, exerting significant pressure on ICT infrastructures. Data volume estimates depict an increase from approximately 41 to 59 zettabytes in 2019, with projections indicating a surge to 175 or 181 zettabytes by 2025.
[Author: Javier Cha and Ian Miller; From essay:"Digital Humanities and the Energetics of Big Data "] - However, a closer examination of the Jevons Paradox in the Web 2.0 transition reveals a more intricate picture than what meets the eye. ICT efficiency gains and market demands manifest in surprising ways. The disparity between the proposed Kryders Law and actual outcomes illustrates this point. In 2005, Mark Kryder projected that digital storage capacity increases would outpace Moores Law and anticipated the availability of 40 TB disks at $40 US dollars by 2020. Contrary to these forecasts, the ICT sector focused less on storage volume and more on performance enhancement and power conservation, driven by the proliferation of mobile devices, cloud computing, and social media. This mismatch between predicted and actual trajectories underlines the unpredictability and complex nature of technological advancements. Theoretical models provide a framework, but real-world tech development and its impact on resources and consumption tend to diverge from expected outcomes.
[Author: Javier Cha and Ian Miller; From essay:"Digital Humanities and the Energetics of Big Data "] - From 2008 to 2023, high-end consumer and enterprise-grade SSDs experienced over 100-fold performance growth in both sequential and 4K random IOPS. In 2008, a standard Seagate HDD required 11,772 seconds (3.27 hours) for sequential 1 TB reads and 1,048,576 seconds (over 12 days) for the same amount at 4K random blocks. The 2023 SSD completes these tasks in under two minutes and approximately two hours, respectively.
[Author: Javier Cha and Ian Miller; From essay:"Digital Humanities and the Energetics of Big Data "] - Additionally, the shift from mechanical disks to flash memory has brought considerable power efficiency gains. In 2009, Intels premium MLC and SLC SSDs already showcased a 27 to 213-fold decrease in energy per TB compared to HDDs. In 2023, Crucials flagship model achieved a further 6- to 10-fold reduction in electricity consumption per TB for sequential reads relative to Intels 2009 models.
[Author: Javier Cha and Ian Miller; From essay:"Digital Humanities and the Energetics of Big Data "] - The environmental and energy costs of global ICT infrastructure similarly present a multifaceted picture. Statements such as “more than 2 percent of global energy use” and 70 billion kilowatt-hours of electricity in 2016 in the United States alone” demand a more nuanced interpretation. Crawfords assertion that “the tech sector will contribute 14 percent of global greenhouse emissions by 2040” carries an important caveat. This is a worst-case scenario based on an exponential fit and contingent on current trends continuing “if unchecked and with “large uncertainty about the lifecycle annual footprint computers... and displays. Relative data are partially measures of ICTs outpaced growth vis-à-vis other industries, while projections are scenario-based ranges with minimum and maximum values. The analysis also varies depending on whether the focus is solely on data center operations or also encompasses the manufacturing, installation, and usage of optical fibers, wireless networks, personal computers, mobile devices, televisions, and gaming consoles. In 2005, data centers accounted for about 1% of the worlds electricity, half of which was dedicated to cooling and power distribution. By 2020, data centers were responsible for 1.4 to 1.6% of global carbon footprint, with communication networks and personal digital devices adding another 1.7 to 2%, while communication networks, desktops, notebooks, displays, tablets, and smartphones emitted an additional 1.7 to 2%.
[Author: Javier Cha and Ian Miller; From essay:"Digital Humanities and the Energetics of Big Data "] - Eric Masanet and his team present a compelling counter-narrative to the dire forecasts about data center energy consumption. Their research indicates that, as of 2020, global electricity usage by data centers remained steady at about 1.1 to 1.5%, due to a concomitant rise in overall power generation to meet the lifestyle demands of the growing middle-class population, particularly in emerging economies. This finding serves as a reminder of the importance of rigorous, substantiated interpretations. Masanet et al.s critique of “oft-cited yet simplistic analyses” that overlook simultaneous trends in energy efficiency is a call for deeper scrutiny. The period between 2010 and 2018 saw a remarkable 25-fold increase in data center storage capacity, but this expansion was counterbalanced by a significant nine-fold reduction in the amount of energy required per unit of storage. While the ICT sector is a major energy consumer and contributor to emissions, it will not consume one-fifth of the worlds electricity by 2025, nor will Japans data centers gobble up the entire nations electricity supply by 2030. A nuanced perspective is vital for crafting strategies, policies, and collective responses that successfully balance the advancement and wider use of digital technologies with the imperatives of environmental sustainability.
[Author: Javier Cha and Ian Miller; From essay:"Digital Humanities and the Energetics of Big Data "] - The objective should be to drastically reduce greenhouse gas emissions in absolute terms and increase the use of renewable energy to 100%. The ICT sector is one of the largest carbon polluters, and industry leaders in the United States have responded with commitments to the renewable transition. Google uses a credit-matching strategy in which it purchases the same amount of renewable energy as its facilities consume. Despite Crawfords criticism of this practice, it is still a positive development and a step in the right direction. Metas global operations have been powered entirely by renewable energy since 2018, and the company aims to achieve net zero emissions across its entire supply chain by 2030. In 2021, AWS reported that 85% of the electricity used in Amazons businesses was derived from renewable sources, with the goal of reaching 100% by 2025. Recently, the cloud giant added 2.7 GW of low-carbon energy capacity in South America, India, and Poland, as well as investing in 379 renewable projects worldwide. Despite these efforts, however, Amazons overall carbon footprint increased by 19% due to the companys growth outperforming its green initiatives.
[Author: Javier Cha and Ian Miller; From essay:"Digital Humanities and the Energetics of Big Data "] - The world outside of the United States needs more than a passing mention. In northern Europe, Meta operates data centers in Odense and Luleå using 100% renewables according to the abovementioned global commitment, and Google powers its Hamina location with electricity from a wind farm in northern Sweden. However, not all facilities in the Nordic countries are environmentally friendly. According to Julia Velkova, renewable sources made up less than 10% of the electricity used to power the Yandex data center in Mäntsälä, Finland, in 2018 and 2019. Between 2010 and 2022, the renewable capacity in Latin America and the Caribbean has nearly doubled from 168 to 314 GW, with Brazil leading the charge. According to Boston Consulting Group, the regions cloud market is worth $10 billion US dollars as of 2022 and will grow at a 30% annual rate. Το meet this demand, Microsoft Chile, Ascenty, Scala Data Centers have already built or are in the process of building carbon-neutral data centers.
[Author: Javier Cha and Ian Miller; From essay:"Digital Humanities and the Energetics of Big Data "] - China also deserves special attention. The Peoples Republic is home to one-quarter of the worlds data centers and the worlds largest emitter of greenhouse gases. Despite its continued reliance on coal, China has reduced its coal use for electricity generation from 72.4% in 2005 to 56.8% in 2020 and has emerged as the leading producer of renewable energy in the world. As of 2021, China total renewable capacity was 1,020 GW, more than three times that of the United States. The issue is that renewable energy appears to be slow to reach the ICT demand concentrated in the east coast. In 2021, BloombergNEF ranked the e-commerce giant Alibaba as the largest buyer of renewable energy among Chinese companies, and Alibaba pledged to achieve carbon neutrality in all its operations by 2030. During the same year, however, Alibaba Cloud reported that only 21.6% of its electricity usage came from clean energy sources, which was the national average in China in 2018. Aware of this issue, the Ministry of Industry and Information Technology and National Development and Reform Commission have mandated that all new hyperscale data centers attain a PUE (power usage effectiveness) ratio of 1.25 by 2025. Chinas challenge is that that remote solar and wind generators in Gansu, Ningxia, Qinghai, Xinjiang, and Inner Mongolia must be connected to digital infrastructure in populous regions via ultra-high-voltage electricity transmission lines. This centrally-planned, supply-driven strategy is met with resistance from various interest groups and will take time to gain local acceptance.
[Author: Javier Cha and Ian Miller; From essay:"Digital Humanities and the Energetics of Big Data "] - Concluding Thoughts
[Author: Javier Cha and Ian Miller; From essay:"Digital Humanities and the Energetics of Big Data "] - The foregoing discussion is written from the perspective of an intellectual and an environmental historian striving for a balance of ideational and materialist viewpoints. Our interest in this topic originates from many hours of musing in graduate school over a narrative of world history from the perspective of information and energy regimes. As area specialists of medieval Korea and early modern China, we approach global and comparative scholarship with an emphasis on the necessity of engaging with sources from multiple continents and in various languages. Our background in premodern history informs our tendency to stress the importance of problematizing historical temporalities in the Annales sense of short-term events, medium-term conjunctures, and enduring longue durée trends.
[Author: Javier Cha and Ian Miller; From essay:"Digital Humanities and the Energetics of Big Data "] - This chapters genesis is also marked by a confluence of unforeseen events. In 2021, when the editors of this Debates in the Digital Humanities volume announced the call for papers, Bitcoins value reached its all-time-high and the web3 community was in the midst of buoyant optimism about the imminent coming of a decentralized web. By the time we completed this chapter, however, the ICT discourse radically shifted with an overwhelming focus on large-scale AI and centralized infrastructure, spearheaded by the advent of OpenAIs ChatGPT.
[Author: Javier Cha and Ian Miller; From essay:"Digital Humanities and the Energetics of Big Data "] - Concurrently, Javier Cha, one of the authors of this chapter, conducted interviews and field research exploring data centers and digital archive solutions, notably at Googles Hamina location, Microsoft Researchs Project Silica, and the Arctic World Archive in Svalbard.
[Author: Javier Cha and Ian Miller; From essay:"Digital Humanities and the Energetics of Big Data "] - Chas journey from Hong Kong to Svalbard required more than 20 hours of air travel. The sobering spectacle of barren landscapes, receding glaciers, and snowless peaks during the descent of Longyearbyen revealed the stark realities of climate change in a region warming at quadruple the global average. The surreal depositing of a symbolic piqlFilm in a decommissioned coal mine within the Arctic Circle fostered a mix of humility and cautious optimism. The striking sight of air conditioners installed inside the Global Seed Vault, a response to the permafrost failing to maintain the required coolness during summer, raised questions about the longevity of the digital data stored in the Arctic World Archive. This poignant experience profoundly echoed what Bethany Nowviskie means by “graceful degradation, preservation, memorialization, apocalypse, ephemerality, and minimal computing with respect to digital humanities practice in the Anthropocene.
[Author: Javier Cha and Ian Miller; From essay:"Digital Humanities and the Energetics of Big Data "] - The path forward for DH requires an in-depth understanding of the energy dynamics in digital information processing. Advanced cold-storage solutions such as Project Silica, M-Disc, and piqlFilm prompt new inquiries about the ultra-long-term preservation of digital data, spanning thousands to millions of years. Yet, in the immediate term, the transformation of the humanities into a substantial energy consumer is a pressing concern. Minimal computing principles, while valuable, is inadequate for addressing this surge in energy demand. Unexpectedly, the recent shift to AI-optimized servers in data centers has led to a marked increase in electricity and water usage, far exceeding that of conventional Web 2.0 setups. Renewable energy sources power many of these centers, and district heating systems in Nordic and Baltic regions show how excess heat can be effectively reused. Our own experiences with large language model training on high-end NVIDIA guzzlers highlighted the significant energy demands of such tasks, often extending over days or weeks, underscoring the urgency to grasp the full scope and future implications of these trends.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - Flows exist prior to any particular assemblage, and are fixed temporarily and spatially by the assemblage. In this distinction between flows and assemblages, Deleuze and Guattari also articulate a distinction between forces and power. Forces consist of more primary and fluid phenomena, and it is from such phenomena that power derives as it captures and striates such flows. These processes coalesce into systems of domination when otherwise fluid and mobile states become fixed into more or less stable and asymmetrical arrangements which allow for some to direct or govern the actions of others.
[Author: Javier Cha and Ian Miller; From essay:"Digital Humanities and the Energetics of Big Data "] - Our hope is that the DH community leverages its expertise in digital technologies to probe the intricate relationship between todays information systems and power generation. The proficiency of DH scholars with digital media and computational methods, combined with the traditional humanities strengths in critical thinking, cultural understanding, and linguistic skills, positions us uniquely to contribute meaningfully to this dialogue. Addressing the environmental impact of digital technology usage necessitates local collaboration with scientists, engineers, educators, and policymakers, especially in communities most affected by climate change. Bridging the gap between the research of activist groups and corporate green claims, DH specialists can create rich, context-specific strategies to mitigate big datas environmental impact, playing a pivotal role in shaping a more sustainable digital humanities landscape.
[Author: Rachel Sagner Buurma; From essay:"Search and Replace Josephine Miles and the Origins of Distant Reading "] - From this day forward, every time you see the name Roberto Busa invoked as a—or the—founding scholar of either quantitative or computational method in the humanities, we want you to mentally search and replace with another name: Josephine Miles.
[Author: Rachel Sagner Buurma; From essay:"Search and Replace Josephine Miles and the Origins of Distant Reading "] - Miles was a poet and an English professor at Berkeley. In the 1930s, as a graduate student at Berkeley, she completed her first distant reading project: an analysis of the adjectives favored by Romantic poets. In the 1940s, with the aid of a Guggenheim, she expanded this work into a large-scale study of the phrasal forms of the poetry of the 1640s, 1740s, and 1840s. In all of this distant reading work, Miles created her tabulations by hand, with pen and graph paper. She also directed possibly the first literary concordance to use machine methods. In the early 1950s, Miles became project director of an abandoned index-card-based Concordance to the Poetical Works of John Dryden. Partnering with the Electrical Engineering department at Berkeley, and contracting with their computer lab and its IBM tabulation machine, Miles used machine methods to complete the concordance. It was published in 1957, six years after she and several woman graduate students and woman punch-card operators began the work. It was thus begun around the time that Busa circulated early proof-of-concept drafts of his concordance to the complete works of St. Thomas Aquinas, and published 17 years before the first volumes of the 56-volume Index Thomasticus began to appear.
[Author: Rachel Sagner Buurma; From essay:"Search and Replace Josephine Miles and the Origins of Distant Reading "] - There are good reasons, of course, that scholars and journalists like to begin with Busa: he was the first concordance-maker to automate all five stages of the process, in 1951. Busa also sought out high-profile partnerships with Thomas Watson of IBM and others; he foregrounded the innovative nature of his work, and his index incorporated new programming approaches as they developed through the later 1950s and 60s. Miles, on the other hand, worked with the engineers and machines that were close at hand in nearby Cory Hall; she credited the woman typists and punchcard operators she collaborated with; she valued “imaginative” programmers.” And Miles came to the discarded Dryden concordancing project from her own difficult experiences attempting to convince the University of California Press to publish her data along with her literary-critical and literary-historical interpretations of it. When she started work on the Dryden project, Miles was already deeply experienced with the problems of publication and workflow that swirl around the history of concordancing, both manual and machine.
[Author: Rachel Sagner Buurma; From essay:"Search and Replace Josephine Miles and the Origins of Distant Reading "] - What we stand to gain from Miles is therefore twofold. Most importantly, we find in her work a literary genealogy for distant reading to stand alongside other long genealogies that track the rise of quantita or empirical approaches to literary history” through literary sociology, for example, as Ted Underwood does. Miles understood the latest in computational concordancing as influenced by the “inventive” concordancing tradition of the last generation, including the great Lane Cooper Concordance for Wordsworth, Cornell 1911”. Miles saw concordances and machine indexing as a core part of literary criticism, for they could help scholars to a broader view of comparisons between poems and poets. And Miless distant reading work was not only literary, it was in an important sense modernist: her work tested and overturned some of her generations defining accounts of modernist and metaphysical poetry as hard” or “concrete.” Miless distant reading projects are therefore part of the history of twentieth-century poetry. And she did not limit her “tabular view” to literary history; her quantitative work also influenced her own poetic style and shaped the Verse Composition course she taught at Berkeley to poets like A. R. Ammons and Jack Spicer.
[Author: Rachel Sagner Buurma; From essay:"Search and Replace Josephine Miles and the Origins of Distant Reading "] - Replacing Busa with Miles has a second function: it can stand as an example of how we might write a history of literary scholarship that does not center originality and individual accomplishment. Though Busa had the help of many associates—Melissa Terras has tracked down and credited Busas female punch-card operators—their names were not credited in his published work. Miles, by contrast, quite carefully credited the women who worked on her projects. She gave the names of her two graduate student workers, Mary Jackman and Helen S. Agoa, on the cover of the Dryden index. She spoke in the preface and in later interviews about the importance of her collaboration with Penny Gee of Berkeleys Cory Hall computing lab. And she emphasized that her Renaissance, Eighteenth-Century, and Modern Language in English Poetry: A Tabular View (1960) would never have been finally printed had not “an heroic woman” and master typist” at the University of California Press offered to type the charts.
[Author: Rachel Sagner Buurma; From essay:"Search and Replace Josephine Miles and the Origins of Distant Reading "] - Substituting Miles” for “Busa,” then, allows us to imagine an origin story for computationally-assisted reading that comes from within the discipline of literary study, and involves a woman project lead who preserved the names of her female collaborators. Miless career also offers us another origin point for distant reading and quantitative methods of literary analysis. And it helps us see how Miless computational concordance project and her distant reading work informed one another, and together informed her own composition of poetry and teaching of poetic composition.
[Author: Rachel Sagner Buurma; From essay:"Search and Replace Josephine Miles and the Origins of Distant Reading "] - The Dryden project initially entered Miless life as something of a “departmental obligation and chore.” When Miless colleague Guy Montgomery died in 1951, he left behind him 64 shoeboxes full of index cards towards a concordance to Drydens complete works. George Potter, the English department chair at Berkeley at the time, wrote to Montgomerys former students, but none could take on the project. Potter asked Miles to do the job because her scholarship had focused for over a decade on concordances and tabulation. Miles remembers Potter saying, “You use concordances so much, and so much counting, you ought to be able to handle this sixty-three shoeboxes of cards for the Dryden concordance which Guy Montgomery left when he died.’” That project, Miles recalls, “led me into years of studying computers, and I did make a computer concordance”. To transform the index-card concordance into a computer concordance, Miles applied for funding from the Faculty Research Committee, which she used to contract with the Electrical Engineering Department to use their IBM tabulation machine and their punch-card staff to create a new, corrected set of cards for tabulation. University President Robert Sproul granted funds for the photolithographic reproduction of the results for publication.
[Author: Rachel Sagner Buurma; From essay:"Search and Replace Josephine Miles and the Origins of Distant Reading "] - Writing to Montgomerys one-time graduate student collaborator Lester A. Hubbard in 1957, Miles described the “experimental” and collaborative nature of the project: “[t]he Concordance underwent so many metamorphoses . . . that at least seven names would have had to be listed as editors”. In her preface to the published volume, Miles thanked faculty from the English, French, and Speech Departments, but she especially singled out the female staff members from the computer lab who “worked under the guidance of Mr. Gordon Morrison and Mr. Boyd Judd at the Computer Laboratory,” including Shirley Rice, Odette Carothers, and Penny Gee, who had punched each card in the Concordance with a single word, a symbol corresponding to the title of the poem in which it appeared, and a line number. Later, Miles remembered Gee as very smart and good” and—most importantly—a true collaborator, as opposed to those “IBM people from San Jose” who would arrive periodically to flatly ask, “What can we do to help you?” “Ive never been able to connect with them,” Miles explains,  though I did with Penny Gee. She really taught me.
[Author: Rachel Sagner Buurma; From essay:"Search and Replace Josephine Miles and the Origins of Distant Reading "] - For the concordances publication, Miles elected to give editorial credit to the two graduate students, Mary Jackman and Helen S. Agoa, who had worked intensely on the concordance in its final stages, declining to place her own name on the cover; her name appears instead as the author of the Concordances Preface. Later, Hubbard would engage a lawyer to demand that he be acknowledged as co-author on the basis of the work he had done to generate the initial set of index cards with Guy Montgomery. Thin slips with Hubbards name on them were distributed to be pasted into the already-published volumes.
[Author: Rachel Sagner Buurma; From essay:"Search and Replace Josephine Miles and the Origins of Distant Reading "] - By the time the Dryden concordance was published in 1957, Miles had been using her own handmade word counts to analyze poetic language for almost twenty years. As a graduate student in the late 1930s at Berkeley, Miless dissertation examined Wordsworths language— specifically, whether he expressed emotions literally or through metaphor. “I developed a method for doing this,” Miles later explained, “which involved counting, because I wanted to show actual proportions, that he did very little else but just state literally. Wordsworth and the Vocabulary of Emotion appeared in print in 1942, and was followed by Pathetic Fallacy in the Nineteenth Century: A Study of a Changing Relation Between Object and Emotion (1942), and Major Adjectives in English Poetry From Wyatt to Auden (1946).
[Author: Rachel Sagner Buurma; From essay:"Search and Replace Josephine Miles and the Origins of Distant Reading "] - Miles employed graduate students at the rate of $25 for 20 hours of work to help her to tabulate the adjectives, nouns, and verbs of past poetry and their place within the syntax of the poem; at times students like Miss Jean Warren” suggested modifications to “methods of analysis.” In the following decade, The Continuity of Poetic Language (1951), Eras and Modes in English Poetry (1957), and Renaissance, Eighteenth-Century, and Modern Language in English Poetry: A Tabular View (1960) appeared.
[Author: Rachel Sagner Buurma; From essay:"Search and Replace Josephine Miles and the Origins of Distant Reading "] - The star of large-scale literary data-gathering rises and falls in different eras, and Miles worked at a time in which the making of data sets was regarded as feminized and mechanical. Accordingly, she experienced difficulties convincing publishers to reproduce her data, and she had trouble finding a reception among literary critics who viewed her datasets as merely preparatory to the true work of evaluation. As Miles described it, her projects fell between the “two stools” of linguistic research and literary study. She didnt identify with the linguists, for linguistic analysis was “too alien to the text.” Meanwhile, literary critics wished to absorb the “interesting” claims of her books while discarding “all the tables and charts that she throws in there.
[Author: Rachel Sagner Buurma; From essay:"Search and Replace Josephine Miles and the Origins of Distant Reading "] - Miless intent was to bring criticism and data together; she viewed her tabulations as correctives to the critics who turn discussions of what poetic language is to prescriptions for what it ought to be, adding that “not only theorists like Richards but also historians like Bateson do more evaluating than tabulating. Christopher Rovee describes her as a rogue formalist of the early new-critical period,” and indeed, she was very much in conversation with the poet-critics and the formalist theorists of mid-century. Miless tabulations of the grammatical tendencies of past poets revealed not only the forms of past poetry, but showed the degree to which poetic values of the mid-twentieth-century overdetermined critics sense of the past, turning poets like Blake or Donne or Wordsworth or Yeats into “mirrors” of present poetry.
[Author: Rachel Sagner Buurma; From essay:"Search and Replace Josephine Miles and the Origins of Distant Reading "] - We may think of Blake, for example, as “active, rebellious, and eccentric,” but Miless work showed how he “as wholeheartedly as any accepted the material of his kind and time, using language in which “nouns and adjectives . . . strongly dominated verbs,” sentences that were “phrasally compounded,” and rhymes that stressed not limits, periods bound and conclusions but rather interior units and correspondances, in echo and onomatopoeia”.
[Author: Rachel Sagner Buurma; From essay:"Search and Replace Josephine Miles and the Origins of Distant Reading "] - In the case of Donne, Miles described how “modern critics like Cleanth Brooks and I.A. Richards, who “emphasize the poem as aesthetic object,” necessarily ignore or distort the “language of conceptual reference and argument” which the tabular view reveals to be dominant in Donnes poetry. Likewise, Miles shows mid-centurys Wordsworth to be almost a complete fabrication, noting that modern critics “ignore the mass of Wordsworths work and cherry pick unrepresentative poems like A Slumber Did My Spirit Seal” with its “rocks, and stones, and trees that represent “our own great concreteness” rather than Wordsworths more typical style of subtle generalization. Yeats, too, is praised in the terms of the present, yet Miless work revealed that the language of his later volumes—Michael Robartes and the Dancer (1921) The Tower (1928), and The Winding Stair (1933)—became less predicative, bending back instead to the balanced and classical mode of his earliest work.
[Author: Rachel Sagner Buurma; From essay:"Search and Replace Josephine Miles and the Origins of Distant Reading "] - Miless tabular views not only rescued past poets held hostage by present-day poetic values, they revealed entirely new genealogies linking past to present. Nearly all of Miless scholarly essays contain illuminating and original asides about modern poets. She notes, for example, how the “Donne tradition” lives on in the Cavalier lyricism of Cummings and Millay” as much as the metaphysical meditation of Frost and Auden”. She describes how the dominance of the “family relations of father, mother, son” to be found in the old ballads reappears in “such poets as Auden or Lowell,” while Ezra Pound and Robert Penn Warren and Federico Garcia Lorca develop upon “Coleridges ballads of night and strangeness”. She sees T. S. Eliot as attempting to “strike a balance” between Miltonic phrasal poetry (qualitative, coordinate) and Donnic predicative poetry (clausal, conceptual, full of logical subordination).
[Author: Rachel Sagner Buurma; From essay:"Search and Replace Josephine Miles and the Origins of Distant Reading "] - Against her eras critical truisms—its emphasis on the image, its separation of poetry from prose, its figuration of the poem as object-Miles carved out an alternative view of modern poetrys challenges and strengths. In her view—one that looked at poems as sentences, and traced the pendulum swings of each century from verbs to adjectives and back again—poets of the 1950s faced the same challenges as “Pope or Thomson” did in the 18th century: they had at their disposal a stifling amount of device to deal with a stfling amount of objects and sensations.” Modern poetry, contended Miles, needs “a Wordsworth of its own, to be the generalizer and steadfast interpreter of its own terms”.
[Author: Rachel Sagner Buurma; From essay:"Search and Replace Josephine Miles and the Origins of Distant Reading "] - Miless own poetry reflected her aspirations to pull back enough from poetic matter to become a “generalizer and steadfast interpreter of . . . terms” but not so much as to become “alien to the text.” Reviewing several volumes of Miless poetry in a 1959 article titled Distance and Surfaces: The Poetry of Josephine Miles, Robert Beloof described Miless achievement in terms of her point of view. She “sees her world from one of the most difficult of artistic points of view . . . the middle distance,” Beloof writes. From this vantage, “she is never aware of it as a great metaphysical scheme on the one hand, nor, on the other, as a stream of detail which is itself the the ultimate reality.” The middle-distance viewer is close enough to see the figures, and their principal attitudes, but not to feel an indissoluble unity with them. Nor is he far enough away that his impression is totally of the over-reaching structure. There is no ease. He is drawn in both directions, to the individuals of the community, and to the great abstractions which order them.”
[Author: Rachel Sagner Buurma; From essay:"Search and Replace Josephine Miles and the Origins of Distant Reading "] - Miles, drawn in two directions, made a third—and deserves to have pride of place in our methodological origin stories.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - George Orwells Big Brother and Michel Foucaults ‘panopticon have dominated discussion of contemporary developments in surveillance. While such metaphors draw our attention to important attributes of surveillance, they also miss some recent dynamics in its operation. The work of Gilles Deleuze and Félix Guattari is used to analyse the convergence of once discrete surveillance systems. The resultant ‘surveillant assemblage operates by abstracting human bodies from their territorial settings, and separating them into a series of discrete flows. These flows are then reassembled in different locations as discrete and virtual data doubles. The surveillant assemblage transforms the purposes of surveillance and the hierarchies of surveillance, as well as the institution of privacy.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - One of the most recognizable figures in cultural theory is the flâneur as analysed by Walter Benjamin. A creature of nineteenth-century Paris, the flâneur absorbs himself in strolling through the metropolis where he is engaged in a form of urban detective work. Concealed in the invisibility of the crowd, he follows his fancies to investigate the streets and arcades, carving out meaning from the urban landscape. Possessing a sovereignty based in anonymity and observation, the flâneur characterizes the urban environment and the experience of modernity.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - There has been an exponential multiplication of visibility on our city streets. Where the flâneur was involved in an individualistic scrutiny of the citys significations, the population itself is now increasingly transformed into signifiers for a multitude of organized surveillance systems. Benjamin recognized the importance of even the earliest prototypes of such technologies, observing how the development of photography helped undermine the anonymity which was central to the flâneur by giving each face a single name and hence a single meaning.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - It is desire which secures these flows and gives them their permanence as an assemblage. For psychoanalysts, desire is typically approached as a form of lack, as a yearning that we strive to satisfy. In contrast, Deleuze and Guattari approach desire as an active, positive force that exists only in determinate systems. Desire is a field of immanence, and is a force without which no social system could ever come into being. As such, desire is the inner will of all processes and events; what Nietzsche refers to as the will to power. As we demonstrate below, a range of desires now energize and serve to coalesce the surveillant assemblage, including the desires for control, governance, security, profit and entertainment.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - Surveillance has become a salient topic for theoretical reflection, and this interest coincides with the quantitative increase in surveillance in western societies. However, this paper does not propose to provide a comprehensive overview of these systems of observation. A number of other authors have documented developments in this rapidly changing area. Instead, we view surveillance as one of the main institutional components of late modernity. Our aim is to reconsider some of the more familiar theoretical preoccupations about this topic. We do so by drawing from the works of Gilles Deleuze and Félix Guattari to suggest that we are witnessing a convergence of what were once discrete surveillance systems to the point that we can now speak of an emerging ‘surveillant assemblage. This assemblage operates by abstracting human bodies from their territorial settings and separating them into a series of discrete flows. These flows are then reassembled into distinct data doubles which can be scrutinized and targeted for intervention. In the process, we are witnessing a rhizomatic leveling of the hierarchy of surveillance, such that groups which were previously exempt from routine surveillance are now increasingly being monitored.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - Writing well in advance of the contemporary intensification of surveillance technologies, Orwell presented a prescient vision. In his futuristic nation of Oceana, citizens are monitored in their homes by a telescreen, a device which both projects images and records behaviour in its field of vision. The thought police co-ordinate this extensive monitoring effort, operating as agents of a centralized totalitarian state which uses surveillance primarily as a means to maintain social order and conformity. Not all citizens, however, are singled out for such scrutiny. The upper and middle classes are intensely monitored, while the vast majority of the population, the underclass proles, are simply left to their own devices.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - The fact that we continue to hear frequent cautions about 1984 or Big Brother speaks to the continued salience of Orwells cautionary tale. In the intervening decades, however, the abilities of surveillance technologies have surpassed even his dystopic vision. Writing at the cusp of the development of computing machines, he could not have envisioned the remarkable marriage of computers and optics which we see today. Furthermore, his emphasis on the state as the agent of surveillance now appears too restricted in a society where both state and non-state institutions are involved in massive efforts to monitor different populations. Finally, Orwells prediction that the proles would largely be exempt from surveillance seems simply wrong in light of the extension and intensification of surveillance across all sectors of society.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - Michel Foucaults analysis of the panopticon provides the other dominant metaphor for understanding contemporary surveillance. In part, Foucault extends Orwells fears, but his analysis also marks a significant departure, as it situates surveillance in the context of a distinctive theory of power. The panopticon was a proposed prison design by eighteenth-century reformer Jeremy Bentham. What distinguished this structure was an architecture designed to maximize the visibility of inmates who were to be isolated in individual cells such that they were unaware moment-to-moment whether they were being observed by guards in a central tower. More than a simple device for observation, the panopticon worked in conjunction with explicitly articulated behavioural norms as established by the emerging social sciences, in efforts to transform the prisoners relation to him or her self. This disciplinary aspect of panoptic observation involves a productive soul training which encourages inmates to reflect upon the minutia of their own behaviour in subtle and ongoing efforts to transform their selves. Foucault proposed that the panopticon served as a diagram for a new model of power which extended beyond the prison to take hold in the other disciplinary institutions characteristic of this era, such as the factory, hospital, military, and school.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - Foucaults analysis improves on Orwells by reminding us of the degree to which the proles have long been the subject of intense scrutiny. In fact, Foucault accentuates how it was precisely this population – which was seen to lack the self-discipline required by the emerging factory system that was singled out for a disproportionate level of disciplinary surveillance. Foucault also encourages us to acknowledge the role surveillance can play beyond mere repression; how it can contribute to the productive development of modern selves. Unfortunately, Foucault fails to directly engage contemporary developments in surveillance technology, focusing instead on transformations to eighteenth and nineteenth century total institutions. This is a curious silence, as it is these technologies which give his analysis particular currency among contemporary commentators on surveillance. Even authors predisposed to embrace many of Foucaults insights believe that rapid technological developments, particularly the rise of computerized databases, require us to rethink the panoptic metaphor. For example, Mark Poster believes that we must now speak of a superpanopticon while Diana Gordon suggests the term electronic panopticon better captures the nature of the contemporary situation. But even these authors are in line with a general tendency in the literature to offer more and more examples of total or creeping surveillance, while providing little that is theoretically novel. For our purposes, rather than try and stretch Foucaults or Orwells concepts beyond recognition so that they might better fit current developments, we draw from a different set of analytical tools to explore aspects of contemporary surveillance.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - The philosopher Gilles Deleuze only occasionally wrote directly on the topic of surveillance, usually in the context of his commentaries on Foucaults work. In conjunction with his colleague Félix Guattari, however, he has provided us with a set of conceptual tools that allow us to re-think the operation of the emergent surveillance system, a system we call the surveillant assemblage.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - While Deleuze and Guattari were prolific inventors of concepts, we embrace only a few of their ideas. Undoubtedly, this means that we are not fully representing their thought. However, our approach is entirely in keeping with their philosophy which animates one to ‘think otherwise: to approach theory not as something to genuflect before, but as a tool kit from which to draw selectively in light of the analytical task at hand.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - The remainder of this paper documents attributes of the surveillant assemblage. Some caution is needed, however, at this point. To speak of the surveillant assemblage risks fostering the impression that we are concerned with a stable entity with its own fixed boundaries. In contrast, to the extent that the surveillant assemblage exists, it does so as a potentiality, one that resides at the intersections of various media that can be connected for diverse purposes. Such linkages can themselves be differentiated according to the degree to which they are ad hoc or institutionalized. By accentuating the emergent and unstable characteristic of the surveillant assemblage we also draw attention to the limitations of traditional political strategies that seek to confront the quantitative increase in surveillance. As it is multiple, unstable and lacks discernible boundaries or responsible governmental departments, the surveillant assemblage cannot be dismantled by prohibiting a particularly unpalatable technology. Nor can it be attacked by focusing criticism on a single bureaucracy or institution. In the face of multiple connections across myriad technologies and practices, struggles against particular manifestations of surveillance, as important as they might be, are akin to efforts to keep the oceans tide back with a broom – a frantic focus on a particular unpalatable technology or practice while the general tide of surveillance washes over us all.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - Perhaps we risk having something still more monumental swept away in the tide. Recall Foucaults controversial (and frequently misunderstood) musings at the end of The Order of Things. In this conclusion to his archaeology of how the understanding of Man has been transformed in different epochs as humanity came into contact with different forces, Foucault suggests that
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - If those arrangements were to disappear as they appeared, if some event of which we can at the moment do no more than sense the possibility ... were to cause them to crumble, as the ground of classical thought did, at the end of the eighteenth century, then one can certainly wager that man would be erased, like a face drawn in sand at the edge of the sea.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - Among the proliferation of late-modern forces which are candidates for contributing to such a radical transformation we can include the intensification of technologized forms of observation.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - The analysis of surveillance tends to focus on the capabilities of a number of discrete technologies or social practices. Analysts typically highlight the proliferation of such phenomena and emphasize how they cumulatively pose a threat to civil liberties. We are only now beginning to appreciate that surveillance is driven by the desire to bring systems together, to combine practices and technologies and integrate them into a larger whole. It is this tendency which allows us to speak of surveillance as an assemblage, with such combinations providing for exponential increases in the degree of surveillance capacity. Rather than exemplifying Orwells totalitarian state-centred Oceana, this assemblage operates across both state and extra-state institutions.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - Something as apparently discrete as the electronic monitoring of offenders increasingly integrates a host of different surveillance capabilities to the point that
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - no one is quite sure any longer what [Electronic Monitoring] is. Voice, radio, programmed contact, remote alcohol testing, and automated reporting station (‘kiosk) technologies proliferate and are used both singly and in a dizzying array of combinations.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - The police are continually looking for ways to integrate their different computer systems and databases, as exemplified by ongoing efforts by the FBI forensics section to link together databases for fingerprints, ballistics and DNA. Still another example of such combinations is the regional police computer system in Central Scotland
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - Phone conversations, reports, tip-offs, hunches, consumer and social security databases, crime data, phone bugging, audio, video and pictures, and data communications are inputted into a seamless GIS [geographic information system], allowing a relational simulation of the time-space choreography of the area to be used in investigation and monitoring by the whole force. The Chief Constable states: ‘what do we class as intelligence in my new system in the force? Everything! The whole vast range of information that comes into the possession of a police force during a twenty four hour period will go on to my corporate database. Everything that every person and vehicle is associated with.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - In situations where it is not yet practicable to technologically link surveillance systems, human contact can serve to align and coalesce discrete systems. For example, various ‘multi-agency approaches to policing are institutionalized. Originally, such efforts were wedded to a welfarist ideology of service delivery, but in recent years social service agencies have been drawn into the harder edge of social control. The coming together (face-to-face, or through electronic mediation) of social workers, health professionals, police and educators to contemplate the status of an at risk individual combines the cumulative knowledge derived from the risk profiling surveillance systems particular to each of these institutions.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - A great deal of surveillance is directed toward the human body. The observed body is of a distinctively hybrid composition. First it is broken down by being abstracted from its territorial setting. It is then reassembled in different settings through a series of data flows. The result is a decorporealized body, a data double of pure virtuality.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - The monitored body is increasingly a cyborg; a flesh-technology-information amalgam. Surveillance now involves an interface of technology and corporeality and is comprised of those surfaces of contact or interfaces between organic and non-organic orders, between life forms and webs of information, or between organs/body parts and entry/projection systems (e.g., keyboards, screens). These hybrids can involve something as direct as tagging the human body so that its movements through space can be recorded, to the more refined reconstruction of a persons habits, preferences, and lifestyle from the trails of information which have become the detritus of contemporary life. The surveillant assemblage is a visualizing device that brings into the visual register a host of heretofore opaque flows of auditory, scent, chemical, visual, ultraviolet and informational stimuli. Much of the visualization pertains to the human body, and exists beyond our normal range of perception.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - a fully supervised and tamper-resistant protection system that automatically activates once secured around an infants ankle or wrist. Staff [are] immediately alerted at a computer console of the newly activated tag, and can enter pertinent information such as names and medical conditions. Password authorization is needed to move infants out of the designated protection area and – if an infant is not readmitted within a predetermined time limit an alarm will sound. An alarm also sounds if an infant with a Hugs tag is brought near an open door at the perimeter of the protected area without a password being entered. The display console will then show the identification of the infant and the exit door on a facility map. Alternatively, doors may also be fitted with magnetic locks that are automatically activated. As well, Hugs can be configured to monitor the progress and direction of the abduction within the hospital. Weighing just 1/3 of an ounce, each ergonomically designed infant tag offers a number of other innovative features, including low-battery warning, the ability to easily interface with other devices such as CCTV cameras and paging systems and time and date stamping.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - Professor Kevin Warwick of Reading University is the self-proclaimed first cyborg, having implanted a silicon chip transponder in his forearm. The surveillance potential of this technology has been rapidly embraced to monitor pets. A microchip in a pets skin can be read with an electronic device which connects a unique identifying number on the microchip to details of the pets history, ownership and medical record. Warwick has proposed that implanted microchips could be used to scrutinize the movement of employees, and to monitor money transfers, medical records and passport details. He also suggests that
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - anyone who wanted access to a gun could do so only if they had one of these implants . . . Then if they actually try and enter a school or building that doesnt want them in there, the school computer would sound alarms and warn people inside or even prevent them having access.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - These examples indicate that the surveillant assemblage relies on machines to make and record discrete observations. As such, it can be contrasted with the early forms of disciplinary panopticism analysed by Foucault, which were largely accomplished by practitioners of the emergent social sciences in the eighteenth and nineteenth centuries. On a machine/human continuum, surveillance at that time leaned more toward human observation. Today, surveillance is more in keeping with the technological future hinted at by Orwell, but augmented by technologies he could not have even had nightmares about.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - The surveillant assemblage does not approach the body in the first instance as a single entity to be molded, punished, or controlled. First it must be known, and to do so it is broken down into a series of discrete signifying flows. Surveillance commences with the creation of a space of comparison and the introduction of breaks in the flows that emanate from, or circulate within, the human body. For example, drug testing striates flows of chemicals, photography captures flows of reflected lightwaves, and lie detectors align and compare assorted flows of respiration, pulse and electricity. The body is itself, then, an assemblage comprised of myriad component parts and processes which are broken-down for purposes of observation. Patton suggests that the concept of assemblage may be regarded as no more than an abstract conception of bodies of all kinds, one which does not discriminate between animate and inanimate bodies, individual or collective bodies, biological or social bodies.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - It has become a commonplace among cultural theorists to acknowledge the increasing fragmentation of the human body. Such an appreciation is evidenced in Groszs schematic suggestion that we need to think about the relationship between cities and bodies as
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - collections of parts, capable of crossing the thresholds between substances to form linkages, machines, provisional and often temporary sub- or micro-groupings ... their interrelations involve a fundamentally disunified series of systems, a series of disparate flows, energies, events, or entities, bringing together or drawing apart their more or less temporary alignments.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - Likewise, the surveillant assemblage standardizes the capture of flesh/information flows of the human body. It is not so much immediately concerned with the direct physical relocation of the human body (although this may be an ultimate consequence), but with transforming the body into pure information, such that it can be rendered more mobile and comparable.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - Such processes are put into operation from a host of scattered centres of calculation where ruptures are co-ordinated and toward which the subsequent information is directed. Such centres of calculation can include forensic laboratories, statistical institutions, police stations, financial institutions, and corporate and military headquarters. In these sites the information derived from flows of the surveillant assemblage are reassembled and scrutinized in the hope of developing strategies of governance, commerce and control.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - In the figure of a body assembled from the parts of different corpses, Mary Shellys Frankenstein spoke to early-modern anxieties about the potential consequences of unrestrained science and technology. Contemporary fears about the implications of mass public surveillance continue to emphasize the dark side of science. Today, however, we are witnessing the formation and coalescence of a new type of body, a form of becoming which transcends human corporeality and reduces flesh to pure information. Culled from the tentacles of the surveillant assemblage, this new body is our data double, a double which involves ‘the multiplication of the individual, the constitution of an additional self. Data doubles circulate in a host of different centres of calculation and serve as markers for access to resources, services and power in ways which are often unknown to its referent. They are also increasingly the objects toward which governmental and marketing practices are directed.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - And while such doubles ostensibly refer back to particular individuals, they transcend a purely representational idiom. Rather than being accurate or inaccurate portrayals of real individuals, they are a form of pragmatics: differentiated according to how useful they are in allowing institutions to make discriminations among populations. Hence, while the surveillant assemblage is directed toward a particular cyborg flesh/technology amalgamation, it is productive of a new type of individual, one comprised of pure information.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - Rhizomes grow across a series of interconnected roots which throw up shoots in different locations. They grow like weeds precisely because this is often what they are. A rhizome may be broken, shattered at a given spot, but it will start up again on one of its old lines, or on new lines. Surveillance has comparable expansive and regenerative qualities. It is now estimated that there are 500,000 surveillance cameras operating in Britain, where a city dweller can now expect to be caught on film every five minutes. Paul Virilio argues that this growth in observation has transformed the experience of entering the city: ‘Where once one necessarily entered the city by means of a physical gateway, now one passes through an audiovisual protocol in which the methods of audience and surveillance have transformed even the forms of public greeting and daily reception. Resounding echoes of his point can be heard in the effusive boastings of an operations director for a British surveillance firm who recounts how The minute you arrive in England, from the ferry port to the train station to the city centres, youre being CCTVd. The study by Norris and Armstrong of British CCTV also demonstrates how this ostensibly unitary technology is in fact an assemblage that aligns computers, cameras, people and telecommunications in order to survey the public streets.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - Deleuze and Guattari emphasize how ‘the rhizome operates by variation, expansion, conquest, capture, offshoots. No single technological development has ushered in the contemporary era of surveillance. Rather, its expansion has been aided by subtle variations and intensifications in technological capabilities, and connections with other monitoring and computing devices. Some of the rhizomatic offshoots of the surveillant assemblage derive from efforts to seek out new target populations that ostensibly require a greater degree of monitoring. The list of such populations is limited only by imagination, and currently includes, for example, the young, caregivers, commuters, employees, the elderly, international travelers, parolees, the privileged and the infirm. Much of this expansion is driven by the financial imperative to find new markets for surveillance technologies which were originally designed for military purposes.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - For Orwell, surveillance was a means to maintain a form of hierarchical social control. Foucault proposed that panoptic surveillance targeted the soul, disciplining the masses into a form of self-monitoring that was in harmony with the requirements of the developing factory system. However, Bauman argues that panopticism in contemporary society has been reduced in importance as a mechanism of social integration. Instead of being subject to disciplinary surveillance or simple repression, the population is increasingly constituted as consumers and seduced into the market economy. While surveillance is used to construct and monitor consumption patterns, such efforts usually lack the normalized soul training which is so characteristic of panopticism. Instead, monitoring for market consumption is more concerned with attempts to limit access to places and information, or to allow for the production of consumer profiles through the ex post facto reconstructions of a persons behaviour, habits and actions. In those situations where individuals monitor their behaviour in light of the thresholds established by such surveillance systems, they are often involved in efforts to maintain or augment various social perks such as preferential credit ratings, computer services, or rapid movement through customs.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - Foucaults larger body of work displays an appreciation for the multiple uses and targets of surveillance. Most discussions of surveillance fixate on his analysis of the panopticon, with its individualized disciplinary form of bodily scrutiny. However, Foucault also analysed aggregate forms of surveillance. Institutions are involved in the production and distribution of knowledge about diverse populations for the purpose of managing their behaviour from a distance. In this way, surveillance also serves as a vital component of positive population management strategies.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - The concept of surplus value has traditionally been associated with Marxism. For Marx, it designated how the owners of the means of production profit from workers excess labour power for which they are not financially compensated. Surveillance plays an important role in this process, as it allows managers to establish and monitor production norms at previously unheard of levels. Today, however, surplus value has escaped from a purely labour-oriented discourse and can now also be located in the language of cybernetics. Increasingly important to modern capitalism is the value that is culled from a range of different transaction and interaction points between individuals and institutions. Each of these transactions is monitored and recorded, producing a surplus of information. The monetary value of this surplus derives from how it can be used to construct data doubles which are then used to create consumer profiles, refine service delivery and target specific markets. There is a growing trade in the corporate sale of such information. Governments are also keen to profit from the sale of information stored in scattered official databases. Millions of dollars are already being made through the sale of data from license bureaus, personal income data and employment records. In a cybernetic world, surplus value increasingly refers to the profit that can be derived from the surplus information that different populations trail behind them in their daily lives.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - The public is slowly awakening to the profits that are being made from the sale of their data doubles. One consequence of this recognition has been the further commodification of the self. Parallel to how the emergence of the wage economy necessitated the fixing of monetary prices to labour power, citizens and economists are now contemplating what, if any, compensation individuals should receive for the sale of their personal information. Dennis reports on a recent study which found that 70 per cent of Britons were happy to have companies use their personal data, on the condition that they receive something in return, such as more personal service or rewards. Privacy is now less a line in the sand beyond which transgression is not permitted, than a shifting space of negotiation where privacy is traded for products, better services or special deals.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - In addition to a desire for order, control, discipline and profit, surveillance has voyeuristic entertainment value. Clips from CCTVs are now a staple of daytime talk shows while programmes such as Americas Dumbest Criminals have helped soften the authoritarian overtones of mass public surveillance. The proliferation of hand-held video cameras has also given rise to Americas Funniest Home Videos, as well as the more morbid Faces of Death videos which portray a procession of accidental fatalities which have been captured on film.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - As the surveillant assemblage transcends institutional boundaries, systems intended to serve one purpose find other uses. In his early analysis of paper-based records, Stanton Wheeler pointed out that it is a characteristic of such records that they can be combined to serve new purposes. The computerization of record-keeping has greatly expanded this ability. For example, police organizations have secured routine, and often informal, access to a host of non-police databases, such as those from insurance companies and financial institutions. Research by Northrop, Kramer and King indicates that the police have become the primary users of many systems originally established for other governmental purposes, and Gordon reports on proposals to link the US federal NCIC police database to computers from Social Security, Internal Revenue, Passport, Securities and Exchange and the State Department. Davis recounts how in some Southern California communities the police now have direct computerized access to school records.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - In surveying the informational horizon for ever more potentially useful sources, police organizations have recently recognized the surveillance and investigative potential of corporate databases. Files from telephone and utilities companies can be used to document an individuals lifestyle and physical location, and marketing firms have developed consumer profiling techniques that contain precise information on a persons age, gender, political inclinations, religious preferences, reading habits, ethnicity, family size, income, and so on. When these sources are combined through computerized data matching, they allow for exponential increases in the amount of information the police have at their disposal. Burnham relates that the FBI has employed commercial databases for undisclosed investigative purposes, and that the US Drug Enforcement Agency has developed its own in-house registry with information culled from mailing and telephone listings, direct marketers, voters records, and assorted commercial sources. Although cloaked in secrecy, this registry was expected to contain 135,000,000 records as of its inception in 1991 and would subsequently receive regular updates of corporate and residential data.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - Ostensibly non-criminal justice institutions are being called upon to augment the surveillance capacities of the criminal justice surveillance system. In Canada, for example, in an effort to deter money laundering, financial institutions are compelled to monitor and report suspicious transactions. More recently, regulations have been introduced to require American banks to compare the financial holdings of their clients against an electronic list of parents who owe child support. Educators and medical practitioners are already legally compelled to report suspected instances of child abuse, and the police have started to request or confiscate media tapes of public disturbances in efforts to identify lawbreakers.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - For both Orwell and Foucault, surveillance is part of a regime where comparatively few powerful individuals or groups watch the many, in a form of top-down scrutiny. Contemporary studies of surveillance continue to emphasize this hierarchical aspect of observation. For example, Fiske concludes his insightful analysis of the surveillance of American Blacks (particularly Black men), by proclaiming that although surveillance is penetrating deeply throughout our society, its penetration is differential. The lives of the white mainstream are still comparatively untouched by it. And while the targeting of surveillance is indeed differential, we take exception to the idea that the mainstream is untouched by surveillance. Surveillance has become rhizomatic, it has transformed hierarchies of observation, and allows for the scrutiny of the powerful by both institutions and the general population.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - All contemporary institutions subject their members to forms of bureaucratic surveillance. Individuals with different financial practices, education and lifestyle will come into contact with different institutions and hence be subject to unique combinations of surveillance. The classifications and profiles that are entered into these disparate systems correspond with, and reinforce, differential levels of access, treatment and mobility. Hence, while poor individuals may be in regular contact with the surveillance systems associated with social assistance or criminal justice, the middle and upper classes are increasingly subject to their own forms of routine observation, documentation and analysis. The more institutions they are in contact with, the greater the level of scrutiny to which they are subjected. In the case of the powerful, this can include the regular monitoring of consumption habits, health profile, occupational performance, financial transactions, communication patterns, Internet use, credit history, transportation patterns, and physical access controls.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - It is not exclusively powerful social groups and institutions which observe the powerful. Mathiesen accentuates the tendency toward bottom-up forms of observation in his claim that a process of synopticism is now at work which parallels Foucaults panopticism. Synopticism essentially means that a large number of individuals are able to focus on something in common. New media, particularly television, allow the general public to scrutinize their leaders as never before. We need only consider the media circus which surrounds Britains royal family to acknowledge this point. Furthermore, the monitoring of the powerful has been eased by the proliferation of relatively inexpensive video cameras. These allow the general public to tape instances of police brutality, and have given rise to inner-city citizen response teams which monitor police radios and arrive at the scene camera-in-hand to record police behaviour. Such monitoring culminates in those surreal situations of labour unrest where picketing workers film the police while the police film the strikers. While not a complete democratic leveling of the hierarchy of surveillance, these developments cumulatively highlight a fractured rhizomatic criss-crossing of the gaze such that no major population groups stand irrefutably above or outside of the surveillant assemblage.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - A further distinction is needed, however, if we are to fully appreciate the distinctive form that the observation of the powerful now assumes. Such surveillance is often a mile wide but only an inch deep. The depth, or intensity, of the surveillance directed at the powerful generally exists as a potentiality of connections of different technologies and institutions. It is activated, or intensified, when there is some perceived ex post facto or prospective need to profile their movements, consumption patterns, reading preferences, tastes in erotica, personal contacts, such that they coalesce into a remarkably detailed data double. The O. J. Simpson case provides a telling example of the intensity that this potentiality can assume when put into motion. Included among the reams of information that the L. A.P. D. were able to collect about O. J. Simpson were details about which pornographic movie he watched in his hotel a few days prior to the murders. The police also approached a private company which sells satellite surveillance photographs to try and discern whether Simpsons now (in) famous white Bronco was in the driveway of Nicole Brown Simpsons home on the night of the murders.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - Premodern living arrangements typically consisted of individuals residing in rural villages where they knew and were known by their neighbours. The mass movements of individuals into cities ruptured these long-standing neighbourly and familial bonds. Individuals in cities became surrounded by streams of unknown strangers. Sociologists have drawn a wide range of implications from this social transformation. Anonymity allowed for new possibilities in self-creation: the freedom to partake in experiments with identities and life projects. Simmel believed that the metropolis grants to the individual a kind and an amount of personal freedom which has no analogy whatsoever under other conditions. Others have accentuated the darker side of these possibilities for self-creation, cautioning how this new found freedom could also be experienced as a daunting obligation, as modern individuals are now compelled to be free, to establish identities and life projects in the face of radical uncertainty about correct courses of action. Bauman observes that modernity transformed identity from the matter of ascription into the achievement [sic] – thus making it an individual task and the individuals responsibility, and these individual life-projects find no stable ground in which to lodge an anchor.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - From the beginning, however, this general narrative of anonymity and invisibility contained a subplot, one which involved countervailing efforts by institutions. The rise in credentials and surveillance systems was a way to create institutional reputations and provide for ways to differentiate among unknown strangers. These new forms of reputation lack the deep subjective nuances which characterized familial and neighbourly relations in the idealized premodern rural village. Instead, knowledge of the population is now manifest in discrete bits of information which break the individual down into flows for purposes of management, profit and entertainment. While such efforts were originally a footnote to the historical rise of urban anonymity, they now constitute an important force in their own right.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - The coalescence of such practices into the surveillant assemblage marks the progressive disappearance of disappearance – a process whereby it is increasingly difficult for individuals to maintain their anonymity, or to escape the monitoring of social institutions. Efforts to evade the gaze of different systems involves an attendant trade-off in social rights and benefits. Privacy advocates bring this point home in their facetious advice that individuals who are intent on staying anonymous should not use credit, work, vote, or use the Internet. Two quite different historical examples accentuate the extent to which the possibilities for disappearance have narrowed.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - A recent biography of a female activist recounted how she was followed in the 1950s by secret service agents. Unbeknownst to her, at one point she managed to evade her pursuers by simply taking an ocean cruise which rendered her beyond the reach of their abilities to track her movements. Clearly, this would not be the case today. Even on the ocean a persons whereabouts could still be discerned through the monitoring of credit card transactions, computer connections, travel arrangements and telephone calls.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - Our second example also concerns ship travel, but this time it involves the greatest naval armada ever assembled – the allied invasion of Normandy in 1944. At that time the Germans were reasonably certain that an invasion of France was imminent, but it was not until the fog lifted on the morning of June 6th to reveal a fleet of over 5,000 ships off the coast that they knew the invasion had truly begun. Again, the contrast between yesterday and today is telling. With advanced military sensing devices that now include globe-scanning satellites and submarines equipped with sensors that can detect the propeller of a ship traveling on the opposite side of the ocean, the surprise appearance of such a massive military grouping is simply inconceivable.
[Author: Kevin D Haggerty and Richard V Ericson; From essay:"The Surveillant Assemblage "] - The invisible armada and elusive activist have faded into historical memories. From now on, such matters will be readily captured by a surveillant assemblage devoted to the disappearance of disappearance.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - Quantity and quality wilt when long separated. But a danger often present is the establishing of quality not upon the determination, or even upon the disregard, but upon the assumption, of quantity.-JOSEPHINE MILES (1946)
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - This led to a second round of sampling as committee members selected their one hundred works based on a complex and sometimes contradictory set of criteria. The works had to be “representative” (daihyō-teki) in the sense of being by “well-known” (chomei na) authors and having high “artistic value” (bungei-teki kachi); they had to be widely read in their own time or continually read up to the present day; they preferably did not contain heavy use of dialect; and finally, they could not skew toward any one author or time period. This was a more amorphous and, as a result, a more subjective kind of probability sampling carried out over multiple dimensions of value. There is also no way to know how each member chose to emphasize any one of these criteria. Any discrepancies were washed out in the final step, which singled out 139 works selected by four or more members. Chronologically they range from Sanyūtei Enchōs Kaidan botan dōrō (A ghost story: the peony lantern, 1884) to Õe Kenzaburōs “Shisha no ogori” (Lavish are the dead, 1957), with the majority falling between 1901 and 1950.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - Numbers, as we learned in chapter 1, have been dancing around the study of literature since that study was disciplined and institutionalized in the late nineteenth century. That dance has become more intimate and intense, however, with the increasing availability of large collections of digitized, machine-readable texts. Their impact is most directly felt at the level of the keyword search, now a nearly unconscious part of the research process that creates a false sense of the archive (or at least a substantive portion of it) as being a few keystrokes away. Although Sōseki, living through his own information revolution, could imagine all those unread eighteenth-century works lining the shelves of modern libraries, he surely could not have imagined being able to see them all. Likewise, Hatano and his successors were confident that modern Japanese literature could be effectively sampled from literary anthologies, but they turned to sampling precisely because they lacked the means (and hands) to count everything. In this sense, their conceptions of everything” were bound by the print cultures and information societies in which they worked. Today, when private companies such as Google and public institutions such as the Hathi Trust Resource Center or the National Diet Library are busy digitizing millions of books, the idea of “everything” seems bound only by what has not made the journey from ink and paper to strings of zeroes and ones.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - At the end of this journey, in its more utopian versions, awaits Borgess total library, a virtual space where all that has ever been written is hyperlinked and ready to hand.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - Less idealistic accounts, however, recast the dream of a universal library and total access as a nightmare of crushing information overload. The problem is twofold, relating to both material concerns and the scalability of reading. Materially, the illusion of total access quickly dissipates with the reality that digitization produces its own constraints on the constitution of the archive. These constraints include the limited ability to reproduce print in all its historical variety and the institutional and social location of digitization projects, which effect what gets digitized and how it is made accessible to users. From the perspective of reading, the constraints come in the form of the algorithms used to search and extract meaning from digital collections, without which their content remains opaque and lifeless. The short history of Google Books nicely encapsulates the gap between universalistic hubris and the reality of how digitization constrains the archive. First conceived by Google cofounder Larry Page in 1999, the project got underway five years later and by 2006 was already subject to hyperbolic speculation about the potential of “the universal library [to] deepen our grasp of history, as every original document in the course of civilization is scanned and cross-linked. As Siva Vaidhyanathan has shown, however, the aspiration to totality soon fell prey to the realities of copyright law, the limits of the digital for representing the variability of print, imperfect search algorithms, competing metadata standards, and concerns about accessibility, authority, and transparency. There is nothing seamless about the transition of the worlds print heritage into digital form.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - Attending to what is lost and deformed in this transition is important, although not at the expense of considering how the transition itself facilitates new ways of recognizing what the print archive is and was. Access to knowledge has always been partial and imperfectly mediated, buoyed by specific assumptions and ideas about what constitutes the “complete” archive or any relevant portion of it. Whats true of the “canon,” as John Guillory has described it, is also true of the archive. “No one has access to the canon as a totality ... no one ever reads every canonical work; no one can, because the works invoked as canonical change continually according to many different occasions of judgment and contestation. Following Derrida, we must acknowledge that these occasions are further determined by the technical structure of the archiving archive,” which shapes “archivable content even in its very coming into existence and in its relationship to the future. Digital collections, in remediating the archive as imagined totality, are also occasions to critically reflect on past and present technical structures for organizing knowledge and to compare their effects on what gets noticed, what gets valued, and what gets read. This chapter explores the impact of the digital on how we think about what to read, setting the stage for a conversation about how and why to read at the scale of less than everything but more than one person could possibly manage.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - At the center of this exploration is Aozora Bunko (hereafter Aozora), the largest digital collection of Japanese language literature in existence. For the foreseeable future, attempts to reason quantitatively about modern Japanese literature at the scale of more than one hundred texts are likely to begin with this collection. Comprised of nearly sixteen thousand manually transcribed and proofed texts by 2018, it is the most reliable repository for machine-readable and open-access prewar texts. This is not likely to change soon owing to the strictness of copyright protections and the challenges that Chinese characters present to optical character recognition technologies. These challenges are compounded in the case of prewar texts with their varying print qualities and diversity of script styles, which make automatic bulk digitization difficult to do with sufficient accuracy. To the extent Aozora remains the digital collection of first resort, its size inviting illusions of representativeness, it is worth investigating the institutional and social mechanisms that inform its construction and enable a particular vision of “modern Japanese literature. What representation of the archive has been produced from Aozoras unique biography? to use Katherine Bodes phrase. No less important, what kinds of representations of past literary production can be created from its uniquely situated vision?
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - Does this sample better represent “modern Japanese literature than those created by Nakamura or Kabashima? It feels more authoritative because it draws on collective expertise and a larger data set. But the question, as a detour through content analysis shows, is too imprecise. The validity of the sample as a representation of some population gains to the degree to which the process of construction is oriented around a specific research question and grounded in a specific interpretive context. Here, the different axes on which works were asked to be ranked, and the lack of a research question, makes it difficult to assess the samples validity on its own. Some biases have likely been evened out, but new ones have just as likely been produced. Only nine of these representative” works, for instance, or about 6 percent, are by women.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - Both of these questions lead to the problem of sampling, which I address in this chapter as a twofold problem. The first part of the problem is that Aozora functions as a sample of the archive in the way a climate scientist might think of an ice core sample, its variegated layers of particles and dust recording, from a localized context, the effects of complex processes and systems driving global climate conditions over time. It is like a sensor that captures, in a locally biased way, some portion of the dust kicked up by the processes shaping the production and consecration of literary value. Understanding what vision of “modern Japanese literature is preserved in Aozora is thus in part a matter of understanding how its layers of soil and sediment, light and dark, align or diverge with those found in other records of the literary past, which naturally differ with historical and institutional location. The second problem of sampling relates to its use in statistics as a set of methods, as well as theories, for drawing inferences about a population from some subset of that population. When working with digital collections such as Aozora to craft large-scale arguments about literary history, we invariably face this twofold problem of how to build a statistically useful sample from a larger, nonintentional sample whose relation to the archive is opaque because the composition of that archive—that imagined total population—is unknowable.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - The problem has itself generated a productive debate about evidentiary practices in literary scholarship and the extent to which they are thrown into new relief by the scale of digital archives. In this chapter, I contribute to this debate by addressing both aspects of the sampling problem. On the one hand, I try to understand the particularity of Aozora as a sample by using a comparative method that aligns it with several large-scale traces of the bibliographic record: bibliographies of foreign translations into Japanese, the contents of omnibus literary anthologies, and a catalog of literary works included in high school textbooks. By comparing these traces of the never completely knowable archive, we begin to see how Aozora uniquely captures the global processes of literary production and valuation only ever visible in such biased, partial traces. I ground this comparison in the related problem of sampling as method, recognizing it as a set of shared conventions and attendant theories for thinking and evaluating the relation of part to whole. To work with digital collections is to confront the practices that literary critics and historians have relied on, and those we might borrow and adapt, for reasoning across different scales of evidence.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - AOZORA AND THE EVIDENCE GAP
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - Aozoras biography emerges out of the same confluence of forces that drove the creation of Google Books. These forces centered around various private and commercial attempts in the 1990s to make electronic books available on the internet. Two of Aozoras founding members, in fact, met via Voyager Japan, a fledgling e-book software company. Tomita Michio, who became Aozoras leading visionary and spokesperson, was a nonfiction writer increasingly frustrated with a publishing industry in the mid-1990s that saw little commercial value in out-of-print titles and that offered few mechanisms for their continued circulation. At the time, he was trying to publish a revised edition of his history of personal computing from 1985, but also a new work, Future of the Book (Hon no mirai), which was an extended meditation on how digitization would transform the way people read. In his frustration, he turned to Voyager Japan where he met Noguchi Eiji, a software developer who helped him develop e-book versions of his books for the Voyager platform. This platform also allowed users to download books from the internet, prompting both men to think about the possibilities of creating an online, virtual bookshelf for electronic books.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - In 1997 this bookshelf materialized with just seven texts, all of them acquired from a corpus linguist at Fukui University who had digitized them for the internet. They included works by Mori Ōgai, Yosano Akiko, Futabatei Shimei, and Nakajima Atsushi, foreshadowing the role that Aozora would come to play as a repository for canonical literary figures. But the original vision for the site was far more expansive—it was to be an open repository for out-of-copyright and self-published works. For Tomita, the digital medium symbolized a liberating force that would democratize the circulation of ideas by untying them from the constraints of static, physical books. In the preface to Future of the Book, he looked with anticipation to a globally networked world where books would be both ubiquitous and instantaneously accessible, as if residing in the blue sky” (aozora) above. One need merely look up to see their own ideas and those of others etched, as if by chalk, in the clouds.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - Tomitas dream of a library in the sky to which anyone had free access as a consumer or producer of content stands in contrast to the archival ambitions of Google Books, the profit motivations of Amazon Books, or even the noncommercial Project Gutenberg, a crowdsourced digital library that was an inspiring model for Tomita and Noguchi that already hosted one thousand titles by 1996. As Aozora grew, however, this emphasis on production waned and the horizons of Tomitas boundless blue sky narrowed.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - Abductive inference, in contrast, proceeds by matching particulars internal to some set of texts with particulars external to them (a hypothesis or analytical construct that best explains the observations). Logically, neither has to imply the other, and inferences about their relation can only be made with some degree of probability. This probability increases the more an analytical construct takes other conditions or variables into account. When Hatano sought to explain the tendency to write longer sentences by relating it to Tanizakis psychopathology, he was making an abductive inference, albeit one weakly supported by empirical evidence. That is, he was making a generalization based on observing a particular feature across many separate textual instances and inferring their patterned relation to another set of particulars (i.e., the writers state of mind or creative intentions). For Krippendorff, such generalizations in content analysis aim not toward accurate representations of the textual universe but toward construct[ing] a world in which the texts make sense and can answer the analysts research questions,” even if within this world certain questions become answerable and others make no sense. This world, this context, is “the analysts best hypothesis for how the texts came to be, what they mean, what they can tell or do.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - Although the horizons narrowed along the dimension of content, they remained broad with respect to access and ownership. Two years after its founding in 1997, the project had more than one hundred volunteers inputting and proofreading texts to be added to the collection. There was internal debate about how this labor would be acknowledged and rewarded, but there was never any question that out-of-copyright material belonged to no single entity and must forever circulate freely. Ostensibly, this meant any work whose author had been deceased for fifty years and upon which there were no additional copyright claims. Beyond this single restriction, volunteers could digitize whatever they wished. The only selection oversight provided by project coordinators was a list of authors dead for more than fifty years and an inventory of which works had already been claimed by volunteers (to avoid duplicate efforts). With these minimal restrictions in place, the number of works in the collection grew from ten in 1997 to five hundred in 1999, seventeen hundred by 2001, forty-seven hundred by 2005, and to nearly sixteen thousand works by 2018. Lacking direct oversight on what to include, the operative logic of selection is effectively the cumulative contingent choices of hundreds of individuals constrained by the intersecting trajectories of biography (i.e., mortality), copyright law, and the availability of texts. Given this bottom-up, laissez-faire method of selection, what claims on representation of literary history might Aozora have? What is the sample of “modern Japanese literature captured by the wisdom of the crowd and the materials available to them for reproducing textual artifacts? What, in the end, can such a sample tell us?
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - This brings us squarely back to the problem of sampling as method. The notion that a greater body of evidence naturally better represents the whole of some phenomenon is a basic tenet of quantitative approaches to literary and cultural history. It is also one of its most anxiety inducing at the level of disciplinary practice. To argue for the value of such approaches requires reckoning with their insistence on examining many texts for synchronic or diachronic patterns instead of individual works through a richly elaborated set of contextual references. This insistence cuts at the heart of literary criticisms institutional identity, which has for more than a century privileged deep and intensive interpretation of single objects in ways that preserve their aesthetic autonomy (and so too the autonomy of the critic). To create and analyze a sample of texts in order to generalize from them is, some claim, “to violate the individuality of the text or deliberately elide the particulars that make the study of literature critical.” In the space between the idea of sampling and the closely read case study—a dichotomy whose vociferous defense reinforces how entrenched the latter is in literary criticism as a disciplinary formation—emerges what Andrew Piper calls an evidence gap.” This gap calls attention to the ways that we, as interpreters of textual meaning at any scale, establish “the representativeness of our own evidence” for how part relates to whole, text to context. It also exposes the disciplinary practices and epistemological assumptions that inform the defense of one kind of representativeness over others.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - To caricature the debate at its extremes, a primary defense of reading individual texts intensively and closely is that this is the only way to attend to all of their variation and to languages inherent ambiguities. Relating high-dimensional and highly particular objects to others in lower dimensional equivalence spaces erases their individual complexity. It denies to interpretation a chromatic plenitude, as Jonathan Culler calls it: “A playing of all possible notes in all possible registers. Defenders of computational methods, on the other hand, point out that the complexity of the particular case is invariably used to make broader claims about the nature and evolution of literary and historical discourse. It is singular and exemplary all at once. But this exemplariness is often assumed from textual evidence that represents only a fraction of everything said or written in a given period. Given the unprecedented ability to observe so much more of this background with digital methods, or so the argument goes, one should explore more cases to validate existing claims and strengthen support for generalizations made with them. From one side sampling appears to undermine interpretation itself by ignoring all the variation and ambiguity that attends the individual text. From the other it looks as though literary criticism has all along been restricted to making arguments with samples of one (i.e., the case study).
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - Both sides ultimately rely on competing theories of how to generalize from the particular, even when this theorizing is defined in opposition to generalization itself as a form of knowledge construction. As Lauren Berlant argues, the case as genre “hovers about the singular, the general, and the normative.” Even when there is an appeal to the singularity concept as “that which resists being generalizable,” this appeal runs “antagonistically into the prevalence of case-study narrativity in scholarship, which mobilizes a whole variety of descriptive and interpretive processes of determining likeness, generality, or patterning.” Conversely, the explicit construction of samples that scaled-up modes of reading demand, and the many ways proposed for explicating the relation of specific parts to specific contexts, entails its own assumptions about individual variation and difference. If methods for relating part to whole have been more formalized in disciplines outside literary studies, this formalization has always entailed its own set of narrative practices for dealing with cases. The point is not to choose one set of practices over another but to ask, citing Berlant again, “how certain norms of making a case got to be that way in a given domain of expertise and to consider how these norms might complement one another in domains that treat one or the other as the only legitimate path to knowledge.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - Part of the reason the aspiration to read at greater scales rankles critics of computational methods is because the emphasis on reading more is often conflated with an ambition to read everything.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - The latter smacks of a positivist outlook where exhaustive representation is believed to produce absolute truth and a Gods eye view of the world. The conflation is understandable given that the rise of computational criticism has coincided with the rise of data science and the hype surrounding “big data” as a means of producing knowledge about the world. As scholars in the emerging field of critical data studies have pointed out, this hype is predicated on the notion that access to vast amounts of data about human interaction provide an objectivity that obviates the need for theory. Samples acquired through social media platforms are so large now that one can mistake them for snapshots of reality in toto, but size is deceiving. It obscures the biases inherent in these data sets by virtue of how the data is collected. Having access to billions of Facebook or Twitter posts creates the illusion that one is observing a whole population when one is really seeing mixtures of subpopulations based on who has access to these platforms and on their platform specific effects. More generally, size obscures the reality that any large data set is also a “data assemblage” built from systems of thought, forms of knowledge, finance, political economy, governmentalities and legalities, materialities and infrastructures, practices, organizations and institutions, subjectivities and communities, places, and the marketplace where data are constituted. Rather than obviating the need for theory, these data assemblages demand critical reflection on the histories of their construction and the ways they are taken for representations and samples of the world.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - The same holds for digital collections that have been built by libraries, research centers, open-source platforms such as Aozora, or private interests—each a product of specific institutional forces, material histories, and selection practices. To critique them as obvious constructions, however, requires more than writing them off as irrevocably compromised or irrelevant to the ways we already have of knowing the literary archive. Any representation of the archive, however it may be mediated, is partial. Access to digital collections and the new evidence gaps they create are an opportunity to reflect on the partiality of knowledge past and present, and to problematize anew the notion of representativeness, not an excuse to double down on the fallacies of exhaustiveness or the irreducibility of the singular case. They should provoke the creation of a more flexible and nuanced language with which to think about how much evidence is enough to make a specific generalization and, conversely, how much is too little.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - Developing this language means looking outside of received disciplinary practices for relating part to whole. Within literary studies proper, the last decade or more has seen an internal interrogation of these practices and a proliferation of new or newly revived methods for turning textual evidence into interpretation and argument. Although generative, the splintering of “close reading” into myriad interpretive modes has done little to disrupt the dominance of “case-study narrativity.” In disciplines that have longer histories of dealing with quantity, there is a much richer, although no less contested, set of strategies for relating evidence to argument beyond the confines of the case study. Whether in statistics, where the idea that sampling could approximate larger populations took decades to establish, or in more recently quantified fields such as sociology, in which case-study and non-case-study approaches represent intellectual trade-offs rather than winner-take-all strategies, there is much to learn from how others have thought the relation of parts to wholes.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - SAMPLING TEXTS AND CONTEXTS
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - One particularly instructive field is content analysis. Although its origins can be traced to the eighteenth century, it emerged as a coherent method in the 1930s and 1940s in an effort by the empirical social sciences to respond to new kinds of information overload, namely, the rise of mass media forms (particularly newspapers) and their weaponization by the state for propaganda. Led by key figures in communication studies such as Harold Lasswell, Bernard Berelson, and Paul Lazarsfeld, content analysis was interested in how these forms shaped public opinion and were themselves shaped by specific ideologies. To understand them, however, meant being able to read large bodies of texts and images in a less impressionistic and conjectural way. They had to be read systematically to combat the human tendency to read textual material selectively, in support of expectations rather than against them.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - At the time, numbers were seen as integral to achieving such systematicity and were deployed to describe the manifest content of mass communication (e.g., stereotypes, ideological messages) with the belief that such content resided inside the texts or images in question. As the field matured, however, and found new applications in historical and literary study, this emphasis on quantification was tempered and the definition of “content” made relative to the contexts of use in which content, and its multiple potential meanings, are situated. What has continued to matter for content analysis are the problems of scale (how to read more texts than a single person can manage), systematicity (how to read for meaning in ways both replicable and consistent), and inference (how to relate the patterns of meaning observed to the contexts that can explain them). On this last point, Berelson and Lazarsfeld were clear from the beginning: There is no point in counting unless the frequencies lead to inferences about the conditions surrounding what is counted.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - When we look back to the first large-scale attempts at quantitative stylistics by Japanese scholars in the 1950s and 1960s, it can seem at times that they missed this critical point, as if they were counting just to count. Put more generously, their counting led them not toward inferences about a set of texts external conditions but to inferences about more texts. Klaus Krippendorff, in his instructive primer on content analysis, makes a critical distinction between these two types of inference. The first, inductive inference, entails generalizations to similar kinds, as when the characteristics of a larger population are inferred from what one observes in a sample of that population, or even in a single case. The early progenitor of quantitative stylistics, Hatano Kanji, was making an inductive inference when he concluded that Tanizaki Junichirō tended to write longer sentences after finding that the sentences in a single work were on average longer than in works by his contemporaries.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - Krippendorffs description of the analysts task will feel familiar to literary critics who seek to relate individual texts to contexts. The difference is that to read many texts in a consistent way content analysts are less able to let the hermeneutic process unfold across many divergent contexts, switching from one to the next as interpretation proceeds. They can still respond to multiple contexts, but for any one they must stop and explicitly construct the world in which the observations recorded from a body of texts “make sense” to the inferences they wish to draw. This involves choosing the sample of texts (and then the sample of features within those texts) that best supports the sense one wants to make. Here is where content analysis as interpretive method is useful for literary critics thinking across the evidence gap opened by the digital turn. It is, first of all, explicit about the relations it constructs between part and whole, whether the relation is of feature to text or of texts to context. Second, it offers several methods for positing these relations and a language for specifying how they bias and constrain the inferences one can make with them. There is less need for such explicitness when dealing with a handful of texts, as one can hold and turn each one, like a gem, to refract the light of one context and then another. Each text becomes the lens through which these contexts (aesthetic, discursive, political, historical) hold together and are made to support interpretative claims on the object. Try generalizing these claims across many more texts, however, and it becomes impossible to hold them all up simultaneously to so many rays of light. This is where sampling becomes a valuable heuristic for generalizing about more texts over a narrower range of contexts.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - In plainest terms, sampling is motivated by questions whose scope exceeds the evidence that is available or possible to observe. One samples when one cannot read everything relevant to the inferences one wants to make. And when one believes that not every unit of observation is absolutely the same, in which case it would be enough to look at a single instance, nor absolutely different, in which case it would be necessary to look at every instance. Sampling carves out a space in between these extreme ontologies, although different sampling techniques will do so based on different assumptions. The techniques of “traditional sampling theory, for instance, as developed in survey research, make several strong assumptions that rarely hold for texts in the ways they do for people: that the individuals sampled are independent of one another; that these sampled individuals are the units being counted; that every individual is equally informative on the questions being asked; and that the sample has the same distributional properties as the population it is meant to represent (e.g., the proportion of men and women in the sample matches the proportion in the population). These assumptions give traditional sampling theory its statistical power, and is the power to which proponents of quantitative method in the 1950s and 1960s appealed when, as noted in chapter 1, they created their samples of “modern Japanese literature.”
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - This power wanes, however, to the extent that sampled units are not equally informative with respect to the questions being asked. As Krippendorff argues, content analysts “have to consider at least two populations at once: the population of answers to a research question and the population of texts that contains or leads to the answers to that question.” He suggests several sampling strategies for bringing these populations into alignment. One is relevance sampling,” also called purposive sampling, which involves selecting texts based on how well they help answer a given research question. The criteria for a texts relevance may be the use of certain keywords, or inclusion in a particular type of publication, and is left to the analyst to specify. One does not sample probabilistically to represent some “true” underlying population of texts but rather constructs the population of relevant texts and samples from this much smaller universe. The critical difference is that this universe is recognized as an artificial world and not presumed to be a stand-in for the world itself. A slightly more formalized technique is called cluster sampling,” which is useful when one can identify lists of larger groups, or clusters, from which to sample randomly or systematically ones primary unit of analysis (e.g., articles, paragraphs, sentences). These might be journals and magazines containing literary works; genre labels assigned by critics or historical actors; or well-defined aesthetic movements in which works, and authors, can be grouped. Here, sampling bias can enter in if the units of analysis are unevenly distributed across the clusters, thus making it more difficult to justify statistical generalizations. A final, more formal strategy is “varying probability sampling,” which Krippendorff defines as useful when one can weight texts by their probability of relevance to a given research question. If the question pertains to the cultural influence of texts, for instance, one might consult best-seller lists, reviews, or book awards to sample more heavily from higher-ranked texts. One could also use this method to reverse known biases in normative discourse by giving greater weight to voices excluded from it. These strategies illustrate some of the considerations that go into constructing the world against which generalizations about a body of texts make sense. Although more constrained than the process of selecting exemplary cases, where relevance can be defined over multiple contexts at once, the strategies are no less open to the critical subjectivity of the researcher who defines “relevance” in relation to her questions and contexts of interest. Compared to these strategies, the samples produced for stylistic analysis in the 1950s and 1960s by Nakamura and others look ad hoc and ill-defined, conflating “relevance” with inclusion in a literary anthology.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - Another point of contrast from the predigital era comes from 1980, when researchers at the National Japanese Language Research Institute aimed to compile a new dictionary of modern Japanese. Their first step was to build a concordance of one hundred representative literary works (totaling more than three million words) from which to select example sentences for the dictionary. Like their predecessors, they first turned to literary anthologies as an index of value or importance. Instead of one anthology, however, they used fifteen, starting with Kaizōshas sixty-three volume Gendai Nihon bungaku zenshū (1926–1931) and ending with Chikuma Shobōs ninety-seven volume Chikuma gendai bungaku taikei (1975–1979). And instead of relying on one persons expertise, they convened a committee of ten prominent scholars and writers to select representative works, including major critics Maeda Ai, Nakamura Mitsuo, and Kōno Toshirō. Each committee member was asked to select one hundred works from a master list of 1,506— the number of works included in at least three of the fifteen anthologies. At this initial stage they were performing a loose kind of varying probability sampling on a population of texts deemed representative by virtue of anthologization, and where texts received greater weight the more they had been anthologized across the previous half-century.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - Every sampling strategy produces biases that over- or underrepresent aspects of the phenomenon being investigated and that must be accounted for in ones analysis. Content analysis offers a more principled language with which to think about these biases relative to the interpretive contexts built around our objects of study and the questions we ask of them. It is also a more explicit language that lends itself to collective critique, replication, or refinement. This is one crucial advantage of the sample just described, however vague its explanation of selection criteria. No such advantage can be ascribed to Aozora. It is a sample of “modern Japanese literature” in the way an ice core is a sample of historical climate patterns, produced from myriad unknowable selection processes, local material constraints, and its status as an online platform. It is what Krippendorff calls a “convenience sample”—a body of texts on which no sampling effort is made and which thus leaves “uncertain whether the texts that are being analyzed are representative of the phenomena the analysts tend to infer. This label applies to any digital archive whose size is taken as sufficient justification for its representativeness. The “bigness” of big data lures the analyst into thinking that a sample is the population, or that it can provide an unbiased estimate of dominant patterns within that population.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - Aozora is not so large that we might naively generalize its quantitative properties to all of “modern Japanese literature.” Such generalizations are rarely the point of computational literary history, and indeed the experiments to follow use a combination of more selective and purposive sampling techniques, filling in Aozoras inevitable gaps where necessary to pursue specific research questions. At other times they try to control for the effects of bias by extracting patterns across many samples to see how they hold up across these samples. Yet because the experiments still use Aozora as a starting point, we cannot ignore its particular relation to that global system of literary valuation of which it is one localized trace. What residual effects of this system do we find etched into its compound layers? In which directions do they skew? Before applying lessons from content analysis about how to sample, we need to understand what we are sampling from. What are the research questions to which Aozora is best suited, the evidence gaps it can help to narrow? This, in turn, is a chance to reflect on the partiality of other existing traces of the global system and to show how their comparison opens new ways of thinking about individual texts in relation to their many possible contexts.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - AOZORA BUNKO AS SAMPLE—THE CASE OF WORLD LITERATURE
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - To understand Aozora as a sample of the literary past first requires subsetting out the works of prose fiction. What initially looks like an impressive number of texts (about 14,300 with duplicates removed) masks their diverse generic difference. Based on the Nihon Decimal Classification (NDC) subject codes assigned to each text, almost 40 percent (5,575 texts) of the collection is fiction. Within this category, 20 percent (1,134 texts) are further classified as juvenile fiction. The next largest categories are essays and criticism (26 percent, or 3,695 texts), poetry (10 percent, or 1,425 texts), works related to theater and drama (4 percent, or 612 texts), and social science material (also 4 percent, or 611 texts). At the low end are categories such as history, letters and diaries, aesthetics, and religion and philosophy, all of which contribute 3 percent or less to the total collection. Together, the collection represents the output of 856 unique authors, including poets as old as Sappho (630–580 BCE) and writers born as late as the 1930s, with the majority (over 60 percent) of them born in the last half of the nineteenth century.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - Text length is another feature that exposes the collections diversity. For those interested in analyzing prose fiction, whether longer novels or shorter stories, Aozoras scope rapidly narrows since many of the works equate to just a few pages of printed text. At the extreme ends of the distribution are Miyamoto Yurikos epic novel Dōhyō (Signposts), a semiautobiographical work from the immediate postwar, and a haiku by Hagiwara Sakutarō (“Returning from Israel, I stand alone on top of the snow). Plotting the overall distribution by sorting the titles according to length in characters, we see that the haiku is far more characteristic of the length of texts in the collection (figure 2.1). Τo get a more precise sense of this, we can look at what falls in the middle of the distribution by calculating the average, or mean, text length. Close to the average is Hayashi Fumikos “Shitamachi” (Downtown), a classic postwar short story that runs to twenty-three pages in the pocket-size edition (bunkobon). The average can be a deceptive marker of the “middle,” however, given its sensitivity to outliers such as Dōhyō, which is 30 percent longer than the next longest text. Another statistic for indicating the middle of a distribution is the median, which is the point that divides the top half of observations from the bottom half. Here the median text length is about a third of the average, or as long as “Tsukiyo no denshinbashira” (Telegraph poles on a moonlit night), a childrens story by Miyazawa Kenji that runs to just nine pages in its bunkobon edition. Selecting out just the “fiction” titles and plotting the distribution of their lengths as a histogram (figure 2.2), the bulk of them still tend to be short, with the median length falling roughly between “Shitamachi” (20,036) and “Tsukiyo no denshinbashira” (6,878). This tendency reflects the historically less dominant status of long form fiction in Japan (i.e., the novel) rather than a predilection toward shorter works on the part of Aozoras volunteers.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - The predominance of shorter works is also a reminder of the specific local forms that narrative fiction took in Japan, at once influenced by ideas from elsewhere but never simply an imitation of them. Translation played an essential role in this mediated development, and indeed it has long been a truism that the history of modern Japanese literature after about 1890 is at once the history of Western literature written in Japanese. Scholars have added critical nuance to this assertion, questioning its narrow focus on “the West as the dominant influence and the assumption that this influence was felt immediately and evenly across the field of literary production, but most agree that translation was a constitutive creative force of modern Japanese literature at the level of both content and form. Any attempt to represent literary output of the period must acknowledge the presence of this “foreign literary hegemony,” strong or weak as it was relative to other national literatures. The legacy of this hegemony is reflected in Aozora too, a collection of ostensibly “Japanese” language texts that has a sizable amount of translated works, of which four hundred fifty are literary translations. Most originate in the prewar period and represent the labor of writers and scholars active in shaping the literary field at the time.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - Although none of the case studies in this book deal with translated fiction on a large scale, its presence in Aozora reveals in miniature the twofold problem of sampling previously described. Specifically, the problem of how to assess the limitations of this convenience sample as a representation of foreign literary hegemony. Consider that half of the four hundred fifty translations represent the work of fifteen authors, all men. This list includes Thomas Mann, Arthur Conan Doyle, Franz Kafka, Hans Christian Andersen, the Brothers Grimm, Edgar Allen Poe, Lu Xun, and Romain Rolland; but also Anton Chekov, Nikolai Gogol, Rilke, Baudelaire, and Dante. Half as many works again, or about one hundred ten titles, are by just three translators: poet Ueda Bin, writer Mori Ōgai, and Russian literature scholar Jinzai Kiyoshi. The influence of all of these figures is well attested in scholarship, but at this small scale, and without a research context in which to relate them, the choices underlying this particular arrangement of foreign literature feels arbitrary and random at best. How might we assess the gap between this arrangement and those we believe are a better reflection of the actual composition of foreign literary hegemony in the prewar period?
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - We could look to history itself and to representations of foreign literature that seemed reasonable at the time. After 1927, writers and readers in Japan would almost certainly have pointed to Sekai bungaku zenshū (Anthology of world literature) as one such representation. Published by Shinchōsha, this two-part, fifty-seven volume series was immensely popular and helped change the face of commercial publishing. Part of the enpon (one-yen) book boom that saw the emergence of a competitive market for cheap, multivolume series of modern great books, older classics, and other less high-minded collections for the masses, Shinchōshas series emerged a clear winner, selling more than four hundred thousand complete sets compared with rival Kaizōshas “modern Japanese literature” series, which sold just two hundred fifty thousand. The series was the latest in a long process of selecting, sorting, and canonizing the mass of foreign texts available for import or translation, whether by critics producing lists of notable books; by librarians curating the shelves for an expanding network of public libraries, or by educators eager to manage “international intercourse” in the Japanese language classroom. Beyond Japan, the series paralleled developments in British and American commercial publishing that from the 1890s saw the confluence of the “scholarly list” mode with the mass-market “enterprising” mode to produce what has been called a “patriarchal capitalist mode. Out of this mode came series like Routledges “World Library,” “World Classics” by Oxford University Press, and J. M. Dents “Everyman Library for Young People,” created in 1905 with the ambitious goal of publishing one thousand great works. Reducing an infinite sea of texts to a curated selection, these series were meant to educate the masses as much as to keep the lay reader “out of the grog shop or away from newspapers.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - The affinity of Sekai bungaku zenshū with this patriarchal capitalist mode is made clear in the unprecedented two-page ad that series editor Satō Giryō created in 1927 and in which he claimed that the series was equivalent in content and price to Everymans Library. This was pure salesmanship in terms of the actual number of works, but part 1 of the anthology contains many similar authors and titles (table 2.1). Dante, Goethe, Milton, Shakespeare, and Cervantes were there to represent the classics; Hugo, Balzac, Dickens, Flaubert, Maupassant, Tolstoy, Turgenev, Dostoevsky, Poe, and Hawthorne stood in for the nineteenth-century novel; Chekov, Gorky, Maeterlinck, Ibsen, and Hamsun, among others, added more contemporary fare. Satō, like Dent, also framed the series as part pedagogical and part civilizing mission, touting it as a “giant textbook for the study of humanity” and “a necessary qualification for being a global citizen.” But the masses were not the only market he and Shinchōsha were after. They knew that the works had to appeal just as much to those writers and intellectuals who would confer on them the cultural capital necessary to make them valuable. When prominent political theorist Yoshino Sakuzō authored a promotional piece in Tokyo nichinichi shinbun (Tokyo daily newspaper) in advance of the series publication, he quoted a friend who, upon seeing the two-page ad, exclaimed that every title, to the very last one, was “food for the soul and a work that should always be close at hand. I want them all so bad Im ready to rush out and get them. This was a textbook for the masses as much as an ornament displaying that one had already absorbed the lessons contained therein.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - The window that the Shinchōsha series provides onto historical conceptions of world literature in Japan is easily explained by Pierre Bourdieus idea of a “dual discourse,” which suggests that the democratization and mass production of a cultures “classics” is always partly an opening up of knowledge only after it has been “permitted, authenticated and ultimately cheapened by the upper strata, whether these are the aristocrats and officials who endorse the books or the professionals and intellectuals who edit and publish them. But this is not the only way to read the intentions behind these choices. In her study of Oxfords “World Classics” series, Mary Hammond reminds us that a sociological frame can obscure more prosaic rationales for text selection. For the editor of World Classics,” what often took precedence over aesthetic merit or canonical recognition was whether a work was out of copyright and cheap to acquire; its length suitable to the series format; and its content inoffensive to lower- and middle-class readers. Shinchōsha, for its part, had been publishing literary translations for a decade, and more than 60 percent of the titles in part 1 had been published in earlier formats. Editor Satō was also adamant about including translations that would be highly legible to mass audiences and took extra measures to edit each translation for clarity even when it was a previously published title. Such pragmatic rationales make the process by which some works are plucked from global flows to be translated and later canonized seem all the more contingent.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - These contingencies raise again the question of how reasonable was this arrangement of writers and works as a representation of what was being translated and circulated prior to this moment. How arbitrary were the editors selections? If they were partly biased by the twin interests of profit and pedagogy, how far and in what direction did they skew? Contextualizing the seeming contingency of these selections requires, as scholars of world literature and book history have shown, an attention to processes of anthologization and curricular canonization; the role of publishers in shaping market dynamics; reception as imbricated in institutional and archival histories; and the effects of translatability and aesthetic form. Here I contend that the bibliographic record itself can be a means of delimiting the contingency often read into acts of selection. Contingent decisions naturally compound over time, lending a historical gravity to the value of works that becomes visible at the scale of this record, or at least the traces of it that are available to us. In the rest of this section, I show how this visibility can help to contextualize the perceived arbitrariness of the representations of world literature in the Shinchōsha anthology, but also in Aozora.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - To reframe this quantitatively, we can imagine each authors record of translation into Japanese as a temporal trend line, calculating the percentage of all translations attributed to that author for each year. Alternatively, we can aggregate the trend lines of many authors together, showing how much collective attention is given to them by the publishing market each year. With Sekai bungaku zenshū, for instance, it makes sense to group authors based on the part in which they were included because the editorial calculus differed for each part. Authors in part 1 were generally older, more canonical figures, with a median birth year of 1836. Authors in part 2, which began publication in 1930, skew younger, with a median birth year of 1876. Here Shinchōsha was placing bets on Jack London, Upton Sinclair, Aldous Huxley, Thomas Mann, and Leonid Leonov, who were still relatively new to the translation market. If we plot the trend lines for each group, we can then determine whether a significant change occurs at the moment of publication. Does the moment mark a break in the trend line, such that the percentage of translations noticeably drops or begins to rise? Or is the moment just one stage in a continuing trend? Determining this becomes a way to understand whether the anthology was responding to the historical gravity of past contingent decisions or setting them into motion.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - Of all the bibliographic traces assembled in this chapter, the one for translations traveled the longest route to become digital data. Scanned and hand input from two print resources, it is incomplete in parts and noisy in others due to errors introduced by optical character recognition (OCR). It also bears the peculiar histories of these two resources. The first, Meiji-ki honyaku bungaku sõgō nenpyō, covers the Meiji period (1868–1912) and provides a comprehensive list of 4,510 literary translations in all genres published in newspapers, magazines, or individual books. Entries are listed by year and categorized by the source authors country of origin. A second resource, Meiji • Taishō • Shōwa honyaku bungaku mokuroku, covers 1912–1955 and is a nearly eight hundred page index compiled by the National Diet Library. Each of the nearly twenty-eight thousand entries, representing works by 2,398 foreign authors, includes metadata about the authors country of origin, dates of publication, translator, and publisher. Like the Meiji index, it records literary translations across all genres but is limited to stand-alone volumes or multivolume sets, including reprints. It is thus blind to the mass of translated material published in general interest and small coterie magazines, which could shift attention more quickly to the newest literary imports. Both resources are mostly blind to literature outside of Euro-America and Russia, thus reinforcing the equation of “world” with “the West,” which was also integral to Shinchōshas vision. Not one Chinese writer, for example, is included in either bibliography.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - Although bearing the symptoms of a particular definition of world literature, insights can still be gleaned from these bibliographic traces. Here I concentrate on the temporal aspects of the data as they help to situate the selections made by editors of the Shinchōsha anthology. Figure 2.3 shows the raw counts of translated items by year for both bibliographic data sources. Every indexed translation is counted equally, whether it is a long form novel published as a stand-alone volume, a short story included in a collection, or a poem from an anthology. This figure already tempts us with several stories about how the field of literary productions relationship with translation evolved over this period. There is a clear story of decline as the Pacific War (along with state censorship and resource scarcity) reached its climax, although overall publication trends are needed to contextualize this decline. There are also less obvious stories about translations rapid rise from the wars ashes and its equally rapid rise in the 1920s, peaking at about nine hundred translations just as Shinchōsha was preparing to release its anthology. One of the most surprising potential stories is that there was so much room for the translation market to grow after what looks like several years of stagnation in the teens. Did the market really contract so much relative to the years before and after? Was it because the publishing industry as a whole was in a slump?
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - These questions are a reminder that no data provide a sufficient explanatory context on their own. Data always beget more data (i.e., other contexts for interpretation). In the case of this particular low point in the teens, we need to know what came before and what relation the raw counts have to overall publishing trends. By merging the data with counts from the Meiji index and dividing the yearly totals by the overall volume of publications for each year, we can revisualize it through these new contexts. Figure 2.4 shows the rolling two-year average for translations as a percentage of total publications from 1883 to 1942 (these totals include newspapers and magazines for the Meiji period). The visualization confirms that the immediate post-Meiji years represent a relative decline in the translation market when compared with the previous decade and in relation to publishing trends overall. What caused the decline, or whether it was felt as such at the time, is beyond the scope of this chapter. We might look to the “High Treason Incident” (taigyaku jiken) of 1910, which put a chill on political and literary expression in these years as the government clamped down on material deemed subversive or detrimental to traditional morality. Foreign books and literary translations were seen as particularly dangerous in this light and were already subject to suppression by Home Ministry censors. We might consider whether dominant aesthetic movements in these years, whether the self-centered Naturalism of I-novel writers or the opposing antipolitical aestheticism (tanbishugi) of Nagai Kafū and others, led to a general inward turn. Or perhaps translators simply shifted their efforts toward magazines and other media not captured in this data set.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - The bibliographic record is only an entry point into deeper qualitative inquiry, but by contextualizing its temporal dynamics we can construct periodizations that are not discernible at higher resolutions. If literary translation entered its own “winter years” from 1913 to 1918, as the figure seems to indicate, how might this evidence add to more localized ideological, aesthetic, or institutional evidence brought to bear on our reading of individual acts of translation? Knowing, for instance, that Tolstoy, Emerson, Maupassant, and Goethe were most widely translated in these years -a period that saw the median birth year of all translated authors drop significantly, bottoming out at 1810, or roughly 108 years in the past (figure 2.5)—the next most translated author, novelist, and playwright, John Galsworthy (1867–1933), and future Nobel laureate, stands out all the more. Ōtani Gyōsekis decision in 1914 to publish twenty-three of his recent dramatic sketches, some of which bore the political consciousness of class and gender issues for which he was just coming to be known, seems all the more fresh, radical, and prescient. As the market for translations recovered from an apparent downturn in the teens, building to a wave that crested just as Sekai bungaku zenshū was published (1929), we might wonder how striking its choices were in light of that wave and its aftermath. Did they reflect and reinforce past trends, or did they deviate from them, signaling futures to come?
[Author: Matthew Kirschenbaum; From essay:"What Is Digital Humanities and Whats It Doing in English Departments "] - These recent, definitional conversations bear the mark of a field in the midst of growing pains as its adherents expand from a small circle of like-minded scholars to a more heterogeneous set of practitioners who sometimes ask more disruptive questions. They also signal the ways in which the applied model of digital humanities work portends significant shifts in the nature of humanities scholarship. When a DH scholar attempts to include within her tenure dossier (if, indeed, the scholar is even on a tenure track and not one of a growing set of “alt-academics) not only articles and books but also, for example, code for a collaboratively built tool that enables other scholars to add descriptive metadata to digitized manuscripts, key questions about the nature of scholarship are raised. Several essays within this volume deal with such questions, and institutions such as the Modern Language Association have compiled guides to help DH scholars begin to answer them.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - It also raises a key methodological question that every analysis of temporal trends must address: how to know when a change in the data is meaningful or not. Time series analysis is the subfield of statistics devoted to this question, and from which I borrow one of the simplest methods for testing whether a “structural break exists at a specific point in a trend line. Known as the Chow test, it tells us whether linear models fit to subsets of a sample (i.e., the trend lines before and after a specified break point) perform differently from one another and from the trend line as a whole. A significant result (p < .05) means that the two subsets perform differently enough to provide evidence of a “structural break” in the data. I apply this test to the moving average of the percent of all translations from authors in parts 1 and 2, breaking the overall trend lines at the respective points of publication (1929 and 1932). The trend lines are shown in figures 2.6 and 2.7. For part 1, a Chow test reveals that there is not significant evidence of a structural break (p = 0.79). Despite considerable variation at the author level, the aggregate trend is consistently downward, suggesting that Shinchōsha bet on authors (forty-six in total) whose fortunes in the translation market were on their way down, although certainly not out. By 1950 their works still make up over 25 percent of all translations in the data set. For part 2 there is more evidence of a structural break (p = .05) because a clear, steady ascent prior to 1932 turns into a noisy trend line showing varied levels of attention from year to year. Shinchōsha was taking a bigger bet with these less proven authors that seems to have paid off, incidentally, for authors Thomas Mann and Georges Duhamel but not Jack London and Aleksandr Kuprin.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - What, then, of the foreign authors Shinchōsha ignored, or the ones yet to appear on the horizon? A final affordance of the bibliographic record is the ability to read the nonselection of authors as itself indicative of the standards of literary value by which the anthology editors were operating. It allows us to see this operation in inverse, as it were, hinting at how improbable it was for an author not to be anthologized given his or her presence in the larger market. Figure 2.8 shows the top fifty translated authors overall between 1912 and 1955, categorized by whether an author was included in Sekai bungaku zenshū or not. Each omission potentially masks a more interesting story, whether about the slower uptake of writers such as Andre Gide and Herman Hesse; the ambiguous positionality of Lafcadio Hearn vis-à-vis received images of world literature; or the genre biases that kept Arthur Conan Doyle from appearing in those images.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - I end with the list of translated authors with which we began. Just as the bibliographic record was used to situate the Shinchōsha anthology against a wider field of historical gravity, we can do the same for Aozora by comparing its contents with figure 2.8. Although the record provides no insight into how these authors fared after 1955, the presence of any overlap is instructive. Doyle, for instance, feels less out of place in light of how much attention his work has been given historically, but so too Chekhov, Poe, Gogol, and Mann. Rather than just a random or haphazard sample of world literature, Aozora has discernable links to the longer history of translation in Japan. At the same time, the absence of Tolstoy and Maupassant is more glaring given their dominance in the bibliographic record, which perhaps at how much attention has shifted away from these writers since 1955. In these gaps between past and present acts of selection is the chance to think about how to supplement Aozora as a digital archive to create more reasonable samples of prewar literary translation. How one does so will ultimately depend on what is to be asked of this material at greater scales of inquiry. But here we have seen how the bibliographic record can be a useful context through which to begin to evaluate our own selections against the compounded contingencies of history.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - AOZORA BUNKO AS SAMPLE—THE CASE OF NATIONAL LITERATURE
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - Turning attention to Japanese language literature, which makes up a far larger portion of Aozora, we might expect the gaps between present and past acts of selection to be narrower, or at least less idiosyncratic. To be sure, copyright law will be a major constraint on its coverage. But unlike in the United States, where the law imposes a clear cutoff (currently all works published before 1924), in Japan copyright is applied to the author and was, until 2018, set at fifty years after death (it is now seventy). It is thus harder to know exactly where the gaps widen into a sharp cliff. Something of copyrights impact is visible if we compare Aozoras contents with the 139 works selected by the National Japanese Language Research Institute (see appendix table A2.1). Aozora contains seventy-nine of them (57 percent), although it could contain one hundred based on the current copyright status of the authors. Excluded due to copyright are authors who made their name in the immediate postwar period (e.g., Mishima Yukio, Ibuse Masuji), but also canonical figures who lived into the early 1970s (e.g., Shiga Naoya, Kawabata Yasunari, and Mushanokōji Saneatsu). But what of the twenty-one titles where copyright is not an issue? What constraints, beyond copyright, might explain their absence, and in turn help understand the gaps between Aozoras representation of Japanese language literature and those found in other traces of the print archive?
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - To get at these questions requires digging deeper into the anthology as archival instrument. We have already learned how so many past samples of Japanese language literature depended on anthologies as a first-order selection mechanism, including the list of 139 representative works just discussed. Contributors to Aozora have similarly relied on anthologies as the basis for their transcriptions. One important way to illuminate Aozoras relation to the print archive, then, is to ascertain the ways in which it aligns with, or diverges from, the contents of literary anthologies and the selection biases contained therein. Undoubtedly, comparison with anthologies reveals more about Aozoras representation of modern literature relative to the history of commercial publishing than to the realities of prewar literary production. Because this history is responsible for instantiating the hierarchies of value that shape perceptions of prewar fiction, however, as Edward Mack and others have argued, the comparison can help delineate Aozoras relation to the prewar print archive vis-à-vis these hierarchies. How does Aozoras representation align with what publishers and editors have collectively imagined prewar fiction to be?
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - As a first point of comparison, consider the anthology mentioned several times in the previous chapter: Gendai Nihon bungaku zenshū (Collected works of contemporary Japanese literature, 1953– 1958), published by Chikuma Shobō (hereafter GNBZ). This is the anthology to which Yasumoto Biten and others in the 1950s turned when looking for ways to scale up their stylistic analysis. Spanning ninety-nine volumes, GNBZ ushered in a new postwar standard for anthologies of “modern Japanese literature,” replicating the success of its earlier namesake, published by Kaizōsha between 1926 and 1931. Like its predecessor, Chikuma Shobōs editors wanted the anthology to be affordable and accessible to a mass audience. They also sought to restore a vision of the modern canon purged of the ideological extremes prevalent in the war years. Editor Usui Yoshimi, the projects mastermind, originally wanted to title it Kokumin bungaku zenshū (Complete works of national literature), drawing on the postwar reinflection of kokumin as a symbol of a more democratic, more human-centered national polity. Usui, a prominent critic and novelist in his own right, was largely responsible for selecting the anthologys contents, which included 1,758 works of fiction, poetry, theater, and criticism. Fiction made up three-quarters of the total. The works represent the output of 215 unique authors— half of whom have five or more works in the collection—and go as far back as Tsubouchi Shōyōs Shōsetsu shinzui (Essence of the novel, 1885-1886) and as far forward as postwar best sellers like Õoka Shōheis Nobi (Fires on the plain, 1952). Only 16 of the authors (7 percent) are women.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - Chikumas bet that Japanese readers would be hungry for a newly purified version of the modern canon, one that shied away from political ideology or from dealing with questions of class and ethnicity (i.e., proletarian and colonial fiction), paid off. The anthology was one of the most successful ever published, selling 1.3 million volumes and saving Chikuma from imminent bankruptcy. With its focus on high-brow vernacular fiction from the 1890s through the early 1930s—the “old literature,” as Nakamura Mitsuo dubbed it in 1952-but also works by a new generation of postwar authors writing in what he criticized as the “old style,” the anthology consolidated a prototypical vision of what counted as literature in this period. That vision is only partially reproduced in Aozora, which contains variants for just 34 percent (600 works) of the titles in the GNBZ across all genres. For fiction alone, this figure jumps to 38 percent (503 works). The gap is narrower in terms of author overlap: Aozora contains titles by sixty-two of the authors with five or more works anthologized in GNBZ, or 58 percent. One might suspect copyright to explain the rest of the disparity, but based on the death dates of the authors in GNBZ, a remarkable 75 percent of the titles have already entered the public domain. In concrete terms, 700 works that could be part of Aozora are not. Figure 2.9 illustrates the copyright cutoff years distributed across all GNBZ titles and conveys what proportion of GNBZs vision of modern Japanese literature could be added to Aozora right now. The portion between the dotted lines also shows how much of the sizable remainder must wait until 2055 before it can be added.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - It is perhaps not surprising that Aozoras contents do not mirror Chikumas very specific postwar vision. But neither does it appear that Aozora is pursuing it, at least based on the copyright status of the works in GNBZ. A few obvious targets are put out of reach by copyright, including standard bearers of the “old literature” such as Kawabata Yasunari and Shiga Naoya, but this is not the overriding constraint on what has been selected for digitization. Some authors simply do not appeal to the tastes and interests of Aozoras volunteers. Tsubouchi Shōyō and Uno Kōji, for instance, have only a dozen texts between them in the collection. When they do appeal, attention is typically given to a different set of texts than the ones selected by GNBZ editors to represent an authors oeuvre. Notably, of the one hundred authors with the most works of fiction in Aozora, almost half (forty-eight) are not included in GNBZ, a large portion of whom are writers of popular and historical fiction. Although this comparison with GNBZ yields a few important clues for uncovering Aozoras underlying logics of selection, it can only take us so far. It reveals nothing of the processes of taste making and canonization over the last seventy years that have likely made certain works and authors more susceptible to digitization. How might we capture something of the aggregate effect of these complex, evolving processes? What titles and authors would rise to the top if we could?
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - John Guillory argues that every attempt to present the canon, even in the form of the omnibus anthology, “remains a selection from a larger list which does not itself appear anywhere.” The canon is always “an imaginary totality of works that is never accessed as a totality because any invocation of it is always partial and contextual. Anthology editors impose their biases on the process of selection even when they are ostensibly trying, as were the editors of Kaizōshas groundbreaking Gendai Nihon bungaku zenshū, to be comprehensive or to capture the “noise” of an entire period. When these biases are funneled through committee, as in the creation of an anthology known to have inspired Kaizōshas series, the Harvard Classics, the results can feel “more or less arbitrary,” as that series editor Charles Eliot once put it. Even though these decisions can seem partial or random in their moment, they can be reinforced and repeated over time. Exploring the compound effect of these decisions (i.e., which titles and which authors were anthologized most often) will never bring the canon as imaginary totality into view, but it points us to the pools and shallows where the unpredictable currents of literary judgment have tended to gather.
[Author: Matthew Kirschenbaum; From essay:"What Is Digital Humanities and Whats It Doing in English Departments "] - Similar definitional debates can be found in the pages that follow. Where, for instance, does new media studies leave off and digital humanities begin? Does DH need theory? Does it have a politics? Is it accessible to all members of the profession, or do steep infrastructural requirements render entry prohibitive for practitioners working at small colleges or cash-strapped public universities? Are DHers too cliquish? Do social media platforms like Twitter trivialize DHs professional discourse? Can DH provide meaningful opportunities to scholars seeking alternatives to tenure-track faculty employment? Can it save the humanities? The university?
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - In 2004, the publisher Nichigai Associates produced an extensive record of these decisions by indexing the tables of contents of 1,255 omnibus and individual author anthologies. It contains nearly six hundred thousand entries by over eight thousand five hundred unique authors across the genres of fiction, poetry, and theater, beginning in 1897 with Narushima Ryūhokus collected works (Ryūhoku zenshū). Figure 2.10 shows the number of discrete zenshū volumes published each year and tells us both familiar and lesser known stories: a first wave of anthologization driven by the enpon (one-yen book) boom of the late 1920s; a second wave in the mid-1950s as publishers such as Chikuma Shobō tried to reestablish a literary market lost to wartime censorship and deprivation while taking advantage of new demand from public and school libraries; and a third phase starting in the late 1960s when the market peaked. The data is suspiciously sparse before 1925. Not included are several multivolume collections that were key precedents for the Kaizōsha-led enpon boom. After 1925, however, the data better represents the range and volume of omnibus publications. To wit, one historical account contends that over three hundred multivolume series were published at the height of the enpon boom (1925–1929). Impressive as this sounds, only forty of these were related to modern Japanese fiction, poetry, or drama. Of these, sixteen were omnibus anthologies. The Nichigai index catalogs twelve of them and lacks only two of the series dedicated to prose fiction.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - Assessing the indexs coverage of the postwar decades is more difficult owing to the tremendous increase in anthology publication. From the 1950s onward, old and new publishers alike competed fiercely for space in the expanding mass market of zenshū publication, producing dozens of variations on Kaizōshas original concept. Takashima Kenichirō estimates that as early as 1953 there were roughly three hundred zenshū of all types in production, accounting for 30 percent of the total book market. Lacking a comprehensive list of all these zenshū, it is hard to estimate what proportion is covered by the Nichigai index. Nevertheless, the trend line for volumes per year, split into the categories of omnibus and author anthology, reflects the current understanding of the rise and fall of the zenshū market. Figure 2.11 shows how each type of anthology contributed differently to the three waves described here and clearly indicates how quickly the zenshū bubble burst in the late 1970s. A decade of speculative investment that saw publishers marketing to new middle-class homeowners and their increasingly educated baby boomer children ended abruptly with Chikuma Shobōs bankruptcy in 1978. After this, individual anthologies comprised the bulk of the market, and omnibus anthologies shifted their focus to literature long left out of the canon. Whatever aggregate view of modern Japanese literature the Nichigai index offers, it will be dominated by editorial choices made before the omnibus bubble burst.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - At the scale of hundreds of anthologies and hundreds of thousands of individual selections, obtaining this view is no longer as easy as examining a list of titles. First, it is necessary to limit the scope of analysis to fiction because other genres, especially poetry, are likely to follow different patterns of anthologization. Using a dictionary of modern Japanese literature, I labeled authors according to the primary genre in which they wrote. Some of the authors labeled as fiction writers by this method naturally wrote in other genres, but the aim is simply to capture those authors whose fiction was likely to be anthologized frequently. This process reduced the original data set to nearly 190,000 entries, of which 38,360 come from 179 omnibus anthologies and the rest from 569 author anthologies. Although still a lot to look at, the units of analysis are now somewhat more comparable. The challenge, as with the previous translation data, is in deciding how to sort these units. Simply counting the most anthologized titles or ranking writers by the number of works anthologized will mask other ways of seeing the data. Should works published in the prewar period be treated the same as postwar works that have had less time to be anthologized? Do we treat an author with a few heavily anthologized works the same as an author with many works spread out across different anthologies? Does any single strategy offer a more reasonable basis of comparison with Aozora?
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - Any strategy will influence how we interpret the data because each will privilege some dimensions at the expense of others. With respect to individual titles, raw frequency presents a skewed perspective of a works importance because it does not account for time. Works that are anthologized earlier are more likely to end up at the top just by virtue of their having had more time to be anthologized. Table 2.2 shows how this can obscure works that were just as successful in their own time. In the first column are the top ten most frequent titles (and their authors) in the data set overall. Subsequent columns list the top ten titles for the periods 1920–1959 (the first and second zenshū booms); 1960–1979 (the peak bubble for zenshū production); and 1980–2003 (after the collapse of the omnibus anthology market). With the exception of Õoka Shōheis Nobi, the overall counts are biased toward prewar works. A couple of these works take an early lead in the first period and maintain this high pace of anthologization in the second. But it appears that the second period is really driving the process of consecration. Six of its top ten titles are included in the overall list. In the third period, we see the impact of the postbubble market for zenshū in the form of lower overall counts, but also a set of authors absent from any of the other lists. This is not due to the relative newness of these titles because all are prewar or immediate postwar works. Rather, there appears to be a marked shift of attention in this final period away from pure” literature and toward popular genre fiction, precisely the kind of thing that was missing from the vision that GNBZ set into motion.
[Author: Matthew Kirschenbaum; From essay:"What Is Digital Humanities and Whats It Doing in English Departments "] - These questions and others have vexed the public discourse around the digital humanities for a few years now, but to date such discussions have taken place predominantly on listservs, blogs, and Twitter. Few attempts have been made to collect and curate the debates in a more deliberate fashion, with the result that some conversations, especially those on Twitter-a platform used extensively by digital humanists—are hopelessly dispersed and sometimes even impossible to reconstitute only a few months after they have taken place.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - Because raw frequency obscures the advantage afforded to works anthologized earlier, we need a way to normalize for time so that we can compare works on a time-neutral playing field, just as we would normalize a citys crime statistics relative to its population. The easiest way to do this is to turn the raw counts into a ratio, dividing a works total number of inclusions by the years since it was first anthologized. In this way, a work anthologized ten times over sixty years will have half the value of one anthologized ten times over thirty years. We can also normalize time by giving more importance to works anthologized intensely over a short period but that did not gain a foothold in the canon. Dividing frequency by the first and last years of anthologization provides us with such an “intensity” measure. Table 2.3 shows the top titles as determined by these two measures. Even from this small sample, it is clear how different are the stories that each ranking tells. Õokas Nobi—the harrowing account of a soldiers final desperate days fighting in the Philippines and a sustained meditation on the dehumanizing effects of war-rises to the top of the Time Normalized list. It shares this space with several award-winning works from the 1950s, namely, Yasuoka Shōtarōs Kaihen no kōkei” (A view by the sea, 1959) and Yoshiyuki Junnosukes “Shūu” (The rain shower, 1954), but also Kawabata Yasunaris canonical Yukiguni (Snow country, 1947). Literary giants of the prewar period have been entirely displaced by their immediate postwar counterparts. In the Intensity column, in contrast, we find familiar authors alongside mostly unfamiliar works—one-hit wonders that flashed brightly before fading, including Inoue Yasushis “Kōzui” (Flood, 1962), anthologized nine times in just three years (1966-1969).
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - We can imagine other normalization strategies using contextual variables such as sales data, which would allow for ranking titles by the commercial success of each anthology. My intent, however, is simply to point to the many possibilities for transforming frequency into relation in a data set. Each forking path through the data entails different assumptions about how its objects relate to one another, and each produces visions of the literary universe with varying degrees of overlap with the vision in Aozora. Works with high intensity, for instance, are likely to recede more quickly from the cultural imaginary and be less ready to hand for Aozoras volunteers because they were less frequently reprinted. Works with more staying power in omnibus anthologies are likely to be more susceptible to digitization. At the same time, if we go by the time normalized measure, they are also more likely to be works still in copyright. In fact, Aozora contains one hundred of the top one hundred fifty works by raw count (67 percent); sixty-nine of the top one hundred fifty by the time normalized measure (46 percent), owing to the increase of in-copyright titles; and a mere thirty-two of the top one hundred fifty by the intensity measure (21 percent). (Recall that the overlap with the LRI list was 57 percent.) These numbers enhance our understanding of how Aozora aligns with the different aggregate visions projected by the Nichigai index and how well it could address research questions relevant to such visions.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - Another perspective on Aozoras degree of alignment is provided when we use authors as the unit of analysis. Here too we might be inclined to sort authors by the number of total works in all omnibus anthologies. But this would privilege authors anthologized earlier, treating every work, no matter its length or genre, as equally informative of an authors stature. If we suppose that brevity raises the chance of a works anthologization, raw frequency will further advantage authors who mostly wrote shorter works. In truth, long novels were not necessarily avoided by omnibus editors, and some publishers made them a selling point while developing printing formats capable of cramming more text on every page. Moreover, just having a greater number of short works is no guarantee of their inclusion, or of the same ones being selected over time. Assessing the impact of length on anthologization will require a study of its own, not to mention precise data on page length, but it is clear that we need a way to both control for time and to acknowledge that the sheer number of works anthologized obscures other factors. The question of how much” needs to be qualified, at the very least, by the question of “how often.”
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - A popular measure from bibliometric studies, known as the Hirsch index (or h-index), does precisely this. Devised as a method to quantify the impact of a scientific researchers output in terms of breadth of citations in academic journals, the measure has become an oft maligned symbol of the devaluation of academic labor under neoliberalism and the rise of a new class of administrative bean counters in the university. Outside of its political uses and abuses, however, the index is an attempt to quantify the amount of an authors productive output by how widely it is cited by peers in that field. Applied to the anthology data, such that selection by an editor is seen as a kind of citation, the h-index transforms raw counts into a relational framework in which how an author is anthologized matters more than how much. Mathematically, the h-index works by taking all published (e.g., anthologized) titles, ranking them by their number of citations (e.g., inclusion in an anthology), and counting down from the first position for as long as the rank is greater than or equal to the citation count at that rank. The rank at the last position where this holds true is the h-index. For instance, the author with the highest h-index in the Nichigai data set is Akutagawa Ryūnosuke (h-index = 21). This means that twenty-one of his most anthologized works have been included in an omnibus zenshū at least twenty-one times. To normalize for time, we can divide this value by the years since the authors first work was anthologized. This “m-index” puts authors of different generations on more equal footing.
[Author: Matthew Kirschenbaum; From essay:"What Is Digital Humanities and Whats It Doing in English Departments "] - Debates in the Digital Humanities seeks to redress this gap and to assess the state of the field by articulating, shaping, and preserving some of the vigorous debates surrounding the rise of the digital humanities. It is not a comprehensive view of DH or even an all-encompassing portrait of the controversies that surround it, but it does represent an attempt to clarify key points of tension and multiple visions of a rapidly shifting landscape. The contributors who provide these visions have a range of perspectives; included among them are some of the most well-known senior figures in the field, well-established midcareer scholars, rising junior scholars, #alt-ac digital humanists, and graduate students. This mix of new and seasoned voices mirrors the openness of digital humanities itself and reflects its strong tradition of mentorship and collaboration.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - Table 2.4 lists the top twenty authors as determined by each metric, with total counts followed by the h-index and m-index values. Comparing these lists, it is clear how much the raw counts privilege writers most active in the prewar period. This view of the literary universe will feel all too familiar to students of modern Japanese literature, with the giants of late-Meiji fiction well represented (e.g., Mori Ōgai, Izumi Kyōka, Higuchi Ichiyō, Natsume Sōseki) and so, too, their dominant Taishō and early-Shōwa period successors (e.g., Akutagawa, Shiga Naoya, Kawabata Yasunari, Tanizaki Junichirō). The h-index measure shuffles the order of some of these names but replaces only four of them with new authors: Kajii Motojirō, Yokomitsu Riichi, Nagai Tatsuo, and Umezaki Haruo. The differences are instructive, however. In the case of Kajii, for instance, we learn that, despite his relatively low overall count, he fares much better than others when accounting for how many of his works have been repeatedly anthologized. Although his and Yokomitsus presence at the top is not unexpected, that of Nagai and Umezaki is, especially given how little scholarly attention their early postwar fiction has received. When controlling for time, their presence groWS even stronger and so, too, does that of other canonical writers who made their name after the war, including Õe Kenzaburō, Sakaguchi Angō, and Mishima Yukio. In the last slot of the m-index column is Õoka Shōhei, whose much lower ranking across all these measures indicates how much his presence in omnibus anthologies is dependent on a few highly anthologized works.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - These lists are just one way of assessing authors popular and scholarly reception in the postwar period. They provide narrow windows onto an imaginary totality that no one had direct access to -a totality filtered through the bits and pieces that made it onto the reference shelves of public and school libraries or the newly outfitted middle-class interiors born of Japans postwar recovery. These windows could be used to justify the construction of a corpus that sampled authors based on their relative positions in these lists, but this would also mean surfacing their underlying assumptions and linking them more concretely to specific research questions. Here the windows are useful to the extent they reveal the compound effects of cumulative editorial choices on the authors who are heavily sedimented into the archive. In particular, they allow us to compare the result of these effects with the compound vision available through Aozora. A direct comparison is more difficult without some measure of relative attention (e.g., which authors are most often viewed on Aozora), a problem I take up shortly. But some insight can be had by looking at the authors best represented in Aozoras fiction (excluding juvenile fiction) in terms of number of titles archived in the collection. Figure 2.12 shows the top twenty authors and their representation relative to the top fifty authors overall. A few names are familiar from previous lists, but the majority are not.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - In fact, nearly half fall into the category of popular genre writers, many of whom are the same authors most frequently anthologized after the bursting of the zenshū bubble: Edogawa Ranpō, Yumeno Kyūsaku, Hisao Jūran, and Okamoto Kidō. Although not excluded from the earlier omnibus anthologies, they appear with much less frequency or are segregated into their own anthologies dedicated specifically to popular” (taishū) literature or one of its subvariants, including detective, historical, fantasy, horror, and science fiction. The impulse to create such a two-tiered market goes back to the original zenshū boom of the late 1920s and is part of the very bifurcation of the field of production that this boom, and related developments in the print culture industry, helped reinforce. Within anthology publication specifically, however, the level of overt investment in popular fiction has fluctuated over time. Indeed, calculating the moving average of the number of volumes dedicated to variants of such fiction, we find that investment bottomed out precisely during the postwar bubble before rising again in the 1980s and 1990s to constitute most of what remained of the omnibus anthology market (figure 2.13). In this respect, the predominance of popular authors in Aozora serves as a corrective to the canonization of mostly prewar authors, and mostly “pure literature” (junbungaku), in the second and third waves of zenshū production. If Aozora partly represents the results of this canonizing process, it also fills in some of the areas buried by or expunged from these earlier waves.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - A final point that must be addressed is the dearth of female authors, which is in keeping with their near total omission from any of the lists generated so far. Within the subset of Aozora works classified as fiction (again excluding juvenile literature), the proportion of texts by female identified authors is less than 9 percent, which is slightly worse than the 10 percent for omnibus anthologies and the 12.5 percent for individual author anthologies. These low figures mask an even more alarming pattern in the anthology data, however. When the proportion of titles by female authors is calculated as a moving average over time, it stays close to 10 percent well into the 1990s (figure 2.14), rising only with the publication of several omnibus series dedicated to women writers. It is no surprise that the processes of canon making in Japan are biased against fiction by women. It is shocking, however, just how strong and consistent the bias is within this sector of the literary publishing market. To what degree it reflects the actual demographics of writers at any given point since the 1930s is a question requiring further study. Most pertinent here is the fact that with respect to gender Aozora reflects all too well the status quo. When we see a figure repeat itself like this across such varied samples—these different sensors of literary values global dynamics -it signals how systematic these dynamics are and how digital archives can compound the biases of their print forebears. In constructing samples from these archives, we must attend to these biases and decide whether to counter them by oversampling the effected subpopulation or address them as historical artifacts of real underlying biases.
[Author: Matthew Kirschenbaum; From essay:"What Is Digital Humanities and Whats It Doing in English Departments "] - The collection builds upon and extends the pioneering volumes that have preceded it, such as A Companion to the Digital Humanities and A Companion to Digital Literary Studies, as well as newer and forthcoming collections such as The American Literary Scholar in the Digital Age, Switching Codes: Thinking Through Digital Technology in the Humanities and Arts, #alt-academv, Hacking the Academy, and Teaching Digital Humanities. In the spirit of those texts and in line with the open-source ethos of the digital humanities, this volume will be published as both a printed book and an expanded, open-access webtext. The University of Minnesota Press is to be much commended for its willingness to share the volume in this wav, a feature that will significantly extend the reach of the book.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - We must also attend to inflection points where these biases even out. Figures 2.13 and 2.14 indicate that the zenshū bubbles collapse ushered in a different vision of the omnibus anthology and its ideal reader. The consecration of national canons was a less profitable venture than appeals to popular taste or attempts to compensate for voices long pushed to the margins of such canons. In the case of popular genre fiction, the postbubble anthology market and the communities of reading it likely fostered in the 1980s and 1990s is well reflected in the logics of selection informing Aozoras construction. In the case of writing by female authors, however, the uptick we see in omnibus anthologies does not transfer to Aozora in any global way. It may be that the pool of female authors from which Aozoras volunteers can draw remains constrained by copyright law. Thus, even as the demographics of authorship have changed over the past half-century, the gender imbalance in the pool of out-of-copyright authors has not. Nevertheless, knowing that Aozora does not reflect the increased attention given to female writers in literary anthologies raises a question about what global processes of literary valuation have stepped in, especially since the zenshū bubble collapse, to shape the ways prewar modern Japanese literature is memorialized and reread. What additional factors have made some works more likely to be digitized than others?
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - There are surely a host of possible forces, including the availability of works in forms other than anthologies, but one of the more stable and widespread mechanisms of canonization is secondary education. Here we find an institution highly centralized in its decision making and highly influential in its reach, but also one with a long institutional memory that tends toward stasis. Before examining Aozoras fiction on its own terms, I want to briefly look at a final bibliographic trace that records in a narrower, more controlled way the global effects of shifting literary values. It comes from a data set that indexes nearly eighty thousand works of literature included in almost seventeen hundred high school textbooks published between 1949 and 2007. The data set is useful as another reference point for situating Aozora in relation to the always imaginary totality of works from which it draws. Pedagogically, it is an opportunity to consider what can be done with a more refined and institutionally specific bibliographic data set and to devise a way of comparing its vision of modern Japanese literature with others. Up to now we have relied on calculations of percentage overlap or observations of differences across the topmost layers of each core sample. As these samples proliferate, however, we need to measure their differences in ways reflective of their overall composition.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - Of all the bibliographic data sets analyzed in this chapter, the textbook data set is by far the cleanest and easiest to navigate. Purchased as digital data directly from the publisher, it comes as a highly structured text file easily converted with a Python script into a database. Each entry consists of the title of a work and metadata (e.g., textbook title, publisher, date) for every textbook in which that work was included. Reliable as it may be as a data source, however, one still needs historical and institutional context to turn it into something useful. It is important to know, for instance, that literary texts are taught within the framework of language education, and that the latter is periodized to reflect Japanese linguistic variation over time. Thus nearly half of the textbooks in the data set are devoted to classical forms. Removing these leaves about 13,000 entries included in 842 textbooks and attributable to 2,752 authors. It is also useful to know that textbooks, which are produced by private publishers, have since 1949 been subject to approval by the Ministry of Education and that approval alone is not a guarantee of widespread adoption. Any new textbook confronts a curricular infrastructure (e.g., lesson plans, examinations) built around all that has come before. In this sense, this data set is a record of editorial choices made in the interest of winning state approval and with an eye to those texts in which instructors and school boards have already invested resources. Finally, prose fiction is generally included in textbooks in a redacted or condensed form. Titles should thus be understood as references to a work in part, not in whole.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - This bibliographic trace provides insight into a system of valuation that rewards stasis and continuity, not dynamism and innovation. Indeed, when examining just the raw counts for each title, it quickly becomes apparent that the systems feedback mechanisms have rewarded the same four works over and over again (see table 2.3). These titles dominate the representation of modern Japanese literature in high school textbooks and are collectively referred to as teiban (standard) texts. Benefiting from institutional processes set into motion in 1951 for Nakajima Atsushis “Sangetsuki” (Tiger-poet), and in 1957 for the other three, these teiban texts gradually accrued ever more value, appearing in 20 percent of modern language textbooks approved in 1975 and, except for Mori Ogais “Maihime” (dancing Girl), in 40 percent of those approved in 2007. After the teiban texts the raw counts fall quickly, dropping to just nine for the one hundredth most frequent work, Umezaki Haruos “Sakurajima” (Cherry island). Looking across the other lists in the table, it is notable that only “Maihime” and Kajii Motojirōs “Remon” (Lemon) appear in any other top ten. The degree of overlap increases after aggregating raw counts by author (see table 2.4). Sōseki is now at the top, a position he holds in no other list. Yet he is followed closely by familiar names—Akutagawa, Ōgai, and Shiga, who sit near the top of all the rankings. Nakajima Atsushi, unsurprisingly, is not far behind. But the absence of his name in the anthology rankings reveals how skewed is the perspective offered by the textbook data on this particular count.
[Author: Matthew Kirschenbaum; From essay:"What Is Digital Humanities and Whats It Doing in English Departments "] - This collection is not a celebration of the digital humanities but an interrogation of it. Several essavs in the volume level pointed critiques at DH for a variety of ills: a lack of attention to issues of race, class, gender, and sexuality; a preference for research-driven projects over pedagogical ones; an absence of political commitment; an inadequate level of diversity among its practitioners; an inability to address texts under copyright: and an institutional concentration in well-funded research universities. Alongside these critiques are intriguing explorations of digital humanities theories, methods, and practices. From attempts to delineate new theories of coding as scholarship to forward-looking visions of trends in big data, the volume sketches out some of the directions in which the field is moving.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - We did not need this textbook data to identify the teiban texts, although it does clarify the magnitude of their monopoly on literary attention in the high school classroom. These data become useful in making visible the cumulative efforts of publishers to steer this attention elsewhere. What is going on below the fixed surface of the teiban texts? Does it correspond at all to the processes shaping Aozora? The relative paucity of prose fiction works included in high school textbooks between 1949 and 2007 makes the question of archival overlap much less relevant. Although there are nearly thirteen thousand titles overall, the bulk of them occur with such infrequency, and in publications with such disparate reach, that it is hard to imagine them informing the cultural imagination to the same degree as works in anthologies. Of the top one hundred most frequent titles found in textbooks, excluding poetry and nonfiction, 87 percent of out-of-copyright works (fifty-five titles) can be found in Aozora. If overlap is not as useful a metric of comparison here, reader habits certainly are. Since 2009, Aozora has published monthly reports of how many times each work is accessed by users. The last columns of tables 2.3 and 2.4 provide the rankings of titles and authors based on these data. To give a sense of the difference in magnitude here, Sōsekis Kokoro was accessed more than four million times from 2009 to 2017, compared with two million for Nakajimas “Sangetsuki.” In all, Sōsekis works were accessed more than seventeen million times, compared with 778,822 times for Higuchi Ichiyō, who holds the twentieth spot.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - Immediately apparent from the list of most accessed titles, which includes the top three teiban texts, is that the high school literary canon has a significant influence on user traffic to Aozora. These texts likely rank so high because teachers and their students find it useful to read them online. But what of the other most frequently accessed titles and authors? Should we expect more overlap between what generations of high school students have been assigned and the preferences of Aozoras users, or between the latter and the aggregate decisions of generations of anthology editors? Given the different time scales, institutional contexts, and media platforms that have produced these rankings, the chance of overlap seems low. Even so, it is worth considering how to compare such lists in a holistic fashion to account for more than just the entries at the top. Several methods are available for measuring the correlation of ranked lists of items, but here I choose a modified Kendalls tau statistic to determine how closely matched the top fifty authors are across the five lists. Figure 2.15 shows the resulting values for each pair of lists, with darker tiles indicating higher correlation. The main insight to be gleaned from the figure is that Aozora is more closely aligned with the anthologies than with the high school textbooks, and that the latter have little overlap with anthologies. Despite the prominence of the teiban texts, curricular choices are on the whole not a good predictor of the authors read most often by Aozoras users.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - With this correlation matrix we have pulled back far enough to take in the many core samples of modern Japanese literature thus far unearthed. Lists of titles have been reduced to ranked lists of authors and further transformed into ordered numerical strings whose correlation can be measured statistically. It is, no doubt, a disorienting perspective, as individual texts and authors recede into their respective core samples like microscopic particles buried within layers of packed ice and soil. Our first inclination might be to rip these layers apart, liberating each particle from this single binding context to expose it again to light from multiple interpretive angles. But it is worth holding our focus for a moment on this reductive view, itself just one possible outcome of a series of conscious interpretive choices and statistical manipulations. Its purpose is not to provide a singular understanding of Aozoras relation to the print archive but a provisional map of the rifts and valleys that form where its representation of Japanese language literature collides with other situated knowledges of this archive. At the very highest scale, which is also the lowest resolution, we have a means to understand how Aozora is informed by different histories of textual selection and valuation as they have unfolded across several intersecting aesthetic, commercial, and educational contexts. Its own biography is woven out of these earlier instruments of archival memory even as it begins to weave its own patterns. We know that Aozora captures several bright patches of the foreign literary field, albeit in rather disconnected bits and pieces. It hews closely to prewar authors canonized in anthologies and textbooks, although it does not always direct attention to the same canonical titles. It stitches together an expansive section for popular genre authors who previously left faint marks in these same anthologies and textbooks. And finally, we know it adheres all too well to the patterns by which women writers have been marginalized in archival memories.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - Going forward, this multiperspectival map will need to be expanded and revised with other bibliographic traces. Those analyzed thus far should also be further explored as potential resources for constructing samples of all kinds, which means identifying the population of questions to which these samples might lead to meaningful inferences. Do the most anthologized works in a given period, for instance, share certain formal or stylistic features? What features distinguish works popular in one period versus another? These are not the questions asked in this book, which is more invested in the prewar field of literary production and in questions specific to the intricacies of that moment. To get at these questions requires targeted supplementing of Aozora as a convenience sample, using traces of the archive more fine-grained than what the bibliographic traces used in this chapter provide. Yet these traces will continue to frame our global understanding of the distinct perspective that the Aozora collection offers, one that will loom larger as the scale of evidence grows from a few hundred texts in the next chapter to several thousand by the last. As a prelude to this progression, it is worth a brief look inward at Aozora to roughly delimit, through its finitude, the population of questions to which it lends itself.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - AOZORA AT THE LIMITS
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - My excavations of the bibliographic record have drawn on several techniques for describing and visualizing the contours of a data set and for refining the set of research questions a collection of texts can address. Applying them to the Japanese language fiction in Aozora, and more narrowly to those works that exceed the mean text length of the collection (e.g., equal to or longer than Hayashi Fumikos Shitamachi”), several intrasample constraints become apparent. Plotting this corpus subset by date of first publication shows that most fall between 1908 and 1954 and are highly concentrated between 1925 and 1940 (figure 2.16). Outside of this period, there are fewer than a dozen texts per year, making analysis of longitudinal trends beyond this time frame unreliable. Conversely, trends within this time frame are likely to be driven by the mass of texts that form the graphs middle peaks. A second point is that this mass, which builds from the early 1920s and crests in the late 1930s, roughly parallels a steady upward trend in overall book publication at this time, at least as indicated by the Shuppan nenkan data cited previously. Yet these same data show that a publishing boom of equal magnitude occurred between 1897 and 1910, a period woefully underrepresented in Aozora. Even if we grant that literary publication made up a smaller share of the total during this earlier boom, the dearth of late-Meiji fiction reinforces again that this corpus is best suited to questions whose historical horizon begins around 1910.
[Author: Matthew Kirschenbaum; From essay:"What Is Digital Humanities and Whats It Doing in English Departments "] - And the field of digital humanities does move quickly; the speed of discourse in DH is often noted with surprise by newcomers, especially at conferences, when Twitter feeds buzz with links to announcements, papers, prototypes, slides, white papers, photos, data visualizations, and collaborative documents. By the typical standards of the publishing industry, this text has seen a similarly rapid pace of development, going from first solicitation of essays to published book in less than a year. To have a collection of this size come together with such speed is, to put it mildlv, outside the norms of print-based academic publishing. That it did so is a tribute to the intensity of the debates, the strength of the submissions, and the responsiveness of the press. But it is also a testimonial to the collaborative process through which the book was produced, a feature seen most clearly in the peer review that it received.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - Within this horizon, the range of suitable questions will be further limited by the authors and works represented. There are 174 unique authors in the corpus, 19 of whom are women (11 percent). Together they are responsible for 126 (7 percent) of all works. Although it accentuates the gender imbalance present in Aozoras fiction as a whole, this smaller corpus does preserve the diversity of high-brow to popular authors (figure 2.17). When ranked by number of titles in the corpus, half of the top twenty authors are closely associated with historical or popular genre fiction, including Nomura Kodō, Unno Jūza, Okamoto Kidō, Yoshikawa Eiji, Sasaki Mitsuzō, and Kikuchi Kan. The other half are mostly canonical authors whose works are well represented in the bibliographic data sets, including Izumi Kyōka, Dazai Osamu, Akutagawa Ryūnosuke, Mori Ōgai, and Natsume Sōseki. Less familiar are Toyoshima Yoshio and Makino Shinichi, who have fared worse in anthologies and textbooks but who were well known in literary circles of their own time and have amassed devoted followers since. This relative diversity at the level of genre and audience bodes well for analyses that want to account for the breadth of literary output in the interwar period and, like much recent scholarship, aspire to blur the historical divisions between pure” and “popular” literature actively being constructed and contested in these years. The chapters that follow leverage this diversity to test and perturb such divisions at the level of style, form, and discourse.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - To do so, however, an additional set of constraints must be accounted for. If it appears that there is diversity at the level of author and genre, there is more homogeneity at the level of work, at least for certain well-represented popular authors such as Nomura Kodō and Okamoto Kidō. Of the 126 works attributed to Nomura, for example, 114 are installments in Zenigata Heiji torimono hikae (The casebook of Zenigata Heiji), a series that began in 1931 and ended in 1957 with nearly 400 episodes of varying length. Each new episode continued the ongoing adventures of Edo-period detective Zenigata and his trusty associate Hachigorō. Okamoto, incidentally, had pioneered this subgenre in the late teens with his Hanshichi torimono chō (The casebook of Hanshichi), of which sixty-two installments are in the reduced corpus. Further inspection reveals that many of the historical fiction writers are represented through a limited number of serialized titles. This is not true for writers of detective and science fiction such as Unno Jūza and Hisao Jūran, but even so more than four hundred works in the corpus belong to a larger series. Given the mostly episodic nature of these series, such that the narrative content varies from one installment to the next, it makes little sense to treat them as single works. Still we must be cognizant of the sampling biases introduced into large-scale analysis by the overrepresentation of works from a single series or author. By not controlling for these biases through the creation of more balanced samples, we might describe as a general trend (i.e., the increased use of specific words) what is really the local effect of serial repetition or authorial style.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - These biases, along with the generic and temporal ones already noted, demarcate this collections “outer limits.” Much of this chapter has focused on how to delineate these limits through comparison with other large-scale core samples of the print archive. Although awareness of these limits is crucial to thinking about the kinds of literary historical questions this particular digital sample can address, the point of such awareness is not to defend (or critique) this corpus as a (mis)representative list or static canonical sample. On the contrary, this awareness is meant to lead us away from myths of absolute fidelity or plenitude—because there is no such thing as a complete picture—and toward the “relational mode of reasoning” that such large collections afford. Relational because this corpus can be subdivided and rebalanced to create different kinds of samples through which to analyze a particular phenomenon; because its limits point us to where it could be supplemented with other texts and other kinds of evidence to expand the population of questions it can address; and finally because it can itself be related to other samples and case studies to confirm or complicate the evidence they provide about large-scale literary trends.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - On this last point, consider how the Aozora fiction corpus allows us to revisit and replicate some of the investigations of literary change introduced in chapter 1. Yasumoto Bitens analysis of the change in mean percentage of kanji characters within literary texts, when redone with this corpus, shows a similar downward trend between 1900 and 1955 (figure 2.18). In absolute terms, the results show a decline from roughly 200 kanji per 1,000 characters to about 150, as compared with his estimated drop from 393 to 275. In relative terms, however, this 25 percent decline is close to his 30 percent. On the one hand, some caution is warranted given that most of the change in our sample occurs at its ends, where the amount of data is sparser. On the other hand, that the trend is approximately reproduced adds evidence to the claim that the decline was a real background change and is thus a meaningful context in which to situate individual works.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - Similarly, we can re-create the plot that Kabashima Tadao and Jugaku Akiko made to explore how noun proportion in a literary passage is related to MVR, or the ratio of verbs to adjectives and adverbs. Figure 2.19 plots the part of speech counts for the corpus. The dashed lines indicate the 2.5th and 97.5th percentiles for both variables, outside of which fall outlier texts relative to the distribution of noun ratio and MVR values overall. This plot replicates their finding that an increase in nouns is negatively correlated with a lower MVR score (e.g., fewer adjectives and adverbs when compared with verbs). Zooming in closer, it brings new outliers into view because of the wider comparative context. Whereas Ibuse and Tanizaki previously stood out as the most “summarizing” and “descriptive,” respectively, now it is the historical fiction writer Mori Ogai who is most summarizing,” the childrens author Miyazawa Kenji who is most statically descriptive,” and the modernist stylist Hori Tatsuo who is most dynamically descriptive.
[Author: Matthew Kirschenbaum; From essay:"What Is Digital Humanities and Whats It Doing in English Departments "] - The book, in fact, went through three distinct stages of peer review, each of which required separate revisions: the first and most innovative process was a semipublic peer-to-peer review, in which contributors commented on one anothers work. Essays then went through an editors review, which was followed finally by a traditional blind review administered by the press.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - The emergence of these new outliers in the much larger Aozora corpus snaps back into focus the problem of the “evidence gap with which this chapter began. Recall that Kabashima and Jugaku were extending Hatanos earlier effort to distinguish the styles of Tanizaki and Shiga using sentence length and parts of speech. Hatano, in turn, was seeking to verify the qualitative assessments of a literary critic for whom the styles of Tanizaki and Shiga were distinctive types. With each expansion in comparative scale, each narrowing of the evidence gap, a new figuration of difference is achieved, and so too are new ways of reading the exceptionality or typicality of specific texts within the larger trends described by these figures. Shiga is displaced by Ibuse, who is displaced, finally, by Ōgai. Should we treat the last figure as more definitive because it is more exhaustive than previous ones? Exhaustiveness in and for itself that unattainable limit point where everything has been read is no guarantor of fact or truth. It is only ever an imagined whole, an instrument of fact making that requires “political and cognitive” investment before it can be collectively acknowledged as fact. In a series of shifting figurations of difference such as those sketched here, it is easy to privilege scale as the final determinant of truth. But knowing of Aozoras various misalignments with the bibliographic record, it seems more prudent to recognize scale as an ongoing, unfinished negotiation over what counts as part and whole; over how much of the whole needs to be seen in order to see enough; and over all the ways the gap between part and whole might be narrowed to establish meaningful variation or typicality within larger unities.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - It is the same negotiation alluded to by Josephine Miles in this chapters epigraph, where she observes that the “establishing of quality of a poem or other aesthetic object is often predicated on assumptions of quantity, the latter of which go unverified beyond the level of general impression.” But these “touchstones in art,” these rarities” singled out as being the most unique or particular along some dimension, also turn out to be fragile because of “their variability from reader to reader.” Miles felt it was time “to give some proportion to the description” of poetic quality by understanding that poetry is all that is in it as well as the best that is in it.” If literary critics no longer frame the exceptionality of artistic objects in terms of quality,” we have found other ways to justify a continued focus on rarities” and the interpretive flexibility that such singular cases allow. The case study continues to be valuable precisely because it does not subordinate the “chromatic plenitude” of a text to the kinds of abstraction that comparison of many texts requires. Comparison invariably entails a narrowing of the dimensions through which texts are described, especially as the space of comparison expands. Their internal coherency and localized meaning fragment, subsumed by higher-order structures and relations that can occlude, as Stanley Fish put it, the protean and various significances which are attached, in context and by human beings, to any number of formal configurations.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - Reading for particularity is indeed one of the cornerstones of literary studies as a discipline and should remain so. But in practice, as Miles suggested decades ago and as others have repeatedly pointed out, even the most focused readings of particularity never wrest themselves entirely from comparison or assumptions about relative quantity. Case studies hover, as Berlant reminds us, “about the singular, the general, and the normative” even when privileging the irreducibility of the individual case or, as Alan Liu says of the New Historicist anecdote, celebrating it as random access into historical processes always everywhere indeterminate. As soon as there is a move to generalize beyond that case—to claim that it is exceptional, typical, or symptomatic of some greater whole—then a kind of quantitative logic comes into play. One has to make assumptions and choices about the relative significance of all the possible contexts that might impinge on a texts meaning (e.g., biographical, historical, discursive, political, readerly). Case-study narrative may try to represent this narrowing of context as driven by the indeterminacy of the text itself, but this only highlights for Liu a paradoxical desire for random determination or determined randomness”—a desire to at once transcend the constraints of history while also reading through them and selecting those most relevant to ones interpretation. Subtending this desire is a series of selections on context that is something like a database query.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - Here is a key incident. It has a microdesign that feels like it may be part of a broader pattern. What does the whole data set of history look like if we filter it through that microdesign (in SQL, e.g., “SELECT author, work FROM history WHERE keyword = nature OR keyword = ‘Napoleon AND year > ‘1802”)? What other “hits” might be returned leading toward pattern recognition—that is, the recognition of “episteme,” mentality, structure,” “power,” etc.?
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - If this reads as a parody of historicist method, the humor works in part because of the degree to which the digital archive and keyword search already mediate so deeply the ways that scholars go about constructing the relation of part to whole, text to context. Access to more evidence, and faster ways of searching across this evidence, have forced anew the centuries old confrontation between different modes of generalization—those latent in the study of singular works and those renewed (or made possible) by changing archival infrastructures. Old as the confrontation is, it feels heightened now because of how embedded these new infrastructures have become in scholarly research practice when compared with the numerical tables, concordances, and mainframe computers of earlier eras. For some, it has become harder to ignore all the evidence we have available for the “establishing of quality,” especially with respect to contexts and scales of description that, for all the affordances of the closely read case, cannot be captured through that practice alone. But to say that we should pay more attention to this evidence is not to cast aside those affordances, nor to hand over the burden of interpretation to scale in and for itself and the algorithms that render this scale knowable. As I have demonstrated, the evidence gap can be narrowed with numbers from many directions, and in many ways. Moreover, these methods, which can be thought of as attempts to construct or model the representativeness of parts in relation to wholes, are themselves protean and variable and subject to interpretation at every step of the way.
[Author: Hoyt Long; From essay:"The Values in Numbers Reading Japanese Literature in a Global Information Age "] - To recognize this is to recognize some of the points of continuity across the case-study and non-case-study opposition. I have addressed these continuities through the language of sampling and selection, which transposes the language of representativeness” into a different key. It is a language that, whether in its statistical and social-scientific registers or in its more humanist variations, encodes epistemologies of generalization—of how we think about the diversity of individual parts within and against the unity of larger wholes. To the extent that literary historians are compelled to address larger bodies of evidence, even and especially when these are known to be partial and incomplete, it is this language we must learn and adapt to our own critical ends. This means drawing on the resources and practices of content analysis and other disciplines, and it will certainly involve continued debate and negotiation over how and when to sample—of how and when to construct wholes external to and exceeding individual texts. That it involves such negotiation, and thus requires scholars to make explicit the “representativeness” of their samples against the piles of archival debris that history leaves in its wake, should be welcome news wherever there is a desire to pry open the black boxes through which critical judgment often operates. The digital archive is a chance to rethink, once again, the various meanings of the individual text and what textual interpretation stands to gain (and lose) from projecting this multiplicity into discrete dimensions across expanded scales of relation.
[Author: Matthew Kirschenbaum; From essay:"What Is Digital Humanities and Whats It Doing in English Departments "] - Recent coverage of the digital humanities (DH) in popular publications such as the New York Times, Nature, the Boston Globe, the Chronicle of Higher Education, and Inside Higher Ed has confirmed that the digital humanities is not just “the next big thing,” as the Chronicle claimed in 2009, but simply “the Thing,” as the same publication noted in 2011. At a time when many academic institutions are facing austerity budgets. department closings, and staffing shortages, the digital humanities experienced a banner vear that saw cluster hires at multiple universities, the establishment of new digital humanities centers and initiatives across the globe, and multimillion-dollar grants distributed by federal agencies and charitable foundations. Even Google entered the frav, making a series of highly publicized grants to DH scholars.
[Author: Matthew Kirschenbaum; From essay:"What Is Digital Humanities and Whats It Doing in English Departments "] - Clearly, this is a significant moment of growth and opportunity for the field, but it has arrived amid larger questions concerning the nature and purpose of the university system. At stake in the rise of the digital humanities is not only the viability of new research methods (such as algorithmic approaches to large humanities data sets) or new pedagogical activities (such as the incorporation of geospatial data into classroom projects) but also kev elements of the larger academic ecosystem that supports such work. Whether one looks at the status of peer review, the evolving nature of authorship and collaboration, the fundamental interpretive methodologies of humanities disciplines, or the controversies over tenure and casualized academic labor that have increasingly rent the fabric of university life, it is easy to see that the academy is shifting in significant ways.
[Author: Matthew Kirschenbaum; From essay:"What Is Digital Humanities and Whats It Doing in English Departments "] - And the digital humanities, more than most fields, seems positioned to address many of those changes. The recently created international group 4Humanities, for instance, argues that the digital humanities community has a “special potential and responsibility to assist humanities advocacy because of its expertise in “making creative use of digital technology to advance humanities research and teaching”. In a moment of crisis, the digital humanities contributes to the sustenance of academic life as we know it, even as (and perhaps because) it upends academic life as we know it.
[Author: Matthew Kirschenbaum; From essay:"What Is Digital Humanities and Whats It Doing in English Departments "] - Weve come a long way from Father Busas digital concordances. Indeed, the rapid ascent of the digital humanities in the public imagination and the concomitant expansion of its purview have masked, and at times threatened to overshadow, decades of foundational work by scholars and technologists who engaged in “digital humanities work before it was known by that name. Though longtime practitioners, having weathered decades of suspicion from more traditional colleagues, have largely welcomed an influx of newcomers into the field-the theme of the 2011 Digital Humanities Conference was “The Big Tent, a metaphor much debated in the pages that follow-some DHers have found the sudden expansion of the community to be disconcerting. Indeed, fault lines have emerged within the DH community between those who use new digital tools to aid relatively traditional scholarly projects and those who believe that DH is most powerful as a disruptive political force that has the potential to reshape fundamental aspects of academic practice.
[Author: Matthew Kirschenbaum; From essay:"What Is Digital Humanities and Whats It Doing in English Departments "] - As the digital humanities has received increasing attention and newfound cachet, its discourse has grown introspective and self-reflexive. In the aftermath of the 2011 Modern Language Association Convention, many members of the field engaged in a public debate about what it means to be a digital humanist.” The debate was sparked by University of Nebraska scholar Stephan Ramsay, whose talk at the convention was bluntly titled Whos In and Whos Out.” Having been asked by the roundtable session organizer to deliver a pithy, three-minute-long take on the digital humanities, Ramsav noted increasingly capacious definitions of the field (“[DH] has most recently tended to welcome anyone and anything exemplifying a certain wired fervor,” he noted) before delivering, with the mock-serious pretension that it would settle the matter once and for all, the pronouncement that, yes, there are some basic requirements one must fulfill before calling oneself a digital humanist: Digital Humanities is not some airy Lyceum. It is a series of concrete instantiations involving money, students, funding agencies, big schools, little schools, programs, curricula, old guards, new guards, gatekeepers, and prestige.... Do you have to know how to code [to be a digital humanist]? Im a tenured professor of digital humanities and I sav ves. ...Personally, I think Digital Humanities is about building things.... If you are not making anything, you are not ...a digital humanist.
[Author: Matthew Kirschenbaum; From essay:"What Is Digital Humanities and Whats It Doing in English Departments "] - Predictably, these comments set off an intense debate during the session itself and in the ensuing online discussions. Ramsay wrote a follow-up blog post in which he softened his stance—moving from “coding” as a membership requirement to the less specific building”—but he still noted that the fundamental commonality that can be found among digital humanists “involves moving from reading and critiquing to building and making”.
[Author: Matthew Kirschenbaum; From essay:"What Is Digital Humanities and Whats It Doing in English Departments "] - The semipublic peer-to-peer review was modeled on a number of recent experiments in peer review, most notably Noah Wardrip-Fruins Expressive Processing, Kathleen Fitzpatricks Planned Obsolescence, Shakespeare Quarterly’s “Shakespeare and New Media” issue, and Trebor Scholzs Learning through Digital Media. In all of these cases, CommentPress, a WordPress blog theme built by the Institute for the Future of the Book, was used to publish draft manuscripts on a site where comments could be added to the margin beside particular paragraphs of the text. Most of the aforementioned examples were fully public, however, meaning that anyone with the link and an interest in a particular text could read and comment on it. For Debates in the Digital Humanities, we chose to go with a semipublic option, meaning that the site was password protected and accessible only to the scholars involved in its production. Draft essays were placed on the site along with a list of review assignments: each contributor was responsible for adding comments to at least one other text. The process was not blind: reviewers knew who had written the text they were reading, and their comments were published under their own names. Often, debates between contributors broke out in the margins of the text.
[Author: Matthew Kirschenbaum; From essay:"What Is Digital Humanities and Whats It Doing in English Departments "] - Whether measured quantitatively or qualitatively, the peer-to-peer review process was effective. In the space of two weeks, the thirty essays that went through the process received 568 comments—an average of nearly twenty comments per essay (the median number of comments received was eighteen). Many contributors went far beyond the single essay that had been assigned to them, commenting on as many as half of the essays in the volume. Lest skeptics assume that a nonblind review process leads inevitably to superficial praise or even to a mild suppression of negative feedback, it should be noted that several features of the peer-to-peer review worked against such possibilities. The semipublic nature of the review meant that the names of reviewers were attached to the comments thev left; a failure to leave substantive comments would have reflected poorly on the reviewers own work. The fact that review assignments were shared openly among the circle of contributors created a sense of peer pressure that made it difficult for reviewers to shirk their duties. And because the peer-to-peer review was not fully open to the public, contributors seemed comfortable providing negative criticism in a more open fashion than they might have had the platform been fully public.
[Author: Matthew Kirschenbaum; From essay:"What Is Digital Humanities and Whats It Doing in English Departments "] - The peer-to-peer review website wound up imparting a sense of community and collectivity to the project as a whole. It also gave contributors a better sense of the full volume in its prepublished state. Whereas contributors to edited collections typically gain a vision of the entire book only when it is finally printed, contributors to Debates in the Digital Humanities were able to see the work of their peers while revising their own essays. This led some authors not only to thank fellow contributors in their acknowledgments for feedback given during peer-to-peer review but also to cite one anothers essays and peer reviews. In short, rather than serving solely as a gate-keeping mechanism, this review process built a sense of cohesion around the project itself. And it was followed and supplemented by more traditional forms of review that provided opportunities for the kind of unfiltered criticism tvpically associated with blind review.
[Author: Matthew Kirschenbaum; From essay:"What Is Digital Humanities and Whats It Doing in English Departments "] - Ultimately, this hybrid, semiopen, multistage model of peer review incorporated the innovations of completely open models of peer-to-peer review while retaining the strengths of more traditional processes.
[Author: Matthew Kirschenbaum; From essay:"What Is Digital Humanities and Whats It Doing in English Departments "] - The resulting text reflects the range of issues facing the digital humanities at the present time. It begins with the section “Defining the Digital Humanities, a subject of perennial discussion within the DH community. Other portions of the book explore the field by moving from theory to critique to practice to teaching, ending with a look toward the future of the digital humanities. Each chapter closes with a short selection of materials reprinted from scholarly blogs and wikis, reflecting both the importance of such networked spaces to digital humanities scholars and the ways in which such middle-state” publishing both serves as a vital channel for scholarly communication and feeds into more formal publishing projects.
[Author: Matthew Kirschenbaum; From essay:"What Is Digital Humanities and Whats It Doing in English Departments "] - The printed version of Debates in the Digital Humanities is the first iteration of this project; it will be followed by an online, expanded, open-access webtext. We are planning a website that will offer not a static version of the book, but rather an ongoing, community-based resource that can be used to track and extend discussions of current debates. Given the speed with which the digital humanities is growing, such a dynamic resource is necessary. And in that sense, this volume is but the beginning of a new set of conversations.